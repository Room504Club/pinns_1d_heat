# 论文总结
### 论文总结

这篇论文的核心思想是，**传统的计算流体力学（CFD）方法虽然非常成熟，但在处理某些问题时遇到了瓶颈，而物理信息神经网络（PINNs）为此提供了一个充满潜力的新范式。**

具体来说，论文可以分为以下几个部分：

1. **指出现有方法的“痛点”**
    
    - 传统的CFD方法，比如有限元法，在过去50年取得了巨大成功，可以很好地求解**正问题**（即给定所有边界条件和初始条件，求解流场）。
        
    - **但是**，它们存在几个重大缺陷：
        
        - **数据融合难**：很难将实验中测量到的、带有噪声的、不完整的数据“无缝”地融入到模拟计算中。
            
        - **网格划分复杂**：对于复杂外形（比如工业应用中的复杂几何体），生成高质量的计算网格非常耗时，甚至是一门艺术。
            
        - **逆问题昂贵**：求解**逆问题**（比如根据部分观测数据反推未知的边界条件、物性参数或整个流场）通常计算代价极高，需要设计全新的算法和代码。
            
        - **代码维护困难**：像OpenFOAM这样的软件代码量巨大（超过10万行），维护和迭代极其困难。
            
2. **提出解决方案：物理信息神经网络 (PINNs)**
    
    - PINN是一种新型的深度学习框架，它将物理定律（以偏微分方程，如纳维-斯托克斯方程NSE为代表）直接编码到神经网络的**损失函数**中。
        
    - **核心优势**在于：
        
        - **数据与物理的无缝融合**：PINN可以同时利用已知的物理方程和稀疏、带噪声的观测数据进行学习。
            
        - **无网格特性**：它通过**自动微分**来计算方程中的导数项，理论上不需要像传统CFD那样划分网格，从而避开了最头疼的网格生成问题。
            
        - **统一正、逆问题**：对于PINN来说，求解正问题和逆问题的框架是统一的，无需为逆问题开发全新的、复杂的算法。
            
3. **通过三大案例展示PINN的强大能力**
    
    - 论文通过三个精心设计的、具有代表性的流体力学案例，展示了PINN在解决逆问题上的卓越表现：
        
        1. **三维不可压缩绕流**：仅利用几个二维平面上的速度数据，成功地、高精度地重建了圆柱绕流产生的复杂三维尾涡流场（包括速度和压力）。这模拟了实验中常见的PIV测速技术，展示了从稀疏数据恢复完整流场的能力。
            
        2. **二维可压缩超音速流**：在物体边界条件未知的情况下，仅利用类似于纹影法实验能观测到的**密度梯度**信息和少量表面压力数据，就成功反演出了钝头体周围的弓形激波流场。
            
        3. **生物医学流动**：模拟了动脉中血栓的变形，仅根据代表血栓形态的**相场**数据，就成功反推出了描述血栓渗透性的未知材料参数。
            
4. **结论与展望**
    
    - 论文明确指出，现阶段的PINN并**不是**要完全取代传统的CFD。在求解标准的正问题时，高精度的CFD求解器在效率和精度上仍然更优。PINN目前的主要挑战在于损失函数的优化（一个高维的非凸问题）。
        
    - **然而，PINN是一个极其强大的补充工具**。当问题是 **“病态的” (ill-posed)，** 比如边界条件未知，但又有一些零散的内部观测数据时，PINN的优势就体现出来了，它远比传统方法更高效、更准确。
        
    - 未来，PINN在流场主动控制、利用迁移学习预测更高雷诺数的流动、为湍流建立封闭模型等方面都有巨大的研究潜力。

---
> [!TIP]- 启发
> ### 对你的启发
> 
> - **换个思路看问题**：传统方法是“给定所有条件，求解唯一结果”。PINN则是“给定一部分物理规则和一部分观测结果，找到最符合这两者的那个解”。这是一种**数据驱动**和**物理驱动**相结合的思路。
>     
> - **神经网络的角色**：在这里，神经网络不再仅仅是一个黑箱的回归或分类工具。它成了一个“函数逼近器”，学习的目标是一个能够满足物理方程和数据约束的函数（即流场解 $u(t,x,y,z)）$。
>     
> - **“物理”如何“通知”网络**：通过将物理方程（PDE）的残差（即把网络输出代入PDE后，看它等不等于0）作为损失函数的一部分。如果残差不为0，就产生一个“惩罚”，迫使网络在训练中调整参数，直到输出的解能满足物理定律为止。
>     
> 
> 总之，这篇论文为你打开了一扇新的大门，让你看到人工智能，特别是深度学习，是如何与经典的物理学深刻地结合起来，去解决那些困扰了科学家和工程师数十年的难题。
> 
> 接下来，你可以把论文的**摘要 (Abstract)** 和**引言 (Introduction)** 部分发给我，我们开始逐段的精读之旅。


# 论文精读

## 摘要

尽管在过去的50年里，使用纳维-斯托克斯方程（NSE）的数值离散化方法来模拟流动问题已经取得了重大进展，但我们仍然无法将含噪声的数据无缝地整合到现有算法中，网格生成过程非常复杂，而且我们也无法解决由参数化的NSE所控制的高维问题。此外，求解流动的逆问题通常成本高到令人望而却步，并且需要复杂昂贵的数学表述和全新的计算机代码。在此，我们回顾了“流动物理信息学习”这一方法，它能无缝地融合数据和数学模型，并通过“物理信息神经网络”（PINNs）来实现。我们将展示PINNs在解决与三维尾流、超音速流和生物医学流相关的逆问题方面的有效性。

> [!TIP]- 注释
> 这段话是在告诉我们，研究流体力学的老方法（我们称之为**传统CFD**，即计算流体力学）虽然很厉害，但遇到了几个“天花板”，而一个叫**PINN**的新方法可能就是打破这些天花板的锤子。我们来逐句拆解一下：
>
>#### **1. 传统方法的辉煌与瓶颈**
>
> - **“尽管在过去的50年里，使用纳维-斯托克斯方程（NSE）的数值离散化方法来模拟流动问题已经取得了重大进展...”**
>    
>>   * **纳维-斯托克斯方程 (NSE)**：你可以把它想象成是描述“流体运动”这门武功的最高秘籍。无论是空气的流动、水的波浪，还是飞机周围的气流，都遵循这套方程。它本质上就是牛顿第二定律（$F=ma$）在流体里的具体体现。
>>   * **数值离散化**：这本“武功秘籍”太深奥了，是用复杂的偏微分方程写的，人脑和电脑都不能直接求解。于是科学家们想了个办法，叫做“离散化”。就好比我们想知道一条曲线的长度，直接测量很难，但我们可以把它切成无数个小直线段，然后把这些小直线段的长度加起来，近似得到曲线的长度。
>>  
>>   * “数值离散化”就是把连续的流动空间和时间，切分成一个个非常小的格子（这叫**网格**）和时间步。在每个小格子里，复杂的方程就被简化成了电脑能计算的加减乘除。这就是传统CFD方法的核心。在过去的50年里，这个方法发展得非常好，让我们能模拟出各种复杂的流动现象。
>
>
> - **“...但我们仍然无法...”**
> 
>>   * **“...将含噪声的数据无缝地整合到现有算法中...”**
>>       * **场景**：想象一下，我们在一个风洞里测量飞机模型周围的速度。测量仪器总会有误差，得到的数据就像一张带有雪花点的老照片，这些“雪花点”就是**噪声**。传统CFD算法就像一个挑剔的处女座，它需要非常干净、精确的边界条件。你给它带有噪声的、不完整的实验数据，它会很难处理，甚至直接“罢工”。
>>   * **“...网格生成过程非常复杂...”**
>>       * 前面说了，传统方法需要先把空间切成小格子。如果模拟一个简单的方盒子里的水流，那很简单。但如果要模拟一辆外形极其复杂的F1赛车，或者一个布满血管的人体器官，给这些奇形怪状的东西划分高质量的网格，就是一个极其痛苦且耗时的工作。这步做不好，后面的计算就全错了。
>>   * **“...无法解决由参数化的NSE所控制的高维问题。”**
>>       * **高维问题**：想象一下，你想设计一个最优的飞机机翼。机翼的形状可能由几十个参数（比如翼展、后掠角、弯曲度等等）共同决定。如果你想找到最好的参数组合，用传统方法，每换一组参数，你都得重新划分网格、重新算一遍，这计算量简直是天文数字。这种由大量参数控制的问题，就叫**高维问题**。传统方法在这种问题面前基本无能为力。
>
>#### **2. 新方法的提出与展望**
>
> - **“此外，求解流动的逆问题通常成本高到令人望而却步...”**
>     
>>   * **正问题 vs 逆问题**：
>>       * **正问题**：就像做一道数学应用题，**所有已知条件都给你了**（比如水流入口的速度、温度，管壁的性质），让你求**结果**（管道内各处的速度、压力分布）。传统CFD擅长这个。
>>       * **逆问题**：正好相反。比如，我们在飞机翅膀表面贴了几个传感器，只测量到了几个点的压力。现在想反过来推断**整个飞机周围完整的流场**是怎样的，甚至想反推**飞机机翼的准确形状**是什么。这种“由果推因”的问题就是**逆问题**。它对传统CFD来说极其困难，成本高昂。
>
> - **“在此，我们回顾了‘流动物理信息学习’这一方法...通过‘物理信息神经网络’（PINNs）来实现。”**
>   
>>   * 这就是解决方案！**PINN**的核心思想是：我们让一个神经网络去学习流场的解。怎么学呢？我们给它两样东西当“老师”：
>>
>>     1.  **物理老师 (Physics-informed)**：就是那本“武功秘籍”——纳维-斯托克斯方程。我们告诉神经网络，你猜出的任何答案，都必须符合这本秘籍里的规则。
>>     2.  **数据老师 (Data-driven)**：就是我们用实验测到的那些稀疏的、带噪声的数据。我们告诉神经网络，你猜的答案，在你经过的那些测量点时，必须和我们的测量值尽量接近。
>>
>>   * 通过同时听从这两位老师的教导，神经网络就能“悟”出那个既符合物理规律，又能匹配真实数据的完美答案。
>
> - **“我们将展示PINNs在解决...三维尾流、超音速流和生物医学流相关的逆问题方面的有效性。”**
>    
>>   * 最后，作者说，别光听我吹牛，我接下来会用三个“硬核”的例子来证明PINN到底有多牛，尤其是在解决上面提到的、让传统方法头疼的**逆问题**上。
>
>---
>
>**一句话总结摘要**：**旧方法（CFD）算流体很厉害，但处理不完美数据和逆问题时很吃力。我们提出一种新方法（PINN），它让神经网络同时学习物理定律和真实数据，能漂亮地解决这些难题。**

> [!NOTE]- 🧠 Parametrized Navier–Stokes Equations（参数化流体方程）
>
>## ✅ 基本定义
>
>**Parametrized NSE** 是带有可变参数的 Navier–Stokes 方程。  
>它描述了在不同物理条件（如速度、形状、黏性等）下，流体如何流动。
>
> NSE 是流体力学中的“牛顿定律”，而 Parametrized NSE 则是在模拟“各种版本的流体世界”。
>
>---
>
>## 🧠 类比解释（面向高中生）
>
>🎮 想象你在玩一个流体模拟游戏：
>
>- 你可以调节水的速度（快 or 慢）
>- 改变管道形状（宽的 or 窄的）
>- 换不同的流体（清水、油、蜂蜜）
>- 加热或冷却水流
>
>每改变一个参数，水的流动都会变化。**Parametrized NSE** 就是在数学上模拟这种“可调条件下”的水流行为。
>
>---
>
>## 📘 数学角度（简化版）
>
>传统 NSE：
>
>$$
\rho \left( \frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u} \right) = -\nabla p + \mu \nabla^2 \mathbf{u}
>$$
>
>参数化 NSE 示例：
>
>$$
\rho(\mu) \left( \frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u} \right) = -\nabla p + \mu(\mu) \nabla^2 \mathbf{u}
>$$
>
>其中：
>- $\mu$ ：表示**可变参数**（比如黏度、雷诺数、几何形状等）
>- 方程右边的系数也可以依赖于 $\mu$
>
>---
>
>## 🧪 常见应用场景
>
>- 🧬 生物医学流体（不同血管直径、血液浓度）
>- ✈️ 超音速飞行（不同马赫数、角度）
>- 🌬️ 风工程（不同建筑外形或风速）
>- ⚙️ 工业设计（变化参数的多模型测试）
>
>---
>
>## 🧩 延伸思考：为什么这么难？
>
>- 参数多 → 方程变复杂
>- 想考虑“所有可能” → 解的维度变高
>- 普通数值方法不够灵活 → 需要新的方法，如 **物理引导神经网络（PINNs）**
>
>---
>
>## 📝 建议插图
>
>可手绘或用 Excalidraw 画以下图例：
>
>1. 🌊 管道中水流示意图  
  > - 管道形状不同、速度不同 → 展示不同流线
>
>2. 📈 参数空间图：  
  > - 用坐标轴表示：X = 黏度，Y = 入流速度  
   >- 每个点表示一个 NSE 方程的不同版本
>
>3. 💻 模拟对比图：传统 vs PINNs
>
> ![[week1_paperlearning1.png]]
>---
>
>## 🏁 小结一句话
>
>> **Parametrized NSE = 给 Navier–Stokes 方程加上多个“可调旋钮”，让它能模拟各种真实流体场景。**
>
>---
>
>## 🔗 延伸阅读
>- [Navier–Stokes 方程简介]()
>- [PINNs：Physics-Informed Neural Networks 简介]()

---
## 1 引言

在过去的50年里，计算流体力学（CFD）在使用有限元法、谱方法乃至无网格方法数值求解不可压缩和可压缩的纳维-斯托克斯方程（NSE）方面取得了巨大的进展 。然而，对于真实的工程应用，我们仍然无法将（多保真度）数据无缝地整合到现有算法中，并且对于工业级复杂的问题，网格生成既耗时又仍像一门艺术 。此外，求解逆问题，例如确定未知的边界条件或传导率等，通常成本高到令人望而却步，并且需要不同的数学表述和全新的计算机代码 。最后，像OpenFOAM这样的计算机程序拥有超过10万行代码，这使得它们几乎不可能在一代代开发者之间进行有效的维护和更新 。

由Karniadakis研究组在一系列论文中提出的“物理信息学习”，无论是基于高斯过程回归还是物理信息神经网络（PINNs），都能够将多保真度/多模态的实验数据与各种形式的纳维-斯托克斯方程（包括不可压缩流、可压缩流和生物医学流）无缝地集成起来 。PINNs使用

**自动微分** 来表示所有的微分算子，因此不再明确需要生成网格 。取而代之的是，纳维-斯托克斯方程以及任何其他的运动学或热力学约束，可以通过惩罚与目标值的偏差（例如，对于守恒律，其残差应为零），被直接整合到神经网络（NN）的损失函数中，并与任何给定的数据（例如，表面的部分压力测量值）进行适当的加权 。

PINNs并非旨在取代现有的CFD代码，事实上，当前这一代的PINN在求解标准正问题时，其精度或效率都不如高阶CFD代码 。这个限制与最小化损失函数有关，该损失函数是一个高维的非凸函数，这也是所有神经网络（甚至包括商业机器学习）面临的一个巨大挑战 。

**然而**，如果待研究的流动问题中存在任何零散的、局部的时空数据，PINN的性能会比任何CFD求解器都精确得多，也高效得多 。此外，PINN的正向和反向问题表述是完全相同的，因此不再需要过去那些昂贵的数据同化方案，正是这些方案阻碍了流动问题在优化和设计应用中的进展 。

在本文中，我们首先回顾PINN的基本原理以及近期使用区域分解方法在多物理场和多尺度流动问题上的扩展 。然后，我们展示了在圆柱体后的不可压缩流中形成的三维尾流的新结果 。我们还将展示一个二维超音速流绕过一个钝头体的结果，最后，我们在一个生物医学流动模拟中推断血栓变形的材料参数 。

> [!TIP]- 解释
> 
> 这段引言为我们整个学习之旅设定了舞台。它告诉我们，旧方法遇到了麻烦，而新方法应运而生。
> 
> ---
> 
> ## 旧方法 (CFD): 一位强大的老前辈，但毛病不少
> 
> 想象一下，你想预测F1赛车周围的空气是怎么流动的。在过去50年里，科学家们开发了一套非常强大的工具，叫做
> 
> **计算流体力学 (Computational Fluid Dynamics, CFD)** 。它就像一位武功高强的老前辈，能解开流体运动的终极秘籍——
> 
> **纳维-斯托克斯方程 (NSE)** 。
> 
> 但这位老前辈有几个很头疼的“怪癖”：
> 
> *
> 
> **挑食** 🍎: 它很难处理真实世界里那些乱七八糟、带噪声的测量数据 。你必须给它非常干净、完美的“食材”（边界条件），它才肯“下厨”。
> 
> *
> 
> **洁癖** 📐: 在计算前，你必须先把赛车和它周围的空间切成亿万个微小的网格。这个过程叫**网格生成**，对于复杂的F1赛车来说，这活儿极其耗时，而且非常依赖工程师的个人技术，堪称一门“艺术” 。
> 
> *
> 
> **死脑筋** 🤔: 它擅长“正问题”，也就是告诉你所有条件，让你算结果。但它不擅长**“逆问题”** 。比如，只告诉你赛车表面几个点的气压，让你反推出整个赛车周围的气流分布，甚至反推赛车的精确形状。这对CFD来说成本极高，几乎得重写一套完全不同的软件 。
> 
> *
> 
> **代码臃肿** 💻: 像`OpenFOAM`这样的CFD软件，有超过10万行代码 。维护和更新它，就像是维护一架有着无数零件的古董飞机，极其困难 。
> 
> ---
> 
> ## 新方法 (PINNs): 一位思想开放的后起之秀
> 
> 为了解决这些麻烦，Karniadakis教授的团队提出了一种新方法，叫做
> 
> **物理信息神经网络 (Physics-Informed Neural Networks, PINNs)** 。
> 
> PINN的核心理念是**“融合”**。它能把物理定律（那本武功秘籍）和乱七八糟的真实数据（比如不同传感器测来的数据）无缝地融合在一起，共同指导一个神经网络去学习 。
> 
> PINN有两招独门绝技：
> 
> #### **绝技1: 自动微分 (Automatic Differentiation)**
> 
> 为了避免“洁癖”问题，PINN彻底抛弃了网格。它是怎么做到的呢？通过
> 
> **自动微分** 。物理方程里充满了各种导数（比如速度的变化率就是加速度）。神经网络本身就是一个由很多简单函数搭起来的超级复杂函数。自动微分就像是给这个网络装了一个“微积分插件”，你只要问它任何一点对于输入的导数是多少，它都能瞬间、精确地告诉你。因为它能直接算导数，所以根本就不再需要把空间切成小网格去做近似了 。
> 
> #### **绝技2: 物理信息损失函数 (Physics-Informed Loss Function)**
> 
> 这是PINN的“大脑”和“导航系统”。神经网络学习的目标，就是让一个叫做**“损失函数”**的东西变得尽可能小。PINN的损失函数是一个“复合评分系统”：
> 
> *
> 
> **物理得分**: 把神经网络预测出的流场，代入到物理方程（NSE）里。如果方程不成立（比如能量不守恒了），就给出一个很高的“惩罚分” 。
> 
> *
> 
> **数据得分**: 把神经网络的预测值，和你手头上已有的、稀疏的真实测量数据做比较。如果差得太远，也给一个很高的“惩罚分” 。
> 
> 神经网络在训练时，会拼命调整自己，目的就是让总的惩罚分（物理分 + 数据分）变得最低。这样一来，它就自然而然地学到了一个既遵守物理规律，又符合真实数据的解。
> 
> ---
> 
> ## 定位: 是“神助攻”，不是“替代品”
> 
> 作者很诚实地告诉我们，PINN不是要取代CFD 。
> 
> * 在解决那种所有条件都给得清清楚楚的“标准正问题”时，目前最高级的CFD软件还是更准、更快 。
> 
> * PINN的挑战在于，它的损失函数是一个**“高维非凸函数”** 。你可以想象在一个无比巨大的、坑坑洼洼、山峦起伏的地图上找最低点，你很容易掉进一个小坑里就以为是最低点了，但其实真正的“世界之谷”还在别处。这是所有神经网络的共同挑战 。
> 
> **但PINN真正的厉害之处在于**：一旦问题变得不完美，比如你只有一些零散的测量数据，PINN的优势就碾压了CFD，它能做得更准、更快 。更棒的是，解决正问题和逆问题用的是同一套PINN框架，大大简化了工作 。
> 
> ---
> 
> ## 本文的“寻宝图”
> 
> 最后，作者给我们画了一张路线图。这篇文章会用三个超酷的案例来证明PINN的实力 ：
> 
> 1. **三维尾流**: 根据少量数据，重建出圆柱体后面复杂的三维漩涡 。
> 
> 2. **超音速流**: 重建出子弹头周围的超音速激波 。
> 
> 3. **生物流**: 推断出动脉里血栓的材料特性 。

>[!NOTE]- 有限元法、谱分析、无网格法的解释
> 当然可以。这三个方法都是求解复杂物理方程的“武功流派”，它们的核心思想都是**“化整为零，逐个击破”**，因为电脑没法直接解决无限复杂的连续问题。
> 
> 下面我用尽量通俗的比喻来解释它们：
> 
> ---
> 
> ## 有限元法 (Finite Element Method, FEM)
> 
> 这是应用最广、最主流的“武功”，像武林中的“名门正派”。
> 
> - **核心思想**: **用简单的“小积木”去搭建复杂的“大模型”**。
>     
> - **比喻**: 想象一下你想用乐高积木拼一个完美的球体。你没有球形的积木，但你有大量的小方块或小三角片。你可以用这些小积木一块一块地拼，积木用得越多、越小，你拼出来的形状就越接近一个真正的球体。
>     
> - **工作流程**:
>     
>     1. **划分网格 (Meshing)**: 首先，把你要研究的物体（比如一个飞机机翼）在电脑里切分成成千上万个互不重叠的、简单的“小积木”。这些“积木”就叫**“有限元”**，它们组成的这张大网就是**“网格”**。在二维中，积木通常是三角形或四边形；在三维中，是四面体或六面体。
>         
>     2. **单元分析**: 在**每一个**小积木内部，我们假设物理量（比如温度、压力）的变化规律非常简单，比如是线性的（像一个平直的斜坡）。这种简单的规律是电脑可以轻松求解的。
>         
>     3. **整体组装**: 把所有小积木上的简单解法在它们的连接点（**节点**）上“拼接”起来，确保在边界上是连续的。最终，这就形成了一个巨大的、但都是由简单加减乘除构成的方程组。
>         
>     4. **求解**: 电脑大力出奇迹，解开这个巨大的方程组，就得到了整个机翼上每个点的近似解。
>         
> - **优缺点**:
>     
>     - **优点**: 适应性极强，特别擅长处理外形极其不规则的物体（比如汽车发动机）。是工程计算领域的“万金油”。
>         
>     - **缺点**: 网格质量对计算结果影响巨大，“划分网格”这一步非常耗时，且需要很强的专业知识。
>         
> 
> ---
> 
> ## 谱方法 (Spectral Method)
> 
> 这是一种非常优雅、高效的“武功”，像武林中的“绝顶高手”。
> 
> - **核心思想**: **用一堆“完美的波”去叠加出任何复杂的形状**。
>     
> - **比喻**: 想象一下音乐。任何复杂的声音，比如小提琴拉出的一个音，都可以分解成一个基准音（基频）和一系列高频的泛音（谐波）的叠加。这些基准音和泛音都是非常简单的正弦波。谱方法就是用同样思路，把一个复杂的解函数看作是由许多简单的“基础波函数”（如正弦、余弦函数）叠加而成的。
>     
> - **工作流程**:
>     
>     1. **选择基函数**: 它不像有限元法那样把物体切成小块。而是在**整个物体范围**内，假设解可以由一组“全局”的、非常光滑的函数（**基函数**）线性组合而成。
>         
>     2. **确定系数**: 它的目标不是求出成千上万个网格点上的值，而是去求解叠加这些“基础波”时，每个波所占的“权重”或“振幅”（也就是系数）是多少。
>         
>     3. **求解**: 通过求解这些系数，就能得到一个在整个区域内都非常精确的、光滑的解。
>         
> - **优缺点**:
>     
>     - **优点**: 对于解本身比较光滑、研究区域形状比较规则（比如矩形、圆形）的问题，谱方法的**精度极高**，可以用比有限元少得多的计算量达到惊人的准确度。
>         
>     - **缺点**: 不擅长处理不规则外形和带有“突变”的解（比如冲击波），因为你需要无穷多个光滑的波才能凑出一个尖锐的角。
>         
> 
> ---
> 
> ## 无网格法 (Meshless Method)
> 
> 这是一种非常自由、灵活的“武功”，像武林中的“逍遥派”。
> 
> - **核心思想**: **彻底抛弃网格的束缚，每个点只关心自己的“朋友圈”**。
>     
> - **比喻**: 想象一个社交网络。有限元法里，每个点都和它在网格上的邻居有固定的连接关系。在无网格法里，没有这种固定的“连接”。每个点都是自由的，它会自己去寻找周围一定范围内的其他点，形成一个临时的“朋友圈”。它的行为（要求解的物理量）是由这个朋友圈里其他点共同影响决定的，离得越近的朋友影响越大。
>     
> - **工作流程**:
>     
>     1. **节点布置**: 不再需要费力地划分网格，只需要在研究区域内撒上一堆计算**节点**就行，哪里复杂就可以撒得密一点。
>         
>     2. **邻域影响**: 对于任何一个节点，它的解都是通过一个函数去拟合它周围“朋友圈”里所有节点的信息来构造的。
>         
>     3. **求解**: 基于这种“朋友圈”式的局部近似，建立并求解整个系统的方程。
>         
> - **优缺点**:
>     
>     - **优点**: 特别适合处理那些外形在剧烈变化的问题，比如模拟水花飞溅、爆炸、材料断裂等。在这些问题里，如果用网格，网格会发生严重扭曲导致计算失败，而无网格法只是点的移动，非常灵活。
>         
>     - **缺点**: 通常计算精度不如前两者，且计算量可能更大。
>         
> 
> |特性|有限元法 (FEM)|谱方法 (Spectral Method)|无网格法 (Meshless Method)|
> |---|---|---|---|
> |**核心比喻**|搭积木 🧱|调音（谐波叠加）🎶|社交网络 👨‍👩‍👧‍👦|
> |**基本单元**|局部、简单的几何体（单元）|全局、光滑的数学函数（波）|离散的点和其邻域|
> |**擅长领域**|复杂几何外形|简单几何、光滑解|大变形、断裂、爆炸|
> |**主要挑战**|网格划分|复杂几何、不光滑解|精度和计算效率|

> [!NOTE]- 解释 “we still cannot incorporate seamlessly (multi-fidelity) data into existing algorithms”
> 
> 首先，我们来翻译一下这两个核心词：
> 
> - **Fidelity (保真度/忠实度)**: 在这里，你可以直接把它理解为**“数据的质量”**或**“精确度”**。
>     
> - **Multi-fidelity (多保真度)**: 意思就是我们手头的数据**质量参差不齐**，有高质量的，也有低质量的。
>     
> 
> 所以，这句话的直白意思是：**“我们仍然无法将那些质量参差不齐的数据，天衣无缝地融合到现有的（传统CFD）算法里。”**
> 
> ---
> 
> #### 侦探破案的比喻 🕵️
> 
> 想象你是一位侦探，正在调查一个案子。你手头有各种各样的线索，这些线索的“可靠性”（也就是保真度）是不同的：
> 
> - **高保真度数据 (High-fidelity Data)**:
>     
>     - 一段4K超高清监控录像，清晰地拍到了嫌疑人的脸。
>         
>     - 实验室提取到的、与嫌疑人100%匹配的DNA样本。
>         
>     - **特点**: 极其可靠，信息量大，但**获取成本高昂，数量稀少**。你不可能在全城每个角落都装上4K摄像头。
>         
> - **低保真度数据 (Low-fidelity Data)**:
>     
>     - 一位老奶奶提供的模糊的口头描述：“我好像看到一个高个子男人跑过去了。”
>         
>     - 一张从很远处拍的、像素很低、几乎看不清人脸的手机照片。
>         
>     - 一个非常粗略的、根据脚印大小推测出的嫌疑人体重范围。
>         
>     - **特点**: 可靠性低，信息模糊，但**获取成本低，数量可以很多**。你可以轻松地询问几十个目击者。
>         
> 
> **“多保真度数据 (Multi-fidelity data)”** 就是指你同时拥有上面这两大类线索。
> 
> ---
> 
> #### 传统算法 (CFD) 的困境
> 
> 传统的CFD算法就像一个**“教条主义的老侦探”**。他的工作手册上写着：只接受“物证”（高保真数据）。
> 
> - 你给他一段高清录像，他能分析得头头是道。
>     
> - 但你给他一堆模糊的口述，他会觉得这些信息“不精确”、“没法用”，就直接把它们扔进垃圾桶。他不知道如何把这些模糊但可能有用的信息，和那段精确的视频结合起来，形成一个完整的证据链。
>     
> - **这就是“无法无缝整合 (cannot incorporate seamlessly)”**。这个老侦探的体系里，没有融合不同质量证据的机制。他要么只用高质量的，要么就干脆不用。
>     
> 
> #### PINN 的高明之处
> 
> PINN就像一个**“聪明的AI神探”**。他破案的逻辑是：
> 
> 1. **尊重所有证据**: 他既看重那段宝贵的4K录像（高保真数据），也仔细倾听每一位目击者的模糊描述（低保真数据）。
>     
> 2. **内心有准则**: 最重要的是，他的大脑里装着一整套“犯罪心理学和行为逻辑”（这就是**物理定律/方程**）。
>     
> 3. **融合推理**: 他开始推理：“根据行为逻辑（物理定律），嫌疑人逃跑路线应该是这样的。这个路线和4K视频（高保真数据）完美匹配。同时，虽然那些口述（低保真数据）很模糊，但大体方向也都指向了这条路。因此，我非常有信心锁定嫌疑人的完整轨迹！”
>     
> 
> PINN就是这样，它利用神经网络的灵活性，将少量精确的“高保真数据”、大量模糊的“低保真数据”和“物理定律”这三者**同时**放进一个锅里炖。物理定律为数据的融合提供了框架和准则，确保最终的结果是符合逻辑、自洽的。
> 
> ---
> 
> **回到流体力学：**
> 
> - **高保真数据**: 几次用超级计算机花费数周时间算出的精确流场结果，或者在昂贵的风洞实验室里用高精度激光测速仪测得的几个点的数据。
>     
> - **低保真数据**: 用个人电脑跑的一个简化模型的粗略结果，或者在流场中随意撒下的一堆廉价传感器测得的带噪声的数据。
>     
> 
> 传统CFD很难同时利用这两者。而PINN可以，它用物理方程作为“粘合剂”，把这些质量参差不齐的数据完美地粘合在一起，得到一个远比只用其中一种数据更全面、更准确的结果。这就是它强大的**“数据融合”**能力。

> [!NOTE]- 段落解释 “Physics-informed learning [7], introduced in a series of papers by Karniadakis’s group both for Gaussian-process regression [8, 9] and physics-informed neural networks (PINNs) [10], ...”
> 好的，这句话确实信息密度很高，因为它一次性引出了这个领域的“门派”、“创始人”、“两大武功”以及它们的“应用范围”。
>
>我们把它彻底拆解，做一个详细的“名词解析”。
>
> ### 句子解析
> 
> **原句**: "Physics-informed learning [7], introduced in a series of papers by Karniadakis’s group both for Gaussian-process regression [8, 9] and physics-informed neural networks (PINNs) [10], can seamlessly integrate multifidelity/multimodality experimental data with the various Navier-Stokes formulations for incompressible flows [11, 12] as well as compressible flows [13] and biomedical flows [14]."
> 
> **一句话大白话翻译**: 由Karniadakis团队开创的一种叫做“物理信息学习”的新方法，可以用两种不同的技术（高斯过程回归 或 PINN），非常顺滑地把各种不同类型、不同质量的实验数据，和各种情况下的流体力学公式（比如不可压缩、可压缩、生物流）结合起来解决问题。
> 
> ---
> 
> ### “名词小词典” 📖
> 
> #### 1. **Physics-informed learning (物理信息学习)** 
> 
> - 这不是一个具体的算法，而是一种**指导思想**或**学习范式**。
>     
> - **核心理念**: 任何机器学习模型在学习数据时，都**必须**同时被告知、并遵守相关的物理定律。
>     
> - **比喻**: 你在训练一个机器人学打篮球。
>     
>     - **传统机器学习**: 你给它看一万个小时的NBA比赛录像（数据），让它自己模仿。
>         
>     - **物理信息学习**: 你不仅给它看录像（数据），还预先在它的程序里写进了牛顿力学和空气动力学方程（物理信息）。这样，它在模仿的同时，还能根据物理定律进行“脑补”和“预判”，比如计算出最优的投篮角度和力度。它的学习会更高效，表现也更合理。
>         
> 
> #### 2. **Karniadakis’s group (卡尼亚达基斯的研究组)**
> 
> - 你可以把他们理解为开创“物理信息学习”这个“门派”的**“一代宗师”**。这篇论文的通讯作者George Em Karniadakis就是这个领域的领军人物。
>     
> 
> #### 3. **Gaussian-process regression (高斯过程回归)** 
> 
> - 这是“物理信息学习”门派下的**两大武功之一**，是一种基于统计学的方法。
>     
> - **核心思想**: “带不确定性的曲线拟合”。
>     
> - **比喻**: 想象你在纸上有几个确定的点，让你把它们连起来。
>     
>     - 普通的方法可能只会画出一条确定的线。
>         
>     - **高斯过程回归**则会画出一条“最可能”的曲线，同时在这条曲线周围画出一个“阴影区域”（置信区间）。这个阴影区域告诉你：“真实的曲线很可能落在这个范围里，越靠近中心线可能性越大，越往外可能性越小。”
>         
>     - 它不仅给出了预测，还给出了对**预测的不确定性**的度量，这在科学计算中非常有用。
>         
> 
> #### 4. **Physics-informed neural networks (PINNs)** 3
> 
> - 这是该门派的**另一大武功**，也是我们这篇论文的主角，是基于深度学习的方法。它的原理我们之前已经详细讨论过了：用一个神经网络去逼近一个函数，其损失函数同时包含对物理方程的惩罚和对数据的惩罚。
>     
> 
> #### 5. **Multimodality experimental data (多模态实验数据)**
> 
> - 这个词和我们之前讨论的`多保真度 (Multi-fidelity)`数据相关，但侧重点不同。`Multi-fidelity`强调的是**数据质量**有高有低，而`Multimodality`强调的是**数据类型**多种多样。
>     
> - **核心思想**: 数据来自不同的“感官”或测量方式。
>     
> - **比喻**: 医生给病人看病，会用到“望闻问切”，这就是一个典型的多模态诊断：
>     
>     - **望**: 看CT影像、看脸色。（视觉数据）
>         
>     - **闻**: 用听诊器听心跳和呼吸。（听觉数据）
>         
>     - **问**: 询问病人的主观感受。（文本/语言数据）
>         
>     - **切**: 量血压、测体温、做血常规化验。（数值数据）
>         
> - 在流体力学里，`多模态数据`就可能包括：用**压力传感器**测到的压力值（点状数值数据）、用**温度计**测到的温度（点状数值数据）、以及用**高速摄像机**拍下的烟雾流动轨迹照片（图像数据）。PINN可以将这些完全不同类型的数据融合在一起使用。
>     
> 
> #### 6. **Navier-Stokes formulations (纳维-斯托克斯方程的多种形式)**
> 
> - 这是流体力学的“根本大法”。但根据不同的应用场景，这个“大法”会有一些变体和简化版。
>     
> 
> #### 7. **Incompressible flows, Compressible flows, Biomedical flows**
> 
> - 这就是“根本大法”应用的几个具体“场景”：
>     
>     *
>     
>     **不可压缩流 (Incompressible flows)**: 比如常温下的水在管道里流动，其密度基本不变 。
>     
>     *
>     
>     **可压缩流 (Compressible flows)**: 比如空气在超音速飞机周围流动，其密度会发生剧烈变化 。
>     
>     *
>     
>     **生物医学流 (Biomedical flows)**: 比如血液在血管里流动，它既是流体，又包含各种细胞，是一种非常复杂的流动。
>     
> 
> ---
> 
> ### 总结
> 
> 所以，这句话合起来就是在说：**Karniadakis团队开创的“物理信息学习”这套思想体系，可以通过统计学方法（高斯过程）或深度学习方法（PINN），将来自不同传感器类型（多模态）、质量有好有坏（多保真度）的各种数据，与适用于水流（不可压缩）、声波（可压缩）、血液（生物流）等不同场景的物理规则相结合，从而解决极其复杂的流体力学问题。**

> [!NOTE]- 解释 “This limitation is associated with the minimization of the loss function, which is a high-dimensional non-convex function”
> 当然，这又是一个非常核心且关键的概念，它解释了为什么神经网络（包括PINN）的训练有时候会那么困难。
>
>我们还是用一个生动的比喻来把它彻底讲明白。
>
> ### 解释 “高维非凸损失函数 (high-dimensional non-convex loss function)”
> 
> 首先，我们回顾一下：训练神经网络的目标，就是去**最小化 (minimization)** 它的**损失函数 (loss function)**。损失函数就像一个“惩罚计分器”，分数越低，说明神经网络的预测结果就越好、越符合物理规律。
> 
> 那么，这个损失函数的“形状”是怎样的呢？这正是问题的关键。
> 
> ---
> 
> #### 想象一个“惩罚地貌”
> 
> 我们可以把损失函数想象成一个巨大的、真实的地形地貌。
> 
> - **你的位置 (Location)**：由神经网络中**所有参数**（成百万上千万个权重和偏置）的当前取值决定。你调整任何一个参数，就相当于在这个地貌上移动了一步。
>     
> - **你的海拔 (Altitude)**：就是你当前位置对应的**损失函数的值**（也就是惩罚分数）。海拔越高，分数越高，代表结果越差。
>     
> 
> 神经网络的训练过程，就好比一个**被蒙上眼睛的登山者**，他的任务是在这个广阔无垠的“惩罚地貌”上，找到**海拔最低的那个点**。
> 
> #### “高维 (High-dimensional)” 是什么意思？
> 
> - 我们熟悉的地形是三维的，我们可以在“前后”和“左右”两个维度上移动，来决定我们的位置。
>     
> - 但神经网络的参数有**上百万个**。所以这个“惩罚地貌”也有**上百万个维度**。我们的大脑无法想象百万维度的空间，但对于蒙着眼的登山者来说，他只是感觉自己有上百万个可以移动的方向。
>     
> 
> #### 关键区别：“凸” vs “非凸”
> 
> 现在到了最核心的概念，“地貌”的形状。
> 
> ##### 1. **凸函数 (Convex Function) - 简单模式**
> 
> - 一个“凸”的“惩罚地貌”，形状就像一个**完美的、光滑的碗**。
>     
> - **特点**: 它只有一个最低点，而且没有任何小坑洼。
>     
> - 对于蒙着眼的登山者来说，任务太简单了！他只需要遵循一个简单的规则：“**永远朝脚下最陡的方向往下走**”，就**保证**能走到那个唯一的、全局最低点。
>     
> 
> ##### 2. **非凸函数 (Non-convex Function) - 困难模式**
> 
> - 不幸的是，几乎所有深度神经网络（包括PINN）的“惩罚地貌”都是**“非凸”**的。它不像一个碗，而更像一个**极其复杂的真实山脉**，有无数的山峰、山谷、盆地、洼地和丘陵。
>     
> - **特点**:
>     
>     - **局部最优解 (Local Minimum)**: 想象你走到了一个山谷的谷底。对你来说，四周所有方向都是上坡路，你感觉自己已经到了最低点。但这个山谷可能只是整个山脉中一个很不起眼的小坑。
>         
>     - **全局最优解 (Global Minimum)**: 在遥远的地方，可能有一个像“马里亚纳海沟”一样、整个地貌中真正的、唯一的最低点。
>         
> 
> **这就是困难所在**：我们那个蒙着眼的登山者（训练算法），遵循着“永远下坡”的原则，很大概率会走到一个**局部最优解**（某个小山谷）里，然后就“卡住”了。因为他环顾四周都是上坡，他以为自己成功了，但他完全不知道，翻过旁边那座高山，还有一个深得多的“大峡谷”（全局最优解）在等着他。
> 
> ---
> 
> ### 总结
> 
> 所以，原句 **“This limitation is associated with the minimization of the loss function, which is a high-dimensional non-convex function”** 的意思就是：
> 
> **PINN（以及其他深度学习模型）的一个核心局限性，来源于它的优化过程。这个过程就如同让一个蒙眼登山者，在一个拥有百万个方向的、地形极其复杂（非凸）的山脉上寻找最低点。他非常容易被某个局部的小山谷（局部最优解）困住，而找不到那个真正代表完美答案的、最深的峡谷（全局最优解）。**
> 
> 这就解释了为什么有时候PINN的训练结果不够精确——它可能只是找到了一个“还不错”的解，而不是那个“最完美”的解。如何有效地跳出“局部最优”，去寻找“全局最优”，是整个AI领域都在努力解决的重大挑战。


## 2 PINNs: 物理信息神经网络

在本节中，我们首先回顾PINN的基本概念，然后讨论其在不可压缩流、可压缩流和生物医学流中的最新进展。

### 2.1 PINNs: 基本概念

我们考虑一个由以下形式给出的参数化偏微分方程（PDE）系统：
$$
\begin{equation}
  \begin{array}{l}
    &f(x, t, \hat{u}, \partial_x\hat{u}, \cdots, \lambda)=0, \ x\in \Omega, \ t\in[0,T] \\

    &\hat u(x,t_0​)=g_0​(x),\  x\in\Omega \tag{1} \\

    &\hat u(x,t)=g\Gamma(t),\ x\in \partial \Omega,\ t∈[0,T] \\
  \end{array}
\end{equation}
$$
其中，$x\in \mathbb{R}^d$ 是空间坐标，$t$ 是时间；$f$ 表示偏微分方程的残差，包含各种微分算子（即 $[\partial_x\hat u,\partial_t\hat u,\cdots]$）；$\lambda=[\lambda_1,\lambda_2,\cdots]$ 是偏微分方程的参数；$\hat u(x,t)$ 是该偏微分方程的解，其初始条件为 $g_0​(x)$，边界条件为 $g\Gamma​(t)$（可以是狄利克雷、诺伊曼或混合边界条件）；$\Omega$ 和 $\partial \Omega$ 分别代表空间域和边界。

在最初的 PINN 的背景下，一个由多个隐藏层组成的全连接前馈神经网络，被用来逼近偏微分方程的解 $\hat u$。该网络将空间和时间坐标 $(x,t)$ 作为输入，如**图1**的蓝色面板所示。设第 $k$ 个隐藏层的隐藏变量为 $z^k$，则该神经网络可以表示为：
$$
\begin{array}{l}
&z^0=(x,t)\\

&z^k=\sigma(W^kz^{k−1}+b^k),\ 1\le k\le L−1 \tag{2}\\

&z^L=W^Lz^{L−1}+b^L, \ k=L\\
\end{array}
$$
其中最后一层的输出被用来近似真实解，即 $\hat u\approx z^L$。$W^k$ 和 $b^k$ 表示第 $k$ 层的权重矩阵和偏置向量；$\sigma(\cdot)$ 是一个非线性激活函数。所有可训练的模型参数，即权重和偏置，在本文中用 $\theta$ 表示。

在PINN中，求解一个偏微分方程系统（如方程(1)所示）被转化为了一个优化问题，通过迭代更新 $\theta$ 来最小化损失函数 $L$：
$$
\begin{array}{l}
L=\omega_1​L_{PDE}​+\omega_2​L_{data}​+\omega_3​L_{IC}​+\omega_4​L_{BC} \tag{3}
\end{array}​
$$
其中 $\omega_{1−4}$​ 是不同损失项的权重系数。方程(3)中的第一项 $L_{PDE}​$ 惩罚控制方程的残差。其他项则用于约束模型预测满足测量值（$L_{data}$​）、初始条件（$L_{IC}$​）和边界条件（$L_{BC}$​）。通常，我们采用均方误差（MSE），即取采样点上 L2​ 范数，来计算方程(3)中的各项损失。采样点被定义为一个数据集 $\{x^i,t^i\}_{i=1}^N$，其中不同损失项的采样点数（用$N$表示）可以不同。通常，我们使用ADAM优化器（一种基于梯度的自适应一阶优化算法）来优化模型参数 $\theta$。

**备注1**: 我们注意到，方程(3)中所示的损失函数的定义是与具体问题相关的，因此对于不同类型的问题，某些项可能会消失。例如，当我们求解一个流体力学正问题时，如果偏微分方程的参数 ($\lambda$) 和初始/边界条件都是已知的，那么数据损失 $L_{data}​$ 就不是必须的。然而，在模型参数或初始/边界条件未知的情况下（即逆问题），则必须考虑测量数据，以使该优化问题可解。我们还注意到，PINN框架可以用来求解一个“超定”系统，例如，一个在经典意义上初始和边界条件都已知、因而适定的问题，但同时在域内或边界上还有一些额外的测量值（例如压力测量）。

构建 $L_{PDE}​$ 损失项的关键步骤之一是计算偏导数，这通过使用 **自动微分（AD）** 来解决。AD依赖于链式法则，将一系列操作的导数组合起来，直接在计算图中计算输出相对于网络输入的导数。偏导数的计算可以得到一个明确的表达式，因此避免了传统数值近似中引入的截断误差。目前，AD已在各种深度学习框架中实现，这为PINN的开发提供了便利。
![[week1_paperlearning2.png]]

_**图 1：物理信息神经网络（PINN）的示意图。**  
一个**全连接神经网络**，以时间和空间坐标（$t,x)$ 为输入，用来逼近多物理场问题的解 $\hat{u} = [u, v, p, \phi]$。这个神经网络输出的 $\hat{u}$ 对输入变量的导数，是通过 **自动微分（AD）** 的方法计算出来的。这些导数被用来在损失函数中构建 **控制方程的残差项**，该损失函数通常由多个被不同系数加权的项组成。神经网络的参数 $\theta$ 和未知的偏微分方程参数 $\lambda$ 可以通过最小化损失函数来同时被学习。_

**图1**展示了PINN的示意图，其中的关键元素（如神经网络、AD、损失函数）用不同颜色标出。这里，我们考虑一个多物理场问题，其解包括速度 ($u, v$)、压力 $p$ 和一个标量场 $\phi$，它们在一个偏微分方程系统 $f$ 中相互耦合。**图1**中的示意图代表了流体力学中大多数典型问题。例如，这里考虑的偏微分方程可以是纳维-斯托克斯方程的布辛涅斯克近似，其中 $\phi$ 是温度。遵循**图1**的范式，我们将在本文余下部分逐个案例地描述PINN的控制方程、损失函数和神经网络配置。

> [!TIP]- 解释
> 
> 好的，现在我们来详细拆解PINN的内部工作原理。你可以把整个过程想象成**“训练一个AI学生去解答一道复杂的物理题”**。**图1** 就是这个AI学生的“学习流程图”。
> 
> ---
> 
> ### 1. 物理题本身 (The Physics Problem - Eq. 1)
> 
> 首先，我们要定义题目。任何一个流体物理问题，都可以被三条规则所描述：
> 
> - $f(...)=0$：**核心物理定律**。这就是最主要的“游戏规则”，比如牛顿定律、能量守恒定律等。这里的`f`就代表这些定律的数学形式（即偏微分方程PDE），它规定了在任何时间、任何地点，流体的各个物理量之间必须满足的关系。
>     
> - $\hat u(x,t_0​)=g_0​(x)$：**初始条件 (Initial Condition, IC)**。这规定了“游戏开始”时（$t_0$​ 时刻），整个场地（$\Omega$）是什么样的。
>     
> - $\hat u(x,t)=g\Gamma(t)$：**边界条件 (Boundary Condition, BC)**。这规定了在“游戏场地”的边界（$\partial \Omega$）上必须遵守的规则。比如，流体不能穿透管道的墙壁。
>     
> 
> 在这里，$\hat u$ 就是我们想要求的答案（比如速度、压力等），而 $\lambda$ 可能是一些未知的物理参数（比如流体的粘性系数）。
> 
> ---
> 
> ### 2. AI学生的大脑 (The Neural Network - 蓝色框)
> 
> 这个AI学生的核心就是**图1**蓝色框里的**神经网络**。
> 
> - **它的任务**：学习成为一个“万能函数” $\hat u(x,t)$。你给它任何一个时间和空间坐标 $(t, x)$，它就能告诉你那个点的物理量（速度$u,v$，压力$p$等）应该是多少。
>     
> - **它的结构**：它由很多层神经元（图中的小圆圈）组成。信息从左边的输入层 $(t,x)$ 开始，经过中间的“隐藏层”，最终到达右边的输出层，得到答案 $\hat u$。
>     
> - **学习的关键**：每一层之间的连接线都有**权重 ($W$)** 和**偏置 ($b$)**，你可以把它们想象成这个大脑里亿万个可以调节的“旋钮”。AI学生的“学习”，就是不断调整这些“旋钮”，来让自己的解答能力越来越强。所有这些旋钮参数合起来，就用一个符号 $\theta$ 表示。
>     
> 
> ---
> 
> ### 3. “物理外挂”：自动微分 (AD - 黄色框 )
> 
> 物理定律 $f$ 里有很多微分项（比如速度对时间的变化率是加速度）。传统方法需要划分网格来近似计算这些微分，很麻烦。但PINN的AI学生有一个“神器”。
> 
> - **自动微分 (AD)**：它就像一个内置的“微积分计算器”。因为神经网络本身就是一个巨大的数学函数，AD可以利用链式法则，**精确而快速**地计算出网络输出 $\hat u$ 相对于输入 $(t,x)$ 的任何阶导数。
>     
> - **作用**：有了这个神器，AI学生在给出答案 $\hat u$ 的同时，还能瞬间算出这个答案对应的所有“变化率”，从而可以直接拿到物理定律 $f$ 里去检验，完全不需要网格。
>     
> 
> ---
> 
> ### 4. “考试评分系统”：损失函数 (Loss - 绿色框)
> 
> AI学生做得好不好，需要一个评分标准，这就是**损失函数 $L$**（也叫“惩罚函数”）。这个总分由四门“科目”的加权平均分构成：
> $$
> L=\omega_1​L_{PDE}​+\omega_2​L_{data}​+\omega_3​L_{IC}​+\omega_4​L_{BC}​
> $$
> - $L_{PDE}$​ (**物理科目**): 把AI的答案 $\hat u$ 和它通过AD算出的“变化率”代入到物理定律 $f$ 里。如果定律成立（$f=0$），这科就得满分（惩罚为 $0$）。如果不成立，差得越多，扣分越多。
>     
> - $L_{IC}​$ (**历史科目**): 检查AI的答案在“游戏开始”时，是否符合初始条件。不符合就扣分。
>     
> - $L_{BC}​$ (**地理科目**): 检查AI的答案在“场地边界”上，是否遵守了边界规则。不遵守就扣分。
>     
> - $L_{data}​$ (**实验科目**): 如果我们有一些真实的实验测量数据，就拿来和AI的答案做对比。如果AI的预测和真实数据差得远，就扣分。
>     
> 
> $\omega$ 就是每门科目的**权重**，告诉AI学生哪门课更重要。
> 
> ---
> 
> ### 5. “学霸的迭代之路”：优化循环 (The Loop)
> 
> AI学生的目标就是让总分 $L$ 越低越好。它是这样“迭代学习”的：
> 
> 1. **答题**: 随便给出一版答案 $\hat u$。
>     
> 2. **算分**: 把答案代入上面的“评分系统”，得到一个总分 $L$。
>     
> 3. **反思**: 计算总分 $L$ 对所有“旋钮” $\theta$ 的梯度（$\partial L/\partial \theta$）。这个梯度会告诉它：“往哪个方向调旋钮，总分下降得最快”。如果物理参数 $\lambda$ 也未知，就一起调整。
>     
> 4. **进步**: 按照梯度的指示，微调一下所有的“旋钮”。
>     
> 5. **循环**: 回到第1步，用新的旋钮设置再答一次题。如此循环成千上万次，直到总分 $L$ 低到一个可以接受的程度 ($\lt \epsilon$)，学习结束。
>     
> 
> 此时，这个AI学生（神经网络）就已经学会了求解这道物理题，成为了一个合格的“物理学家”。

> [!NOTE]- 边界条件
>
>我们还是用一个简单的生活实例来把这三个概念彻底讲清楚。
>
> ### 边界条件 (Boundary Condition)
> 
> 想象一下，你要用电脑模拟一个房间里的温度分布。这个房间就是你的“研究区域”，而房间的**墙壁、天花板和地板**就是**“边界”**。
> 
> **边界条件**，就是你告诉电脑，在这些墙壁、天花板和地板上，温度应该遵守什么样的规则。这些规则决定了热量是如何与外界交换的，从而最终决定了整个房间的温度分布。
> 
> **狄利克雷 (Dirichlet)**、**诺伊曼 (Neumann)** 和 **混合 (Mixed)** 就是三种不同类型的“墙壁规则”。
> 
> ---
> 
> #### 1. 狄利克雷边界条件 (Dirichlet) — “恒温墙”
> 
> 这是最直接的一种规则：**直接指定边界上的确切数值**。
> 
> - **生活场景**：
>     
>     - 你把房间的左边墙壁整个贴上了巨大的冰块，强制让这面墙的温度**恒定在0°C**。
>         
>     - 同时，你把右边墙壁改造成了一个巨大的发热板，强制让它的温度**恒定在50°C**。
>         
> - **你设定的规则就是**：
>     
>     - 在左墙 (x=0) 上，温度 T = 0。
>         
>     - 在右墙 (x=L) 上，温度 T = 50。
>         
> - **一句话总结**：狄利克雷条件直接告诉你边界“**是什么**”（What it **IS**），即它的值为多少。
>     
> 
> ---
> 
> #### 2. 诺伊曼边界条件 (Neumann) — “保温墙” 或 “加热墙”
> 
> 这种规则不直接告诉你边界的温度，而是告诉你**通过边界的热量流速**。在数学上，这个“流速”就对应着温度的**“变化率”或“导数”**。
> 
> - **生活场景**：
>     
>     - **“保温墙” (零流速)**：你把天花板换成顶级的真空绝热材料，确保**没有任何热量**能从天花板流进或流出。这时，通过天花板的热流速率为 **0**，即温度在垂直于天花板方向上的**变化率为0**。
>         
>     - **“加热墙” (恒定流速)**：你把地板改造成一个地暖系统，它以一个**恒定的功率**（比如每平方米100瓦）向上加热。你不知道地板的具体温度是多少（也许靠近门口的地方凉一点，靠近窗户的地方热一点），但你知道它整体向房间输送热量的**速率是固定的**。这个固定的速率，就规定了温度在垂直于地面方向上的**变化率是一个常数**。
>         
> - **你设定的规则就是**：
>     
>     - 在天花板上，温度变化率 ∂T/∂n = 0。
>         
>     - 在地板上，温度变化率 ∂T/∂n = C (一个与加热功率有关的常数)。
>         
> - **一句话总结**：诺伊曼条件告诉你边界上的值“**变化得有多快**”（How fast it **CHANGES**），即它的导数/梯度是多少。
>     
> 
> ---
> 
> #### 3. 混合边界条件 (Mixed) — “组合墙”
> 
> 顾名思义，就是把上面两种规则组合起来用。在一个复杂的问题中，不同的边界可以有不同的规则。
> 
> - **生活场景**：
>     
>     - 你家的房间，靠左的墙是贴着冰块的“**恒温墙**”（狄利克雷条件），而天花板是“**保温墙**”（诺伊曼条件）。
>         
> - **你设定的规则就是**：在整个房间的边界上，一部分区域遵守狄利克雷规则，另一部分区域遵守诺伊曼规则。
>     
> 
> ### 总结表格
> 
> |条件类型|规定了什么|生活比喻|核心问题|
> |---|---|---|---|
> |**狄利克雷 (Dirichlet)**|边界上的**确切数值** (e.g., T=50°C)|恒温墙|“这里的值**是**多少？”|
> |**诺伊曼 (Neumann)**|边界上的**通量/变化率** (e.g., ∂T/∂n=C)|保温墙 或 恒功率加热墙|“这里的值**变化得有多快**？”|
> |**混合 (Mixed)**|以上两者的组合|真实的房间（有窗有墙有门）|组合应用|

> [!NOTE]- 解释 “均方误差 (MSE)” 和 “L2范数 (L2-norm)”
> 好的，这句话是描述我们如何“量化”惩罚分数的，也就是“扣分”的具体计算方法。它看起来复杂，但背后的思想非常直观。
>
>我们还是用一个具体的例子来把它讲明白。
> 
> **一句话概括**: 这句话是在说，我们用一种叫做**“均方误差 (MSE)”** 的方法来计算神经网络的“总扣分”。这个方法在数学上和计算一个叫**“L2范数”**的东西有关。
> 
> ---
> 
> #### 射箭比赛的类比 🎯
> 
> 想象你是一个射箭运动员，正在进行一场比赛。你的任务是射中靶心。
> 
> - **靶心 (Bullseye)**：代表着**“真实解”** (True Value)，也就是物理问题的正确答案。
>     
> - **你的箭射中的位置**：代表着**“神经网络的预测解”** (Predicted Value)。
>     
> 
> 我们怎么评判你这一箭射得好不好呢？
> 
> #### 1. 误差 (Error)
> 
> 最直观的评判标准，就是你的箭离靶心有多远。这个**“距离”**，就是**误差 (Error)**。
> 
> #### 2. L2范数 (L2-norm) - “我们熟悉的距离公式”
> 
> 这个听起来高大上的“L2范数”，其实就是我们初中就学过的**勾股定理**和**两点间距离公式**。
> 
> - 如果靶心在坐标原点 $(0,0)$，你的箭射在了点 $(x,y)$上，那么误差距离就是 $\sqrt{x^2+y^2}$。
>     
> - 这个$\sqrt{x^2+y^2}$ **就是**从原点到 $(x,y)$ 的**L2范数**。它就是我们最熟悉的直线距离。
>    
> 
> #### 3. 方误差 (Square Error)
> 
> 在机器学习中，我们通常不喜欢带根号的计算，而且我们想让“惩罚”更有效力。所以我们不直接用距离，而是用**距离的平方**来作为单次射击的“扣分”，也就是**方误差 (Square Error)**。
> 
> - $单次扣分 = (误差距离)^2=(\sqrt{x^2+y^2})^2=x^2+y^2$
>     
> 
> 这样做有两个巨大的好处：
> 
> - **结果恒为正**：平方一下，不管你是往上射偏了还是往下射偏了，扣分都是正数。
>     
> - **重罚离谱的错误**：
>     
>     - 你射偏了2厘米，扣分是 22=4分。
>         
>     - 你射偏了10厘米，扣分是 102=100分！
>         
>     - 你看，虽然距离只差了5倍，但扣分却差了25倍。这会迫使神经网络（运动员）拼命避免那些错得离谱的预测。
>         
> 
> #### 4. 均方误差 (Mean Square Error, MSE)
> 
> 你一共射了N箭（比如100箭），我们怎么计算你的总成绩呢？我们把每一次的“扣分”（方误差）加起来，再除以总次数N，得到一个**平均扣分**。这就是**均方误差 (MSE)**。
> $$
> MSE=\frac{1}{N}\sum_{i=1}^N​(第i次射击的误差)^2
> $$
> 这个MSE分数，就成为了我们损失函数中一项的具体数值（比如 $L_{data}$​ 或 $L_{BC}​$）。
> 
> ---
> 
> ### 总结
> 
> 所以， **“In general, the mean square error (MSE), taking the L2-norm of the sampling points, is employed to compute the losses in equ. 3”** 这句话的完整流程是：
> 
> 1. 我们在很多**采样点**（`sampling points`，可以想象成比赛中射了N箭）上进行比较。
>     
> 2. 在每个点上，我们计算**误差 (Error)**，也就是“预测值”和“真实值”的差距。这个差距向量的长度，就是**L2范数**。
>     
> 3. 我们计算这个误差的**平方 (Square)**，这相当于L2范数的平方，也放大了惩罚。
>     
> 4. 我们把所有采样点上的“方误差”加起来，求一个**平均值 (Mean)**。
>     
> 5. 最后得到的这个**均方误差 (MSE)**，就是我们用来衡量神经网络表现的“最终扣分”。这个分数越小，说明神经网络预测得越准。

> [!NOTE]- 解释ADAM优化器
> 好的，我们来深入探讨一下 **ADAM优化器 (ADAM optimizer)**。
>
>在之前的比喻中，我们说神经网络的训练过程就像一个**“蒙着眼的登山者”**，在复杂的“惩罚地貌”上寻找海拔最低的谷底。
>
>**优化器**，就是这位登山者所采用的**“下山策略”**。它决定了登山者在每一步应该朝着哪个方向走，以及步子应该迈多大。
>
>ADAM是一种非常流行且高效的“下山策略” 。
> ---
> 
> #### 1. 最基础的策略：梯度下降 (Gradient Descent)
> 
> 在认识ADAM之前，我们先看看最简单的策略是怎样的。
> 
> - **策略**: “永远朝着当前脚下最陡峭的方向往下走一步”。这个“最陡峭的方向”就是数学上的**梯度 (Gradient)**。
>     
> - **步长 (Learning Rate)**: 每一步迈多大，是由一个叫做**“学习率”**的参数决定的。
>     
> 
> **这个基础策略的缺点很明显**：它太“死板”了。
> 
> - 如果学习率（步长）太大，在到达谷底时，它可能会因为步子太大而一步跨到山谷对面，永远在谷底附近来回震荡，无法到达最低点。
>     
> - 如果学习率太小，那么它下山的速度会非常非常慢，可能要花上天文数字的时间才能走到谷底。
>     
> 
> 这就像一个只会用同一种步长走路的人，无论在平地还是在悬崖边，他每一步都迈出一米，这显然不聪明。
> 
> ---
> 
> #### 2. 更聪明的策略：ADAM优化器
> 
> ADAM (Adaptive Moment Estimation) 是一种**自适应**的优化算法，它比基础的梯度下降要智能得多。你可以把它想象成一个经验丰富的“专业登山者”，他有两件法宝：
> 
> ##### **法宝一：动量 (Momentum) - “滚下山的雪球”**
> 
> ADAM在决定下一步往哪走时，不仅仅看当前脚下的坡度，它还会“回忆”一下自己刚刚是从哪个方向过来的。
> 
> - **比喻**: 想象一个雪球从山上滚下来。它不仅会顺着当前的坡度滚，还会因为自身的**惯性（动量）**继续往前冲。
>     
> - **作用**:
>     
>     1. **加速下降**: 如果雪球连续地朝着同一个方向滚动（梯度方向很稳定），它的速度会越来越快，从而能更快地到达谷底。
>         
>     2. **冲出小坑**: 如果雪球滚进了一个很浅的小坑（局部最优解），它可能会因为自身的惯性而直接“冲”出这个小坑，继续寻找更深的山谷。
>         
>     3. **减少摇摆**: 在一个狭窄的山谷里，普通的梯度下降可能会在两壁之间来回摇摆，而带有动量的雪球会抑制这种摇摆，更稳定地沿着山谷的中心线前进。
>         
> 
> ##### **法宝二：自适应学习率 (Adaptive Learning Rate) - “因地制宜的步长”**
> 
> 这是ADAM最强大的地方。它会为神经网络中**每一个独立的参数**（我们之前比喻成“旋钮”）都智能地调整其独有的学习率（步长）。
> 
> - **比喻**: 登山者在不同的路面上会使用不同的步长。在平坦、开阔的下坡路上，他会大步流星；在陡峭、狭窄的碎石路上，他会小心翼翼地迈着小碎步。
>     
> - **作用**: ADAM会追踪每一个参数的“历史表现”。
>     
>     - 对于那些**梯度一直很稳定、更新很频繁**的参数（说明正走在一条“康庄大道”上），ADAM会**逐渐减小**它的步长，以便在接近谷底时进行微调，避免错过最低点。
>         
>     - 对于那些**梯度很稀疏、偶尔才更新**的参数（说明正走在一条“羊肠小道”上），ADAM会**给它一个相对较大**的步长，鼓励它更大胆地探索，更快地前进。
>         
> 
> ---
> 
> ### 总结
> 
> **ADAM优化器 = 动量 (Momentum) + 自适应学习率 (Adaptive Learning Rate)**
> 
> 它结合了两种先进思想，创造出了一位既有“惯性”懂得冲刺，又会“因地制宜”调整每一步大小的“智能登山者”。
> 
> 这使得ADAM非常鲁棒（robust），在绝大多数任务中都有良好且高效的表现，而且不太需要我们去手动精调那个全局的“学习率”参数。因此，它成为了深度学习领域最常用、也是大家首选的优化器之一。

> [!NOTE]- 解释 “超定 (Over-determined)” 系统
> 
> #### 什么是“超定”？
> 
> 简单来说，一个系统是“超定”的，意思是**你拥有的信息（方程或线索）数量，超过了你要求解的未知数数量**。
> 
> 因为信息太多，它们之间很可能会出现**微小的矛盾**，导致你无法找到一个“完美”的解来同时满足所有的信息。
> 
> #### 寻宝游戏的比喻 🗺️
> 
> 想象一下，你在寻找一个埋在地下的宝藏，它的位置是 `(x, y)` 这两个**未知数**。
> 
> - **情况一：适定系统 (Well-determined)**
>     
>     - **线索1**: “宝藏离老橡树正好10米远。” (这给了你一个以橡树为圆心，半径10米的圆)
>         
>     - **线索2**: “宝藏离古井正好8米远。” (这给了你另一个以古井为圆心，半径8米的圆)
>         
>     - 这两个圆会有两个交点，你得到了两个可能的解。这时，你拥有2个未知数和2条线索，信息量刚刚好，这就是**适定**的。如果再给你一条线索“宝藏在河的北岸”，你就能确定唯一的位置。
>         
> - **情况二：超定系统 (Over-determined)**
>     
>     - 你已经有了上面那两条精确的线索。现在，你的一个朋友跑过来，拿着一个有点不准的GPS说：“我用GPS测了一下，宝藏大概在**这里**！” 他在地图上指了一个点。
>         
>     - 麻烦来了：他指的这个点，**并不**正好在你画的那两个圆的交点上，而是稍微偏了一点。
>         
>     - 现在，你拥有了**3条线索**，但你依然只需要求解 `(x, y)` 这**2个未知数**。由于GPS的误差，这3条线索互相矛盾，你**不可能**找到一个点，能同时满足“离橡树10米”、“离古井8米”和“正好在GPS标记点”这三个条件。
>         
>     - **这个系统，就是“超定”的**。
>         
> 
> #### “超定”意味着什么？
> 
> 在纯数学的理想世界里，超定系统通常被认为是“无解”的。
> 
> 但在现实世界和机器学习中，我们的目标变了：我们不再寻求那个不存在的“完美解”，而是去寻找一个**“最佳妥协解 (best-fit solution)”**。在寻宝游戏中，你会找一个点，它可能不“完美”地在两个圆上，也不“完美”地在GPS标记上，但它离那两个圆的交点**非常近**，同时也离GPS标记点**非常近**。这个点就是让所有线索的**“总误差”最小**的那个点。
> 
> #### PINN如何处理“超定”问题？
> 
> 这正是PINN大显身手的地方。论文中提到的情况是：
> 
> 1. 我们有一个**“适定”**的物理问题：有物理方程（PDE）和必需的初始/边界条件。用传统方法，这已经足够求解了。
>     
> 2. 但除此之外，我们**还有一些额外的“线索”**：一些在研究区域内部测量到的真实数据（比如房间中心某几个点的温度）。
>     
> 3. **物理定律 + 边界条件 + 内部测量数据 > 未知数**，这就构成了一个**“超定”系统**。
>     
> 
> PINN的损失函数天生就是为解决这种问题而设计的：
> 
> - LPDE​, LIC​, LBC​ 会迫使解满足物理定律和边界条件（让你尽量靠近那两个圆的交点）。
>     
> - Ldata​ 会迫- 使解靠近那些内部的测量数据（让你尽量靠近朋友的GPS标记点）。
>     
> 
> 通过最小化总损失 `L`，PINN**自动地**在所有这些（可能相互矛盾的）要求之间寻找一个**最佳的平衡点**，最终给出的就是那个“最佳妥协解”。

> [!NOTE]- 解释自动微分 (Automatic Differentiation)
> 
> #### 我们为什么需要计算导数？
> 
> 首先要明确，我们之所以需要一个计算导数的方法，是为了构建并计算损失函数中的“物理惩罚项”
> 
> $L_{PDE}$​。因为物理定律（PDE）中充满了各种对时间、空间的导数（如速度、加速度等），我们必须先算出神经网络输出的导数，才能检查它是否遵守了物理定律。
> 
> 那么，在计算机里，计算一个复杂函数的导数，主要有三大“门派”。为了理解自动微分的巧妙之处，我们先看看另外两个。
> 
> ---
> 
> #### 门派一：数值微分 (Numerical Differentiation) — “近似的笨办法”
> 
> 这是我们最容易想到的方法，也是你在高中物理中学到的“瞬时速度”的求法。
> 
> - 方法：为了求函数在 $x$ 点的导数，我们在 $x$ 附近取一个极小的距离 $h$，然后用公式 $(f(x+h)−f(x)) \div h$ 来近似。
>     
>     **缺点**：这**永远只是一个近似**！$h$ 取多小是个大难题：取太大，近似不准；取太小，电脑又会因为浮点数精度问题产生巨大误差。这种由近似带来的误差，就是原文中提到的 **“截断误差 (truncation errors)”** 。
>     
> 
> #### 门派二：符号微分 (Symbolic Differentiation) — “书面的笨办法”
> 
> 这就像我们自己在草稿纸上做微积分。
> 
> - **方法**：给定一个函数的数学表达式，比如 $f(x)=x^2$，我们利用求导规则，推导出它的导数表达式 $f′(x)=2x$。
>     
> - **缺点**：神经网络是一个极其巨大的、由成千上万个简单函数层层嵌套而成的函数。如果你想先把它的完整数学表达式写出来，这个表达式可能会长达数万页，复杂到无法想象，电脑根本无法处理。这个问题被称为 **“表达式爆炸”**。
>     
> 
> ---
> 
> #### 门派三：自动微分 (AD) — “精确的巧办法”
> 
> 自动微分吸取了前两者的优点，规避了它们的缺点。它既能得到**精确的**导数值（像符号微分），又**避免了可怕的表达式爆炸**。
> 
> 它的核心思想是：**任何复杂的计算，都可以拆解成一系列基础运算的组合**。而这些基础运算（如加、减、乘、除、sin、exp等）的导数是已知的。
> 
> - 核心机制：链式法则 (Chain Rule)
>     
>     AD的美妙之处在于，它把一个复杂的函数看作一个 **计算图 (computational graph)**，然后在这个图上，利用我们都学过的**链式法则**，像多米诺骨牌一样，从后往前精确地传递导数值。
>     
> - 一个简单的例子：
>     
>     假设我们要计算函数 $y=sin(x^2)$ 在 $x=3$ 时的导数。
>     
>     1. **拆解**: AD会把这个计算拆成两步：
>         
>         - 第一步: $a=x^2$
>             
>         - 第二步: $y=sin(a)$
>             
>     2. **局部求导**: AD知道每个基础步骤的导数：
>         
>         - $dy/da = cos(a)$
>             
>         - $da/dx = 2x$
>             
>     3. **链式法则拼接**: 根据链式法则 $dy/dx = (dy/da) * (da/dx)$，它把这些导数值乘起来。
>         
>         - 当 $x=3$ 时, $a=3^2=9$。
>             
>         - $dy/dx$ 的值就是 $cos(9) * (2*3) = 6 * cos(9)$。
>             
> 
> 
> **总结**: AD并没有推导出一个通用的导数公式 $y′=2xcos(x^2)$，而是对于一个具体的输入值 `x=3`，它通过一步步的数值计算，直接得到了导数的**精确数值** $6*cos(9)$。
> 
> #### AD的巨大优势
> 
> 对于一个庞大的神经网络，AD也是这样逐层、逐个神经元地应用链式法则，最终得到输出相对于输入的精确导数值。
> 
> 这种方式计算出的导数是一个 **明确的、精确的表达式（an explicit expression）**，而不是一个近似值。因此，它完美地 **避免了数值微分中讨厌的“截断误差”** 7。这对于依赖精确物理方程的PINN来说，是至关重要的。

> [!NOTE]- 解释计算图 (Computational Graph)
> 
> **计算图 (Computational Graph)** 是现代人工智能，特别是深度学习的基石。理解了它，你就能明白神经网络和自动微分的底层逻辑。
> 
> ---
> 
> #### 什么是计算图？
> 
> 简单来说，计算图就是一张**“数学公式的流程图”**。
> 
> 它是一种将任何复杂的数学运算，拆解成一步步基础运算，并用图的形式来表示的方法。这张图由两种东西构成：
> 
> - **节点 (Nodes)**：代表**数据**（比如输入变量、常数）或**运算**（比如加、减、乘、除、sin等）。
>     
> - **边 (Edges)**：代表**数据的流动方向**的箭头，它把数据从一个节点传向下一个节点。
>     
> 
> #### 工厂流水线的比喻 🏭
> 
> 想象一下，你要计算一个稍微复杂一点的公式：$e = (a + b) * (b + 1)$
> 
> 我们可以把这个计算过程想象成一条工厂的流水线：
> 
> - **原材料**: 输入变量 `a` 和 `b`，还有一个常数 `1`。
>     
> - **最终产品**: 输出结果 `e`。
>     
> - **工位**: 每个工位只负责一个最简单的操作，比如“加法”或“乘法”。
>     
> 
> 这条流水线（计算图）是这样工作的：
> 
> 1. **工位1（加法）**: 从仓库接收原材料 `a` 和 `b`，把它们相加，生产出半成品 `c = a + b`。
>     
> 2. **工位2（加法）**: 从仓库接收原材料 `b` 和 `1`，把它们相加，生产出半成品 `d = b + 1`。
>     
> 3. **工位3（乘法）**: 接收来自工位1的半成品 `c` 和来自工位2的半成品 `d`，把它们相乘，生产出最终产品 `e = c * d`。
>     
> 
> 我们可以把这条流水线画成一张图：
> 
> 
> ```mermaid
> graph LR
>     A[a] --> C{c = a+b}
>     B[b] --> C
>     B --> D{d = b+1}
>     const[1] --> D
>     C --> E{e = c*d}
>     D --> E
> ```
> 
> 
> 这张图，就是函数 $e = (a + b) * (b + 1)$ 的**计算图**。你看，一个复杂的公式被清晰地拆解成了几个简单的步骤。
> 
> ---
> 
> #### 为什么计算图这么重要？
> 
> 计算图是两大关键技术的核心：
> 
> **1. 它让复杂的神经网络变得“有序”**
> 
> 一个深度神经网络，本质上就是一个**极其巨大和深邃的计算图**。输入数据 `(t, x)` 是原材料，经过成千上万个“加权、求和、激活”的工位（神经元），最终生产出产品 $\hat u$。把神经网络看作一个计算图，让我们可以清晰地组织和管理这个庞大的计算流程。
> 
> **2. 它是自动微分 (AD) 的“导航地图”**
> 
> 这是最关键的一点。还记得我们说自动微分是靠 **“链式法则”** 来精确求导的吗？计算图为链式法则的实施提供了完美的路线图。
> 
> - **正向传播 (Forward Pass)**: 就像我们的流水线，数据从左到右，从原材料到最终产品，一步步计算出最终结果 $e$。
>     
> - **反向传播 (Backward Pass)**: 自动微分求导的过程，就是在这个计算图上**从右向左，沿着箭头反向走一遍**。
>     
>     - 从最终产品 $e$ 出发，我们知道  $\partial e / \partial e=1$。
>         
>     - 反向走到工位3（乘法），利用乘法的求导规则和链式法则，可以算出 $\partial e/ \partial c$ 和 $\partial e/ \partial d$。
>         
>     - 再反向走到工位1（加法），利用加法的求导规则和从工位3传来的  $\partial e/ \partial c$，可以算出 $\partial e/ \partial a$ 和 $\partial e/ \partial b$。
>         
> 
> 通过这样沿着图反向走一遍，我们就能算出最终输出 $e$ 相对于 **所有** 原始输入（$a$ 和 $b$）的精确导数值。这个在计算图上反向传播导数的过程，就是大名鼎鼎的 **反向传播算法 (Backpropagation)**，它是训练几乎所有现代神经网络的核心引擎。

> [!NOTE]- 解释 “布辛涅斯克近似 (Boussinesq approximation)”
> 
> #### 它是什么？
> 
> 首先要明确，**布辛涅斯克近似**不是一个新的物理定律，而是对**纳维-斯托克斯方程 (Navier-Stokes equations, NSE)** 的一种**巧妙的简化**。
> 
> 这个简化的目的，是为了在处理一类特定问题时，能让复杂的方程变得更容易求解，同时又不会丢失最核心的物理现象。
> 
> #### 它专门解决什么问题？
> 
> 它主要用于处理由**浮力 (Buoyancy)**驱动的流动问题，我们称之为**自然对流 (Natural Convection)**。
> 
> 想象一下：
> 
> - 冬天，暖气片加热了它周围的空气，热空气变轻上升；房间另一边的冷空气变重下沉，形成了一股循环气流。
>     
> - 你用锅烧水，锅底的水被加热后变轻上升，锅顶的凉水变重下沉，形成了翻滚的对流。
>     
> - 白天的海风：陆地比热容小，升温快，陆地上方的空气变热上升；海面上方的冷空气流过来补充，形成了海风。
> 
> 
> ![[week1_paperlearning3.jpg]]
> 
> 这些现象的共同点是：流体的运动不是由水泵或风扇驱动的，而是仅仅因为 **温度差异** 导致了 **密度差异**，继而由**重力**引发了**浮力**，最终产生了流动。
> 
> #### 它的“戏法”是什么？——“抓大放小”
> 
> 解决这类问题的难点在于，当温度变化时，流体的密度 `ρ` 也会跟着变化。要把一个时时刻刻在变化的 `ρ` 代入到完整的纳维-斯托克斯方程中去计算，数学上会变得异常复杂。
> 
> 布辛涅斯克近似的“戏法”就是**“抓大放小”**：
> 
> 1. **“放小”——忽略密度变化对“惯性”的影响**
>     
>     - 对于水或空气来说，即使温度变化了几十度，其密度的变化通常也只有百分之几，非常微小。
>         
>     - 这个近似大胆地假设：在计算流体的**惯性**（即 F=ma 中的质量 `m` 部分）时，我们可以**忽略**这点微小的密度变化，直接把密度 `ρ` 当作一个**常数**来处理。这极大地简化了方程。
>         
> 2. **“抓大”——保留密度变化对“浮力”的影响**
>     
>     - 它虽然忽略了密度变化对惯性的影响，但它敏锐地意识到，恰恰是这点**微小的密度差异**，在与**重力 `g`** 相乘后，产生了驱动整个流动的**核心动力——浮力**。
>         
>     - 因此，布辛涅斯克近似**只在**有重力 `g` 参与的那一项（即浮力项）里，**保留**并考虑密度的变化 `Δρ`。
>         
> 
> #### 拥挤电梯的比喻 👥
> 
> 想象一部非常拥挤的电梯，代表一团流体。
> 
> - **计算惯性（“放小”）**：要计算电梯的启动加速度，你需要知道里面所有人的总质量。一个精确但麻烦的办法是称每个人的体重。一个聪明的近似办法是：“假设每个人都是70公斤的标准体重，然后乘以总人数就行了。个别人是68公斤还是72公斤，对电梯启动的惯性影响微乎其微。”——**这就是忽略密度对惯性的影响**。
>     
> - **计算浮力（“抓大”）**：虽然体重差异对总惯性影响小，但它却是导致电梯内“流动”的关键！如果一个100公斤的壮汉（**冷的、密度大的流体**）和一个50公斤的瘦子（**热的、密度小的流体**）站在一起，壮汉会因为重力更大而站得更稳，可能会把瘦子挤到一边去。这种由体重差异和重力共同导致的“挤压效应”，就是**浮力**。我们必须考虑这个体重差异，才能理解电梯内的“人员流动”。——**这就是保留密度在浮力项中的影响**。
>    
> 
> ---
> 
> ### 总结
> 
> **布辛涅斯克近似**是一个伟大的物理简化，它通过“抓大放小”的策略，将复杂的变密度流问题，转化成了一个更容易求解的、同时又能精确描述自然对流核心驱动力的常密度流问题。
> 
> 所以原文中说
> 
> `φ is the temperature` 1，正是因为在这类问题中，
> 
> **温度 $\phi$** 是“因”，它引起了密度的微小变化，而这个微小的密度变化在重力作用下产生了浮力，成为了驱动整个流场运动的“果”。

> [!NOTE]- 图一的解释
> 
> 我们可以把这段话拆成四句，它像一个导游，带着我们一步步走完**图1**的整个流程。
> 
> ---
> 
> #### 1. 神经网络：一个聪明的“猜测者”
> 
> > **“一个全连接神经网络，以时间和空间坐标(t, x)为输入，用于逼近多物理场解 $\hat u=[u,v,p,\phi]$。”**
> 
> - 这句话描述的是图中的**蓝色方框**。这个神经网络就是我们之前提到的“AI学生”或“猜测者”。
>     
> - **输入 (Input)**：你问它一个问题：“在 $t$ 时刻的 $x$ 位置，流场是什么样的？”
>     
> - **输出 (Output)**：它会给出一个全面的“猜测”，这个猜测包含了多个物理量，比如流体的速度 $u$ 和 $v$、压力 $p$、以及一个标量（比如温度 $\phi$）。一次性求解多个物理量，这就是 **“多物理场解 (multi-physics solutions)”** 的意思。
>     
> 
> ---
> 
> #### 2. 自动微分：一台内置的“微积分引擎”
> 
> > **“$\hat u$ 相对于输入的导数通过自动微分（AD）计算得出...”**
> 
> - 这句话描述的是**黄色方框**。神经网络给出的“猜测” $\hat u$，会立刻被送到“自动微分”这个模块里。
>     
> - AD就像一台高效的微积分引擎，它的任务就是计算出神经网络的猜测结果 $\hat u$ 里面所有物理量（$u,v,p, \phi$）相对于时间和空间 ($t,x$) 的**变化率（即导数）**。例如，它能从速度 $u$ 计算出加速度 $\partial u/\partial t$。
> 
> ---
> 
> #### 3. 损失函数：一张包含“物理规则”的“记分卡”
> 
> > **“...然后被用来在损失函数中构建控制方程的残差项。该损失函数通常由多个被不同系数加权的项组成。”**
> 
> - 这句话描述的是**绿色方框**。我们为什么要费劲去计算导数呢？就是为了用在这里。
>     
> - **构建残差 (Formulate residuals)**：我们把神经网络的猜测 $\hat u$ 和通过AD算出的导数，一起代入到物理定律（比如纳维-斯托克斯方程）里。如果这个猜测是完美的，方程两边应该相等，其差值应该为 $0$。这个“差值”，就叫做 **“残差 (residual)”**。残差不为0，就说明神经网络的猜测**违反了物理定律**。
>     
> - **加权求和**: 这个“物理定律残差” $L_{PDE}​$，会和其他几个损失项（如初始条件损失 $L_{IC}$​，边界条件损失 $L_{BC}​$，数据损失 $L_{data}​$）组合起来，形成最终的总损失 $L$。每个损失项前面都有一个权重 $w$，用来表示这门“科目”的重要性。
>     
> 
> ---
> 
> #### 4. 优化：一个“一石二鸟”的学习过程
> 
> > **“神经网络的参数 $\theta$ 和未知的偏微分方程参数 $\lambda$ 可以通过最小化损失函数来同时被学习。”**
> 
> - 这句话描述的是图中最下方的 **“优化循环 (Update Loop)”**。这是整个PINN框架最巧妙的地方之一。
>     
> - 在通过最小化总损失 $L$ 的过程中，PINN可以**同时学习两样东西**：
>     
>     1. **神经网络的参数 $\theta$**：也就是网络内部所有的“旋钮”。调整它们，相当于让神经网络学习 **“如何求解问题”**。
>         
>     2. **未知的物理参数 $\lambda$**：比如我们可能不知道流体的粘性系数是多少。PINN可以把这个粘性系数 $\lambda$ 也当作一个可学习的变量。在优化的过程中，它会去寻找那个能让总损失最小的 $\lambda$ 值。这相当于让神经网络 **“反推出问题本身包含的未知常数”**。
>         
> - 这种“一石二鸟”的能力，使得PINN在解决**逆问题 (Inverse Problems)** 时威力巨大。

 > [!NOTE]- 图一中AD的解释
> ### $\partial t(n)$: 时间导数 (Time Derivative) ⏰
> 
> - **它是什么**: 这个符号 $\partial /\partial t$ 表示“对时间的变化率”。它计算的是物理量（如速度、压力）是如何**随时间变化**的。上标 ($n$) 表示可以计算任意阶的导数（比如一阶导数是速度，二阶导数是加速度）。
>     
> - **它起什么作用**: 它的作用是提供所有与时间相关的物理量。在流体力学中，流动是随时间演化的，所以像加速度 (`∂u/∂t`) 这样的项是物理方程（`L_PDE`）的核心组成部分。
>     
> - **一个例子**: 在牛顿第二定律 $F=ma$ 中，$a$ 是加速度，也就是速度 $u$ 对时间的一阶导数 ($a=\partial u/\partial t$)。AD模块的这个部分就是用来计算这个加速度的。
> 
> ---
> 
> ### $\partial x(n)$: 空间导数 (Spatial Derivative) 🗺️
> 
> - **它是什么**: 这个符号 $\partial /\partial x$ 表示“对空间位置的变化率”。它计算的是物理量是如何**从一个点到另一个点**变化的。
>     
> - **它起什么作用**: 它的作用是描述物理量在空间中的分布和变化。比如流体内部的摩擦力（粘性力）就和速度在空间中的变化率有关。物理方程 $L_{PDE}$ 中包含了大量的空间导数项，比如 $\partial p/\partial x$（压力梯度）和 $\partial^2 u/\partial x^2$（粘性项）。
>     
> - **一个例子**: 当你搅动蜂蜜时，你会感到阻力，这个阻力（粘性力）的大小就取决于你搅动时，蜂蜜速度在空间上的变化有多剧烈。这个变化就是通过空间导数来描述的。
>     
> 
> ---
> 
> ### `I`: 恒等算子 (Identity Operator) unchanged
> 
> - **它是什么**: "Identity" 意为“恒等”，这个 $I$ 是最简单的一个算子，它的作用是 **“什么都不做”**。输入是什么，输出就是什么。
>     
> - **它起什么作用**: 它的存在是为了在图中清晰地表示：我们在计算损失函数时，不仅需要用到解的各种导数，也需要用到解**本身的值** $\hat u=[u,v,p,\phi]$。
>     
> - **一个例子**: 在计算数据损失 $L_{Data} = \hat u - u_{data}$ 时，我们就需要用神经网络直接输出的解 $\hat u$，而不是它的导数。$I$ 算子就代表了这个直接传递的过程。
>     
> 
> ---
> 
> ### $\partial n$: 法向导数 (Normal Derivative) 🧱
> 
> - **它是什么**: $\partial /\partial n$ 是一个特殊的导数，它代表着沿着**垂直于边界的方向**的变化率（“法向”就是垂直的意思）。
>     
> - **它起什么作用**: 它的作用是专门用来计算**诺伊曼边界条件 (Neumann Boundary Condition)** 的。我们之前讨论过，诺伊曼边界条件规定了边界上的“通量”（比如热量流过墙壁的速率）。这个通量在数学上就等于物理量沿边界法线方向的导数。
>     
> - **一个例子**: 如果一个水管的管壁是完全不透水的，那么水流速度 $u$ 在垂直于管壁方向上的分量必须为 $0$。更进一步，如果墙壁是绝热的，那么温度 $T$ 在垂直于墙壁方向上的变化率（法向导数 $\partial T/\partial n$）就必须为 $0$。AD模块的这个部分就是用来计算这个 $\partial T/\partial n$ 的，以便在 $L_{BC}$ 中进行核对。
>     
> 
> ### 总结表格
> 
> |符号|名称|作用|解决的问题|
> |---|---|---|---|
> | **$\partial t(n)$** | 时间导数 | 计算物理量随时间的变化率|物体如何运动？（如加速度）|
> | **$\partial x(n)$** | 空间导数 | 计算物理量随空间的变化率|物理量如何分布？（如压力梯度）|
> |**$I$**|恒等算子|直接传递解的原始值|计算需要解本身的地方（如数据损失）|
> |**$\partial n$** | 法向导数 | 计算垂直于边界的变化率|检查是否满足边界通量规则（诺伊曼条件）|
### 2.2 PINN的近期进展

PINN最初在[19, 20]中被提出（另见[15]），它在科学计算领域和流体力学领域都吸引了大量的关注。在此，我们回顾一些关于其方法论和在流体力学中应用的相关工作。

得益于其在函数逼近方面的高度灵活性和表达能力，PINN已被扩展用于求解各种类型的偏微分方程（PDEs），例如，积分-微分方程[21]、分数阶方程[21]、曲面上的偏微分方程[22]和随机微分方程[23]。一种基于伽辽金方法的PINN变分形式（hp-VPINN）被提出来处理非光滑解的偏微分方程[24]。此外，变分hp-VPINN考虑了区域分解，类似的点态版本也在CPINN[25]和XPINN[26]中被研究。一种通用的、带区域分解的PINN并行实现被用于流动问题[27]；NVIDIA的库SimNet[28]也是一个非常高效的PINN实现。

另一个重要的扩展是为神经网络推断出的PDE解进行不确定性量化[29, 30, 31, 32, 33]。这已通过使用贝叶斯框架进行了研究[33]。此外，其他一些关于PINN的研究则专注于神经网络架构和训练的发展，例如，使用多保真度框架[34]、自适应激活函数[35]、损失函数的动态权重[36]、硬约束[37]和基于CNN的网络架构[38]，这些都可以提升PINN在不同问题上的性能。在理论方面，一些近期的工作[39, 40, 41]为PINN的收敛性提供了更多的保证和见解。

方法论的发展启发了在其他领域的大量应用，特别是在流体力学领域，因为流动现象可以被纳维-斯托克斯方程（NSE）所描述。在[15]中，最初的“原味”PINN被提出来，用于根据二维圆柱绕流的速度测量数据，反推出NS方程中的未知参数（例如，对流项的系数）。在这项工作之后，PINN被应用于各种流动问题[10, 11, 12, 13, 14, 42, 43, 44, 45, 46, 47, 48, 49]，涵盖了可压缩流[13]、生物医学流[14, 42, 50]、湍流对流[48]、自由边界和斯蒂芬问题[47]等应用。

PINN在解决流体力学问题中一个主要的吸引人的优势是，一个统一的框架（如图1所示）可以同时用于正问题和逆问题。与传统的CFD求解器相比，PINN在整合数据（流动物理量的观测值）和物理（控制方程）方面更为优越。一个非常有前景的应用是在流动可视化技术上[12, 51]，通过它，流场可以很容易地从浓度场或图像等观测数据中被推断出来。相反，这类逆问题对于传统的CFD求解器来说是非常困难的。更多在一般力学中的相关工作可以在[52]中找到关于湍流的，在[53]中找到关于相场断裂模型的，以及在[54]中找到关于推断非均匀材料模量的。

![[week1_paperlearning3.png]]

_**图2：PINN在不可压缩流中的案例研究：模拟圆柱绕流三维尾流的图示。** (a) 整个区域内涡量（$x$ 分量）的等值面，其颜色由流向速度标定。带蓝色边框的立方体代表本案例中的计算区域。(b) 该区域内的速度场和压力场。该模拟由CFD求解器Nektar执行，它基于谱 / hp元方法[2]。_

---

> [!TIP]- 注释
> 
> 这一大段信息量非常密集，因为它引用了大量其他科学家的工作。我们可以把它梳理成三个部分：**PINN自身是如何被“升级”的**，**升级后的PINN被用在了哪些“新地方”**，以及**为什么它这么受欢迎**。
> 
> ---
> 
> #### 1. PINN的“升级套件”：方法论的进展
> 
> 这部分在介绍，为了让PINN变得更强大、更快、更可靠，科学家们都开发了哪些“黑科技”。
> 
> - **扩大“攻击范围”**：最初的PINN主要解决标准的PDEs。现在的升级版已经可以处理更奇怪、更复杂的方程了，比如**积分-微分方程**、**分数阶方程**（导数不再是整数阶）、**曲面上的方程**（比如计算肥皂泡表面的物理），甚至是**随机微分方程**（方程里包含随机因素）[21, 22, 23]。
>     
> - **处理“疑难杂症”**：
>     
>     - 对于解不光滑（比如有尖角）的问题，开发了`hp-VPINN`这样的特殊版本来应对[24]。
>         
>     - 为了解决超大规模问题，科学家们使用了**“区域分解”**（Domain Decomposition）的思想，就像“分而治之”。把一个巨大的计算区域切成很多小块，让不同的PINN或计算核心分头解决，最后再拼起来。`XPINN`就是这个思想的代表[26]。NVIDIA公司甚至推出了一个专门用于此的高效库`SimNet`[28]。
>         
> - **增加“靠谱度”分析**：
>     
>     - 开发了**“不确定性量化”**（Uncertainty Quantification）技术[29, 30, 31, 32, 33]。这不仅仅是给出一个答案，还会告诉你“我对这个答案有多大的信心”。比如，它会说：“这个点的速度是10m/s，置信度是95%”。这通常用**贝叶斯框架**来实现[33]。
>         
> - **优化“引擎和训练”**：
>     
>     - 科学家们从各个角度改进神经网络本身和训练过程，比如采用**自适应激活函数**（让神经元更“聪明”）[35]，**动态调整损失权重**（在训练的不同阶段关注不同的“科目”）[36]，或者使用为图像处理而生的**CNN架构**[38]等。
>         
> - **奠定“理论基础”**：
>     
>     - 还有一些数学家在努力从理论上证明PINN为什么能收敛、在什么情况下会失败，为这个技术提供坚实的理论保证[39, 40, 41]。
>         
> 
> ---
> 
> #### 2. PINN的“应用版图”：从“原味”到“遍地开花”
> 
> - **梦开始的地方**: 最早的、最基础的“原味版”（vanilla）PINN，是被用来解决一个经典的二维圆柱绕流问题，从一些速度数据点，反过来推测物理方程里的一个未知参数[15]。
>     
> - **遍地开花**: 在这个基础之上，PINN迅速被应用到了流体力学的各个角落[10-14, 42-50]，比如：
>     
>     - **高速飞行器**相关的**可压缩流**[13]。
>         
>     - **血管、血栓**相关的**生物医学流**[14, 42, 50]。
>         
>     - **暖气、地幔对流**相关的**湍流对流**[48]。
>         
>     - **冰块融化**这类**自由边界问题**[47]。
>         
> 
> ---
> 
> #### 3. PINN的“核心魅力”与“杀手级应用”
> 
> - **核心魅力**: 作者再次强调，PINN最大的魅力在于它用**一个统一的框架**，就能同时解决**正问题**和**逆问题**，并且能**完美融合物理和数据**[15]。
>     
> - **杀手级应用**: 一个非常有前景的应用是**“流动可视化技术”**[12, 51]。
>     
>     - **这是什么意思呢？** 想象一下，在风洞实验里，我们释放了一些烟雾来观察气流的形态，拍下了一段视频。传统方法很难从这些烟雾的形态反推出整个流场精确的速度和压力。
>         
>     - 但PINN可以！它可以只“看”这些烟雾的浓度变化图像，结合物理方程，就能反推出背后完整的、三维的速度和压力场。这对于传统CFD来说是一个极其困难的逆问题，而PINN却能轻松应对。

> [!TIP]- 图2解释
> 
> 这张图非常重要，因为它展示了我们这次要研究的物理问题的复杂性，并给出了一个**“标准答案”**。请注意，这张图本身**不是用PINN画的**，而是用一个非常强大的传统方法画的。PINN的目标，就是要看看能不能用更少的信息，“重现”出这个标准答案。
> 
> ---
> 
> #### 图 2(a): 问题的“全貌”
> 
> 这张图展示了一个经典的流体力学问题：**流体绕过一个圆柱体**。就像水流过桥墩，或气流吹过烟囱一样，在圆柱体的后方会形成一片非常复杂的、混乱的流动区域，我们称之为**“尾流 (Wake)”**。
> 
> - **那些黄绿色的“面条”是什么？——涡量等值面 (Iso-surface of Vorticity)**
>     
>     - **涡量 (Vorticity)**: 你可以把它想象成流体中无数个微小的“漩涡”或“龙卷风”。涡量就是用来衡量在流体的每一个点上，流体旋转得有多快、多剧烈的一个物理量。
>         
>     - **等值面 (Iso-surface)**: 这是一个用来可视化三维空间数据的方法。想象一下，你把这团流体中所有“旋转速度”等于某个特定值（比如每秒10圈）的点都找出来，然后用一个曲面把它们全部连接起来，这个曲面就是“涡量等值面”。我们看到的这些像面条一样的管状结构，就清晰地勾勒出了尾流中那些主要漩涡的形状和位置。
>         
> - **颜色代表什么？**
>     
>     - 这些“漩涡面条”上的颜色，代表了流体在这一点的**流向速度（streamwise velocity, u）**，也就是沿x轴方向前进的速度有多快。
>         
> - **那个蓝色立方体是什么？**
>     
>     - 这是本案例研究中，我们要用PINN去重点分析的**“目标区域”**。我们不研究整个大方块，只关心这个从圆柱体后方截取出来的、涡流最剧烈的小区域。
>         
> 
> ---
> 
> #### 图 2(b): “目标区域”的内部细节
> 
> 这四张小图，是把图2(a)中的那个**蓝色立方体**放大，向我们展示了它内部详细的物理情况。
> 
> - **`u velocity`**: x方向（前进方向）的速度分布。
>     
> - **`v velocity`**: y方向（左右方向）的速度分布。
>     
> - **`w velocity`**: z方向（上下方向）的速度分布。
>     
> - **`pressure`**: 压力分布。
>     
> 
> 你可以看到，即使是在这么一小块区域里，速度和压力的分布也是极其复杂和不均匀的，充满了各种团块和条纹。
> 
> ---
> 
> #### 最关键的信息：这些图是谁画的？
> 
> > **“该模拟由CFD求解器Nektar执行，它基于谱/hp元方法[2]。”**
> 
> 这句话是理解本案例的关键！它告诉我们，图2中展示的所有这些精美、复杂的图像，都是由一个名为 **Nektar** 的**传统CFD求解器**生成的。
> 
> 我们之前讨论过，**谱方法 (spectral method)** 是一种精度极高的传统计算方法。Nektar用的正是这种高级方法，所以它的计算结果被认为是**非常准确的、可信的**。
> 
> 因此，在这个案例中，**Nektar的计算结果，就扮演了“标准答案”或者说“老师的参考答案”的角色**。
> 
> 接下来的挑战就是：我们能不能**只用蓝色方块边界上的一些稀疏数据**，就让PINN这个“聪明的学生”，**反推出方块内部所有这些复杂的速度和压力场**，并且其结果能和Nektar这个“标准答案”对得上？
> 
> 这就是这个案例研究想要回答的问题。

> [!NOTE]- PINN的“武功”升级详解
> 
> 
> #### 1. 新技能：学习求解更“诡异”的方程
> 
> 最初的PINN擅长解标准的偏微分方程（PDEs），但现实世界远比这复杂。于是科学家们训练它掌握了更厉害的“魔法”，能处理更多类型的方程：
> 
> 
> **积分-微分方程 (integro-differential equations) **
> 
> * 这是什么？ 这类方程不仅包含“微分”（瞬时变化率），还包含“积分”（累积效应）。
> 
> * 高中生比喻: 想象一下预测明天的气温。标准的微分方程可能只关心“今天下午”和“今天晚上”的气温变化。而积分-微分方程则认为，明天的气温不仅和今天的变化有关，还和过去一整个星期所有天气状况的累积效果有关。它考虑了“历史的包袱”。
> 
> 
> **分数阶方程 (fractional equations) **
> 
> * 这是什么？ 我们熟悉的导数都是整数阶的（一阶、二阶……）。分数阶方程则处理像“0.5阶导数”这样的奇怪东西。
> 
> * 高中生比喻: 想象汽车换挡。整数阶导数就像只有1档、2档、3档。而分数阶导数则意味着你拥有1.5档这样的中间状态。它非常适合描述那些具有“记忆效应”的材料，比如你把一块口香糖拉长再松手，它不会立刻弹回，而是慢慢地、似乎“记着”被拉伸过的状态，蠕动着恢复。
> 
> 
> **曲面上的偏微分方程 (surfaces PDEs) **
> 
> * 这是什么？ 很多物理现象不是发生在平坦空间里，而是发生在曲面上。
> 
> * 高中生比喻: 想想肥皂泡表面绚丽的色彩流动，或者不规则的飞机发动机表面热量的传导。这些问题都需要在曲面上求解方程。这个技能让PINN能处理这种“非平面”问题。
> 
> 
> **随机微分方程 (stochastic differential equations) **
> 
> * 这是什么？ “Stochastic”就是随机的意思。这类方程里包含了随机的、不可预测的因素。
> 
> * 高中生比喻: 想象一粒灰尘在空中飘浮的轨迹。它的运动不仅受重力和空气阻力影响，还不断被空气中大量分子的随机碰撞所干扰。这种包含随机“噪声”的系统，就需要用随机微分方程来描述。这个技能让PINN能够处理充满不确定性的问题，比如金融市场的预测。
> 
> ---
> 
> #### 2. 新装备：处理更“棘手”的状况
> 
> - **非光滑解 (non-smooth solutions)**
>
>
>     **这是什么？** 大多数物理问题的解是光滑的曲线。但有些问题的解存在“尖角”或者“断层”，比如超音速飞行产生的激波，或者两种不同液体接触的分界面。这种解就是“非光滑”的。<br>
>         
> - **变分形式 (variational formulation) 和 伽辽金方法 (Galerkin method)**
>     
>     **这是什么？** 这是科学家们想出的一种更“柔和”的求解方式，专门用来对付上面的“非光滑解”。
>     
>     **高中生比喻**: 想象一下检查一张图片是否正确。标准方法是逐个像素去对比，如果图片里有个尖锐的边缘，这个方法在边缘处就会遇到麻烦。而“变分”思想则不那么死板，它不关心每个点是否绝对精确，而是关心**某个区域内的“平均能量”或者“总体误差”是否最小**。这种着眼于“总体”而非“局部”的思路，对处理尖锐变化更有效。**hp-VPINN** 就是这种思想武装下的PINN版本。
>     
> 
> ---
> 
> #### 3. 新战术：解决更“庞大”的敌人
> 
> 当问题太大，一个PINN算不过来时，就需要团队合作。
> 
> - **区域分解 (domain decomposition)**
>
>
>     **这是什么？** “分而治之”的策略。把一个巨大的计算区域，切分成很多个小块，让每个小块由一个独立的PINN来负责计算7。
>     
>     
>     **高中生比喻**: 玩即时战略游戏，你要攻占一张巨大的地图。单兵作战很慢，聪明的做法是把地图分成几个战区，你和你的盟友们（多个PINN）同时从不同战区进攻，并在交界处互相通报战况，协同作战，这样就能大大加快攻占全图的速度。CPINN 和 XPINN 就是两种不同的“协同作战”战术。<br>
>     
> - **并行计算 (parallel implementation)**
>
>
>     **这是什么？** 这就是让“区域分解”这种战术得以实施的技术基础。它能让多个计算核心（比如多个GPU）**同时**进行计算，而不是排队一个一个算。<br>
>     
> - **NVIDIA SimNet**     
>
>
>     **这是什么？** 这是芯片巨头英伟达(NVIDIA)公司开发的一个现成的、非常高效的PINN软件库。
>     
>     **高中生比喻**: 你可以自己一点点造零件组装一台高性能电脑（从零开始写代码），也可以直接去买一台外星人(Alienware)整机。**SimNet** 就相当于英伟达官方出品的“PINN超级计算机”，它把各种并行计算和优化都帮你做好了，你直接拿来用就行，非常高效。

> [!NOTE]- PINN的“高阶”升级详解
> 
> 我们继续用之前的比喻，这部分介绍了如何让我们的“AI学生”不仅会解题，还要**学得更聪明、更诚实、更有理论依据**。
> 
> ---
> 
> #### 1. 增加“诚实度”：我有多大的把握？
> 
> - **不确定性量化 (Uncertainty Quantification)**
>     
>     - **这是什么？** 这项技术不仅让PINN给出一个答案，还要它同时说明**“我对这个答案有多大的信心”**。
>         
>     - **高中生比喻**: 想象一下天气预报。
>         
>         - **没有不确定性量化**：“明天有雨。” (这是一个确定的答案，但你不知道它多大可能会发生。)
>             
>         - **有不确定性量化**：“明天有**80%的概率**下雨，降雨量在**5到10毫米之间**。”
>             
>     - 第二种预报显然更有用，因为它告诉了你预测的**“置信区间”**和**“概率”**。这就是“不确定性量化”要做的，它让PINN的预测结果从一个单纯的数字，变成了一个包含“最佳猜测”和“可能误差范围”的完整评估，这在工程和科学决策中至关重要。
>         
> - **贝叶斯框架 (Bayesian Framework)**
>     
>     - **这是什么？** 这就是实现上面“不确定性量化”常用的一种数学思想和框架。
>         
>     - **高中生比喻**: 侦探破案。他一开始可能有一个**“先验信念”**（比如“根据统计，大部分案件是熟人作案”）。然后，每当得到一个新证据（比如不在场证明），他就会根据这个证据来**更新**他之前的信念。贝叶斯框架就是一套严格的数学方法，用来描述和计算这种**“根据新证据更新已有信念”**的过程。在PINN里，它能帮助模型根据已有数据，推断出解的概率分布，从而量化不确定性。
>         
> 
> ---
> 
> #### 2. 优化“学习方法”：如何学得更快、更好？
> 
> 这部分是科学家们为了提高PINN的训练效率和最终精度，而设计的各种“聪明的学习技巧”。
> 
> - **多保真度框架 (multi-fidelity framework)**: 我们之前讨论过，就是让PINN学会同时利用“高质量的精读教材”和“低质量的课堂笔记”来学习。
>     
> - **自适应激活函数 (adaptive activation functions)**: 神经元内部有一个叫做“激活函数”的组件，它像一个“开关”，决定了这个神经元如何响应输入信号。普通的激活函数是固定不变的。而“自适应”的则像一个**智能调光开关**，它可以在学习过程中自己调整自己的“灵敏度”，找到最适合当前问题的开关形式，从而加速学习。
>     
> - **损失函数的动态权重 (dynamic weights of the loss function)**: 我们知道总损失是几门“科目”分数的加权和。普通的权重是固定不变的。而“动态”权重则允许PINN在训练过程中**动态调整学习重心**。比如，在训练初期，它发现“物理科目”(LPDE​)挂科得最严重，它就会自动增加这门课的权重，投入更多精力去攻克它。
>     
> - **硬约束 (hard constraints)**: 普通的PINN是通过损失函数来“惩罚”违背物理定律的行为，这是一种**“软约束”**（你做错了，我扣你分）。而“硬约束”则更进一步，它通过**巧妙地设计神经网络的结构**，使得网络的输出**天生就满足**某些物理定律（比如质量守恒）。这就像不是靠惩罚来阻止你穿墙，而是直接把墙做成你无论如何也穿不过去的实体，是一种更强的约束。
>     
> - **基于CNN的网络架构 (CNN-based network architectures)**: CNN（卷积神经网络）是一种在处理**图像**等具有空间结构的数据时表现极其出色的网络结构。把它引入PINN，可以帮助模型更有效地从流场这种类似图像的数据中提取特征，提升学习效率。
>     
> ---
> #### 3. 提供“理论证书”：你这个方法科学吗？
> 
> - **理论保证和收敛性见解 (theoretical guarantees and insights into the convergence)**
>     
>     - **这是什么？** 光说一个方法好用还不够，数学家们还想从**理论上证明**它为什么好用。
>         
>     - **高中生比喻**: 你造了一辆新车，在路上跑得飞快。这是**实验结果**。但你还需要一份由工程师和物理学家出具的**“设计蓝图和理论分析报告”**，从数学和力学上证明这辆车的设计是稳定和可靠的，并且在理论上可以达到某个最高速度。这些理论工作就是为PINN提供这样的“安全认证”和“性能保证”，告诉我们它在什么条件下一定能**收敛**（即最终能找到正确的解），以及为什么能收敛。

> [!NOTE]- PINN的“新战场”详解
> 
> 想象一下，我们的“AI英雄”PINN在掌握了各种“升级技能”后，就开始去挑战一系列以往非常难对付的“超级BOSS”。下面就是这些“BOSS”的介绍：
> 
> ---
> 
> #### 1. 可压缩流 (Compressible Flows) 
> 
> - **它是什么？** 
>     在这种流动中，流体的**密度会发生显著变化**。这通常发生在流速非常高（接近或超过音速）或者压力变化巨大的情况。
>     
> - **生活中的例子**:
>     
>     - 超音速战斗机飞行时，机头前方空气被急剧压缩，形成肉眼可见的**激波 (Shockwave)**。
>         
>     - 火箭发动机喷出的燃气。
>         
>     - 天然气在管道中高压输送。
>         
> - **挑战在哪里？** 
>     激波是一个极端的“非光滑”现象，流体性质（密度、压力、温度）会在一个极其狭窄的区域内发生剧烈跳变，这对传统数值方法的计算网格和稳定性都提出了极高的要求。
>     
> - **挤地铁的比喻**: 
>     想象地铁里的人群。在空旷的站台，人群密度低。当所有人挤进一节车厢时，密度急剧升高。可压缩流研究的就是这种“人群密度”会发生变化的流动。
>    
> 
> ---
> 
> #### 2. 生物医学流 (Biomedical Flows) 
> 
> - **它是什么？** 发生在生物体内部的流体力学问题。
>     
> - **生活中的例子**:
>     
>     - 血液在心脏和动脉、静脉中的流动。
>         
>     - 空气在我们呼吸时，在气管和肺泡中的流动。
>         
>     - 药物通过血液被输送到身体各处。
>         
>   
>         
>- **挑战在哪里？** 
>     生物医学流极其复杂。首先，血液本身就不是一种简单的流体，它包含了红细胞、血小板等各种成分。其次，“管道”（血管）是柔软、有弹性的，会随着心跳而搏动。而且，还可能出现**血栓 (Thrombus)** 这样的病变，进一步干扰流动。这类问题通常是 **逆问题**，比如我们只能通过外部的超声图像，去反推血管内的血流速度和血压，这非常困难。
>         
> 
> ---
> 
> #### 3. 湍流对流 (Turbulent Convection Flows) 
> 
> - **它是什么？** 
>     这个词包含了两个概念的结合：
>     
>     - **对流 (Convection)**: 我们之前提过，由温度差异引起的、热流体上升、冷流体下沉的流动。
>         
>     - **湍流 (Turbulent)**: 指的是一种高度混乱、无序、充满大大小小漩涡的流动状态。它不是平稳的、一层层流动的“层流”。
>         
> - **生活中的例子**:
>     - **一锅烧开的沸水**。锅底加热导致水产生“对流”，而剧烈的沸腾使得整个流动变得混乱不堪，充满了气泡和漩涡，这就是“湍流对流”。
>         
>     - 地球的大气环流和海洋洋流。
>         
> - **挑战在哪里？** 
>     湍流被认为是“经典物理学最后一个尚未解决的难题”。它的行为横跨极大的时间和空间尺度，从巨大的气旋到微小的水花，都包含在内。用计算机直接模拟湍流的所有细节，需要天文数字般的计算资源。
>     
> 
> ---
> 
> #### 4. 自由边界和斯蒂芬问题 (Free Boundary and Stefan Problems) 
> 
> - **它是什么？** 
>     在这类问题中，流体（或固体）的**边界本身是未知的，并且会随时间移动**，这个移动的边界也是我们需要求解的一部分。**斯蒂芬问题**是这类问题中最经典的一个例子，特指涉及**相变**（如固态变液态）时的移动边界问题。
>     
> - **生活中的例子**:
>     - **一块正在融化的冰块**。冰（固相）和水（液相）之间的分界线，就是“自由边界”。随着热量传入，冰块融化，这个分界线的位置和形状时时刻刻都在改变。
>         
>     - 钢铁厂里，将钢水浇铸到模具里冷却凝固的过程。
>         
> - **挑战在哪里？** 
>     传统方法需要不断地重新划分计算网格来追踪移动的边界，这非常繁琐且容易出错。而PINN的无网格特性，使其在处理这类边界会“变形”或“移动”的问题时，具有天然的优势。

> [!NOTE]- 解释 “从流动可视化推断流场”
> 
> **原句**: "A promising application is on the flow visualization technology [12, 51], where the flow fields can be easily inferred from the observations such as concentration fields and images."
> 
> **翻译**: “一个非常有前景的应用是在流动可视化技术上1111，通过它，完整的流场可以很容易地从例如浓度场和图像等观测数据中被推断出来。”
> 
> ---
> 
> #### 你的理解与原文的细微差别
> 
> 你提到“通过流场的速度或者温度等值图反推”，这非常接近。但原文说的是从更**“原始”**和**“不完整”**的视觉信息来反推，而不是从已经处理好的“等值图”。
> 
> 原文提到的**“观测数据”**是：
> 
> - **浓度场 (concentration fields)**: 比如在水流中注入一滴墨水，观察墨水是如何散开和流动的。我们能直接观测到的是墨水的**浓度**分布，而不是水的速度。
>     
> - **图像 (images)**: 比如用高速摄像机拍下烟雾在风洞中绕过飞机的形态，我们得到的是一张张包含烟雾形态的**照片**。
>     
> 
> 挑战在于，这些浓度或图像信息，**本身并不直接告诉我们速度和压力是多少**。你需要一个“超级大脑”才能从这些视觉线索中反推出背后隐藏的完整物理场。
> 
> ---
> 
> #### 一个更贴切的比喻：从烟雾轨迹推断风的秘密
> 
> 想象一下，你站在一座山顶，看到远处一个工厂的烟囱正在冒烟。长长的烟柱在空中飘荡，形成了各种复杂的形状。
> 
> 你手里只有一部望远镜和一部相机，你只能**看到和拍下那条烟柱的形状和飘散的形态**（这就是“图像”和“浓度场”）。
> 
> **这个“逆问题”就是**：能否只根据这条烟柱的视觉形态，就精确地计算出从地面到高空**每一处**的**风速、风向、气压和温度**是多少？
> 
> - **传统CFD的困境**: 这对传统方法来说几乎是不可能的。它就像一个侦探，只看到地上的脚印，却无法推断出罪犯的身高、体重和行走速度。
>     
> - **PINN的超能力**: PINN就是那个“超级大脑”。它能做到这一点，因为它的学习过程是这样的：
>     
>     1. **神经网络“猜测”**: PINN会先对整个空间的风速、风向、压力场进行一个**猜测**。
>         
>     2. **正向模拟**: 基于这个“猜测”出的风场，PINN会利用物理方程（纳维-斯托克斯方程）在计算机里**模拟**一下，如果风真的是这样吹的，那么烟雾会形成什么样的轨迹。
>         
>     3. **与现实对比 (计算L_data)**: 它把自己模拟出的烟雾轨迹，和你用相机真实拍下的照片进行对比。如果差得很远，**数据损失项 (Ldata​)** 就会非常高。
>         
>     4. **物理规则检查 (计算L_PDE)**: 同时，它还会检查自己“猜测”的风场本身是否违反了流体力学定律。如果违反了，**物理损失项 (LPDE​)** 就会很高。
>         
>     5. **优化和学习**: 最后，PINN会不断调整它对风场的“猜测”，目标就是让“模拟的烟雾轨迹和真实照片的差异”（Ldata​）以及“对物理定律的违反程度”（LPDE​）这两个总惩罚分降到最低。
>         
> 
> 当损失函数最小化时，PINN就找到了一个既符合物理规律，又能完美解释你所看到的烟雾形态的、**隐藏在背后的完整流场**。
> 
> ---
> 
> **总结一下**：
> 
> 所以，这句话的意思是，PINN有一个“杀手级”应用，就是可以充当一个“超级翻译器”：
> 
> **输入**是像墨水扩散、烟雾形态这样的**视觉图像**（这些是“果”），**输出**是驱动这一切发生的、完整的、包含各处速度和压力的**三维流场**（这是“因”）2222。这是一种极其强大的“由果推因”的能力。

> [!NOTE]- “固体力学”中的新应用详解
> 
> 这两个例子展示了PINN如何解决固体（比如金属、岩石、生物组织）的破碎和内部属性探测问题。
> 
> ---
> 
> #### 1. 相场断裂模型 (Phase-field fracture model)
> 
> - **这是什么？** 这是一种用于模拟**材料如何开裂、破碎**的先进数学模型。
>     
> - **挑战在哪里？**
>     
>     - 模拟裂缝的产生和扩展是非常困难的。因为裂缝是一个**急剧变化的、不连续的**东西，它的尖端在材料中移动，这使得材料的边界和拓扑结构都在变化。传统数值方法（如有限元法）在处理这种移动的、尖锐的“断口”时非常头疼，计算量巨大且容易出错。
>         
> - **“相场”模型的聪明之处**
>     
>     - 它换了一个思路，不再把裂缝看作一条无限细的、锐利的“线”，而是把它看作一个**“模糊的”损伤区域**。
>         
>     - 它引入了一个叫**“相场”**的变量 `φ`，这个变量在0和1之间取值。
>         
>         - `φ=1` 代表材料是**完好无损**的。
>             
>         - `φ=0` 代表材料已经**完全断裂**。
>             
>         - `0 < φ < 1` 的值则代表材料处于一个**“半损坏”的过渡状态**。
>             
> - **墨水浸染的比喻**:
>     
>     - 想象一下，一条裂缝就像一滴墨水滴在宣纸上。它不是一条清晰的线，而是一个**从中心（全黑，完全断裂）向边缘（灰色，半损坏）逐渐过渡到白纸（完好）的浸染区域**。
>         
>     - “相场断裂模型”就是用数学方程来描述这个“墨水渍”（损伤场`φ`）是如何随时间和受力而扩大和蔓延的，从而非常优雅地模拟了裂缝的扩展。
>         
> - **PINN能做什么？**
>     
>     - PINN可以被用来求解这个描述“损伤场`φ`”如何演化的复杂偏微分方程，从而高效地模拟材料的断裂过程[53]。
>         
> 
> ---
> 
> #### 2. 推断非均匀材料的模量 (Inferring modulus in a nonhomogeneous material)
> 
> - **这是什么？** 这是一个经典的**逆问题**，目标是“看透”一种复杂材料的内部结构。
>     
> - **名词解释**:
>     
>     - **非均匀材料 (Nonhomogeneous material)**: 指的是材料的内部属性不是处处相同的。最典型的例子就是**水果蛋糕**。蛋糕坯很松软，但里面镶嵌着坚硬的核桃和有嚼劲的葡萄干。
>         
>     - **模量 (Modulus)**: 这是衡量材料**“硬度”或“刚度”**的物理量。比如，钢铁的模量很高，橡胶的模量很低。
>         
>     - **推断 (Inferring)**: 就是根据外部的观测，反向推测内部的未知信息。
>         
> - **给水果蛋糕做“CT扫描”的比喻**:
>     
>     - 想象一下，你拿到一块水果蛋糕，但你不能切开它。你**只能**用一根牙签从蛋糕的表面去戳它，并记录下在不同位置戳下去时，你感觉到的**阻力大小**（这就是外部观测）。
>         
>     - **你的任务是**: 能否只根据这些从表面获得的“戳刺感”，就绘制出一张完整的、三维的“蛋糕内部结构图”，精确地标示出每一颗核桃和葡萄干的位置和硬度？
>         
> - **挑战在哪里？**
>     
>     - 这非常困难。你只掌握了外部的、有限的信息，却要反推整个内部连续变化的、未知的属性分布。
>         
> - **PINN能做什么？**
>     
>     - PINN是解决这类逆问题的利器。我们可以让材料的**模量** `λ(x)` 作为一个**依赖于空间位置的、待学习的函数**，并将其作为神经网络的一部分。
>         
>     - 我们把已知的外部“戳刺”数据作为 `L_data`，把固体力学方程作为 `L_PDE`。
>         
>     - PINN在训练时，会不断调整内部的模量函数 `λ(x)`，直到它所预测出的表面“戳刺感”（通过`L_PDE`计算得出）与真实的测量值（`L_data`）完全吻合为止。
>         
>     - 最终，PINN就学会了那个未知的、非均匀的模量分布函数 `λ(x)`，相当于用AI给这个材料做了一次无损的“CT扫描”[54]。

> [!NOTE]- **spectral/hp element method**（光谱/hp 元素法）解释
>### ✅ 一、这是在干什么？
>
>这段话讲的是在做一个流体模拟：  
>想象一根柱子放在空气或水流中（比如圆柱在风洞里），流体绕着它流过，会在柱子后面形成“尾流”（wake flow），也就是那些旋涡和扰动。研究者想知道这个流动到底长什么样、速度和压力是怎样变化的。
>
>但实际实验很贵，所以他们用电脑“模拟”这个流动——这就需要用一种数值方法。
>
>---
>
>### ✅ 二、为什么需要“spectral/hp element method”？
>
>模拟流动时，我们必须把流体控制方程（如 Navier-Stokes 方程）写进电脑里。这些方程太复杂，不能用手解，所以必须用一种“数值方法”来**近似**地解。
>
>普通的方法可能不够精确、效率不高，而 **spectral/hp element method** 是一种非常高精度的数值方法，专门用来处理像这种复杂的流体问题。
>
>---
>
>### ✅ 三、这方法怎么工作的？我们用比喻来理解
>
>想象你想画一张地图，但不是一笔画完，而是：
>
>1. **先把地图分块**：把整个区域分成很多小格子（像拼图一样）。这就叫做“元素法”中的“element”部分。
  >  
>2. **在每一块里精细画图**：你在每一小块里，不是画简单的直线，而是用非常精细的曲线（数学上叫“高阶多项式”）来逼近真实情况。这就是“光谱（spectral）”方法的特色，追求**极高精度**。
  >  
>3. **“hp”表示两个调节旋钮**：
  >  
 >    - **h**：调节“网格的大小”——你可以把拼图分得更细；
>        
>     - **p**：调节“每块里的曲线精度”——用更高阶的多项式；  
 >       → 所以“hp方法”就是一种可以自由调节 **精度和计算量** 的方法。
>        
>
>---
>
>### ✅ 四、总结：一段话让你记住
>
> **Spectral/hp element method** 就像是用高精度拼图法来“画出”复杂流动。它先把空间分成很多小块，在每块里用非常精细的数学曲线来模拟流体的速度、压力等变化，既能处理复杂形状，又能达到很高的计算精度。
>
>---
>
>![[week1_paperlearning4.png]]


## 3 针对三维不可压缩流的案例研究

在本节中，我们展示PINN在解决不可压缩流中逆问题方面的有效性。我们特别将PINN应用于基于少量二维双分量（2D2C）速度观测值来重构三维流场。所提出的算法能够基于有限的数据非常精确地推断出完整的速度场和压力场，这在只有二维测量（例如，平面粒子图像测速法）可用时，为诊断复杂流动提供了非常有前景的方法。

### 3.1 问题设置

在本节中，我们考虑雷诺数 $Re = 200$ 时，流体绕过一个静止圆柱体的三维尾流。为了评估PINN的性能，我们使用 **谱/hp元方法** 在数值上生成了参考解。计算区域被定义为 $\Omega : [−7.5, 28.5] \times [−20, 20] \times [0, 12.5]$，其中的坐标已通过圆柱体的直径进行了无量纲化。圆柱体的中心位于 $(x, y) = (0, 0)$。我们假设在入口边界 $x = −7.5$ 处，速度是均匀的($u = 1$)。在侧向边界 $y = \pm 20$ 处使用周期性边界条件，在出口 $x = 28.5$ 处指定为零压力。此外，在圆柱体表面上施加了无滑移边界条件。本案例研究的控制方程是雷诺数 $Re = 200$ 的无量纲不可压缩纳维-斯托克斯方程。

在此配置下，绕圆柱体的尾流是三维且非定常的。模拟一直进行到漩涡脱落流变得稳定为止。最后，收集时间相关的数据用于PINN的训练和评估。三维流的模拟结果如图2所示，其中图2(a)展示了由流向速度 $u$ 着色的流向涡量等值面（$\omega_x = −0.3$）。在本节中，我们对基于有限数据的三维流重构问题感兴趣，并且我们只关注尾流中的一个子区域，即 $\Omega_s: [1.5, 7.5] \times [−3, 3] \times [4, 9]$，它在图2(a)中由一个蓝色边框的立方体表示。三个速度分量和压力场的等高线图如图2(b)所示。一个 $61 \times 61 \times 26$ 的欧拉网格被用于绘图。为了展示运动的非定常性，我们考虑了50个时间快照，时间步长 $\Delta t = 0.2$，这大约是两个漩涡脱落周期。

在这里，我们的目标是应用PINN从少数几个二维平面上的速度观测值来重构三维流场。如图3所示，本文考虑了三种不同的“实验”设置：

- **案例1**：观测两个 $x$ 平面 $(x = 1.5, 7.5)$，一个 $y$ 平面 $(y = 0)$ 和两个 $z$ 平面 $(z = 4.0, 9.0)$。
    
- **案例2**：观测两个 $x$ 平面 $(x = 1.5, 7.5)$，一个 $y$ 平面 $(y = 0)$ 和一个 $z$ 平面 $(z = 6.4)$。
    
- **案例3**：观测一个 $x$ 平面 $(x = 1.5)$，一个 $y$ 平面 $(y = 0)$ 和一个 $z$ 平面 $(z = 6.4)$。
    
我们注意到，对于这些横截平面，只有投影向量（两个分量）被认为是已知的。例如，在z平面上可以观测到速度 $(u, v)$，而与之正交的分量 $(w)$ 是未知的。这样做的目的是为了模仿真实实验中的**平面粒子图像测速法 (planar particle image velocimetry)**。此外，这些二维双分量（2D2C）观测的分辨率是不同的，具体可以在**表1**中找到。

表1：PINN在不可压缩流中的案例研究：2D2C观测的详细信息。

| 横截面 (Cross-section) | 观测到的速度分量 (Observed velocity components) | 观测到的空间分辨率 (Observed spatial resolution) |
| :------------------ | :-------------------------------------: | :-------------------------------------: |
| x-plane             |                $(v, w)$                 |             $61 \times 26$              |
| y-plane             |                $(u, w)$                 |             $61 \times 26$              |
| z-plane             |                $(u, v)$                 |             $61 \times 61$              |
**图3：PINN在不可压缩流中的案例研究：从2D2C观测重构三维流场的问题设置。** 
**案例1：** 观测两个 $x$ 平面 $(x = 1.5, 7.5)$，一个 $y$ 平面 $(y = 0)$ 和两个 $z$ 平面 $(z = 4.0, 9.0)$。
**案例2：** 观测两个 $x$ 平面 $(x = 1.5, 7.5)$，一个 $y$ 平面 $(y = 0)$ 和一个 $z$ 平面 $(z = 6.4)$。 
**案例3：** 观测一个 $x$ 平面 $(x = 1.5)$，一个 $y$ 平面 $(y = 0)$ 和一个 $z$ 平面 $(z = 6.4)$。
注意，对于这些横截平面，只测量了投影向量。目标是使用PINN，从这些2D2C的观测数据中推断出所研究区域内的三维流动。
![[week1_paperlearning5.png]]

> [!TIP]- 注释
> 
> 这一整段的核心就是**设计一个巧妙的逆向工程问题**。我们可以把它分成两步来理解：
> 
> 第一步，如何制作一份完美的“标准答案”；
> 第二步，如何从这份答案中抽出几页不完整的“考卷”来为难PINN。
> 
> ---
> 
> #### 1. 制作“标准答案”——生成参考数据
> 
> - **问题背景:**
>     作者选择了一个非常经典的流体力学问题：**圆柱绕流。** 他们设定的 **雷诺数 $Re = 200$** 是一个特殊的“性格值”，在这个值下，流体绕过圆柱后会产生一种非常漂亮的、交替脱落的漩涡，我们称之为 **“卡门涡街”**，并且这种流动是三维非定常的（即复杂且随时间变化）。
>     
>     
>- **获取答案**: 
>     作者使用了一个非常精确的传统CFD求解器（**基于谱/hp元方法** ）完整地模拟了这个问题。这就得到了一个极其精确的、包含所有时空点上完整速度和压力信息的 **“标准答案”** 或 **“参考解”**。图2展示的就是这个“标准答案”的一部分。
>     
> - **划定考区**: 
>     他们不打算让PINN解决整个大问题，而是只截取了后面涡流最激烈的一小块区域（图2a中的**蓝色立方体 $\Omega_s$**）作为“考区”。
>     
> 
> ---
> 
> #### 2. 设计“考卷”——为PINN准备不完整的观测数据
> 
> 这是最关键、最巧妙的一步。作者并没有把蓝色方块里的“标准答案”直接给PINN，而是模拟了一个 **非常逼真的实验场景**。
> 
> - **核心理念**: 
>     模仿一种叫做 **“平面粒子图像测速法 (PIV)”** 的真实实验技术。这种技术就像用一台特殊的“激光相机”给流体“拍照”，但它一次只能拍下一个**二维平面**，并且通常只能测出这个平面内的**两个速度分量**。
>    
> - **2D2C观测 (二维双分量)**: 
>     这就是“考题”的核心难点。2D代表测量是在一个二维平面上，2C代表只测量了两个分量 (Component)。
>     
>     - 比如，**表1** 告诉我们，在一个垂直于x轴的平面上（x-plane），我们只能测到左右方向的速度 $v$ 和上下方向的速度 $w$，而**无法测到**前后方向的速度 $u$。
>         
> - **三个难度递增的“考试案例”**: 作者设计了三个案例，信息量越来越少：
>     
>     - **案例1**: “考卷”上有5张不完整的“照片”（5个2D2C平面数据）。
>         
>     - **案例2**: 减少到4张“照片”。
>         
>     - **案例3**: 进一步减少到只有3张“照片”。
>         
> - 终极挑战:
>     
>     PINN的任务就是，在只拿到这几张极其稀疏、且信息不完整的“照片”的情况下，能否运用它对物理定律（纳维-斯托克斯方程）的理解，“脑补”出“考区”内所有点在所有时刻完整的、三维的速度场和压力场？
>     
> 
> 这就像一个医生，只看了你身体的几张不完整的CT切片，就要在脑中重建出你整个器官的三维动态模型。这充分展示了PINN在解决实际工程和实验中常见的、数据极其有限的 **逆问题** 方面的巨大潜力。

> [!NOTE]- 图3的解释
> 
> 我们可以把图中的绿色“面条”（涡流结构）想象成一个藏在暗室里的、形状复杂的**隐形雕塑**。我们的目标，就是通过“光”照亮它的几个截面，来推断出整个雕塑的全貌。
> 
> ---
> 
> #### 我们的“探测器”——观测平面
> 
> 图中那些半透明的彩色平面，就是我们用来“拍照”的“探测器”。
> 
> - **x-plane (黄色)**: 一张**竖直**的探测平面，它与x轴垂直。
>     
> - **y-plane (青色)**: 一张**竖直**的探测平面，它与y轴垂直。
>     
> - **z-plane (品红色)**: 一张**水平**的探测平面，它与z轴垂直。
>     
> 
> ---
> 
> #### 三场难度递增的“考试”
> 
> 这三张子图代表了为PINN精心设计的三场难度递增的考试，考验它在信息越来越少的情况下，表现会如何。
> 
> - 案例1 (图a): “信息最丰富的考试”
>     
>     这是最简单的一场考试。我们一共放置了5个探测平面来收集数据：2个黄色的(x-plane)，1个青色的(y-plane)，和2个品红色的(z-plane)。这是PINN能获得信息最多的一次。
>     
> - 案例2 (图b): “信息有所减少”
>     
>     难度升级。我们减少了一个水平的探测平面，总共只剩下4个平面。
>     
> - 案例3 (图c): “信息最稀疏的挑战”
>     
>     这是最难的一场考试。我们只在每个方向上各放一个探测平面，总共只有3个平面可以用。
>     
> 
> ---
> 
> #### 关键的“限制条款”
> 
> **“注意，对于这些横截平面，只测量了投影向量。”** 
> 
>     这句话再次强调了我们之前讨论过的**2D2C (二维双分量)** 概念。这就像我们的“相机”有缺陷：
> 
> - 在任何一张2D探测平面上，我们**只能**测量到速度向量在该平面内的**投影**，也就是两个速度分量。
>     
> - 我们**永远无法**测量到垂直于该平面的那一个速度分量。
>     
> 
> ---
> 
> ### 最终目标
> 
> 这张图设定的最终目标非常明确：PINN这位“考生”，能否仅凭这几张**稀疏的 (sparse)**、且信息**不完整的** 2D“照片”，就利用自己对物理规律的理解，“脑补”出整个隐形雕塑（三维流场）的完整、精确样貌？
> 
> 科学家们通过对比这三个案例的结果，就可以系统地评估PINN对数据稀疏性的鲁棒性。

> [!NOTE]- 参数选择的背后逻辑
> #### 1. 为什么选择雷诺数 $Re = 200$？
> 
> **一句话回答：** 因为 $Re = 200$ 是一个流体力学研究中的“黄金”数值，它所对应的流动现象足够复杂，能体现出PINN的价值，但又没有复杂到完全混乱（湍流），从而保证了问题有一个清晰、稳定的“标准答案”。
> 
> - **详细解释**：
>     
>     - 雷诺数是流动的“性格”判据。对于圆柱绕流问题：
>         
>         - **`Re` 很低时** (比如 $\lt 40$): 流动非常平稳，像蜂蜜一样，我们称之为“层流”。这个问题太简单了，用来测试PINN有点“大材小用”。
>             
>         - **`Re` 很高时** (比如 $\gt 1000$): 流动会变成完全混乱、不可预测的“湍流”，就像湍急的瀑布。这个问题太复杂了，很难得到一个稳定、可重复的“标准答案”。
>             
>         - **`Re` 在200附近时**: 这是一个完美的“过渡区”。流动开始变得不稳定，在圆柱体后面会周期性地、交替地脱落下一串非常规则、漂亮的漩涡（我们称之为“**卡门涡街**”），并且流动形态开始呈现**三维特性**。
>             
>     
>     * 作者在文中也确认了这一点：
>     
>         **“在此配置下，绕圆柱体的尾流是三维且非定常的 (3D and unsteady)”** ，并且拥有**“稳定的漩涡脱落流 (stable vortex shedding flow)”** 。
>     
>     - 因此，选择 $Re = 200$ 是为了创造一个**有代表性的、足够复杂（三维、非定常）、但又有清晰规律**的 benchmark（基准测试问题），非常适合用来检验一个新算法的性能。
>         
> 
> ---
> 
> #### 2. 时间步长 $\Delta t=0.2$ 是如何无量纲化的？
> 
> **是的，这个时间步长是经过无量纲化的。**
> 
> - **标准是什么？** 无量纲化的标准是 **“流体流过一个圆柱体直径所需的时间”**。
>     
> - **详细解释**：
>     
>     - 论文中明确提到了两个无量纲化的标准量：
>         
>         1. **特征长度 L：圆柱体的直径 D** (“coordinates are non-dimensionalized by the diameter of the cylinder”) 。
>         
>         2. **特征速度 V：来流的速度 U** (“velocity (u = 1) is uniform at the inflow boundary”) 。
>         
>     - 有了特征长度和特征速度，我们就可以定义一个**特征时间 $t^*$**：
>         
>         $$t^∗=\frac{特征速度}{特征长度}​=\frac{U}{D}​$$
>         
>     - 这个 $t^*$ 就代表了流体以入口速度 $U$，正好流过一个圆柱体直径 $D$ 所需要的时间。所有的时间，包括时间步长 $\Delta t$，都是用这个 $t^*$ 作为单位来衡量的。所以 $\Delta t=0.2$ 的意思是 $0.2 \times t^*$，即 $0.2 \times (D/U)$ 秒。
>         
> 
> ---
> 
> #### 3. 为什么选择时间步长 $\Delta t=0.2$？
> 
> **一句话回答：** 因为这个步长选择得“不疏不密”，正好可以用50个时间点清晰地捕捉到大约两个完整的漩涡脱落周期。
> 
> - 详细解释:
>     
>     * 论文中给出了最直接的线索：
>     
>     **“我们考虑了50个时间快照，时间步长 $\Delta t=0.2$，这大约是两个漩涡脱落周期 (two periods of the vortex shedding cycle)”**。
>     
>     - 我们可以计算一下：
>         
>         - $总时长 = 时间步数 \times 时间步长 = 50 \times 0.2 = 10$ 个无量纲时间单位。
>             
>         - 这10个时间单位包含了大约2个漩涡脱落周期。
>             
>         - 所以，**一个完整的漩涡脱落周期 T 大约是 $10 / 2 = 5$ 个无量纲时间单位**。
>             
>     - 那么，$\Delta t=0.2$ 意味着什么呢？这意味着在一个完整的漩涡脱落周期内，我们采集了 $T / \Delta t = 5 / 0.2 = 25$ 个数据点（快照）。
>         
> - **结论**: 
>     用25张照片去记录一个完整的周期性动作（比如一个钟摆的来回摆动），这个采样频率是相当合理的，它足以清晰地捕捉到这个动态过程的细节，而又不至于产生过多冗余的数据。这表明作者选择 $\Delta t=0.2$ 是为了在计算成本和结果精度之间取得一个很好的平衡。

> [!NOTE]- 确定漩涡脱落周期的三种途径
> #### 核心概念：斯特劳哈尔数 (Strouhal Number, St)
> 
> 在回答这个问题前，需要先介绍一个关键的无量纲数：**斯特劳哈尔数 (St)**。
> 
> - **它是什么？** 它是一个描述**流动振荡频率**的无量纲数。对于卡门涡街来说，它就代表了漩涡脱落的无量纲频率。
>     
> - **它和周期的关系**：频率 $f$ 和周期 $T$ 互为倒数 ($T = 1/f$)。所以，无量纲周期 $T$ 和斯特劳哈尔数 $St$ 也互为倒数：
>     
>     $$T_{dimensionless}​=\frac{1}{St}$$​
>     
> 
> 所以，“知道漩涡脱落周期”，本质上就是“知道 $Re=200$ 时的斯特劳哈尔数是多少”。
> 
> ---
> 
> #### 途径一：“站在巨人的肩膀上”——查阅文献
> 
> 这是**最主要、最常用**的方式。
> 
> **圆柱绕流**是流体力学中被研究得最透彻、最经典的问题，没有之一。在过去一百多年里，无数的科学家和工程师通过实验和模拟，已经把不同雷诺数下，斯特劳哈尔数$St$的数值测定得非常精确了。
> 
> - **对于 $Re = 200$ 这个工况，$St$ 的值约等于 0.2** ** 是一个广为人知的、写在教科书里的经典数据。
>     
> - 因此，作者在设计实验前，根本不需要自己去测量，直接可以从文献或手册中查到这个“标准值”。
>     
> 
> 我们可以用这个值来验证一下：
> 
> $$周期T=\frac{1}{St}\approx\frac{1}{0.2}​​=5$$
> 
> 这和我们上一轮根据论文中的数据（总时长10，包含2个周期）反推出来的结果**完全一致**！这表明作者正是基于这个已知的周期 $T=5$ 来设计他们的采样时间和步长的。
> 
> ---
> 
> #### 途径二：“先试运行一下看看”——从自己的模拟中直接测量
> 
> 作者为了得到“标准答案”，本身就需要运行一个非常精确的传统CFD模拟（用Nektar求解器）1。
> 
> 在进行用于PINN训练的数据采集之前，他们完全可以先把这个CFD模拟跑起来，让流场发展一段时间。当稳定的“卡门涡街”出现后，他们可以直接在模拟结果中进行测量：
> 
> - **监测升力系数**: 
>     流体绕过圆柱时，由于漩涡的交替脱落，会在圆柱上产生一个周期性上下波动的力（升力）。他们可以画出这个升力系数随时间变化的曲线。
>     
> - **测量周期**: 
>     这条曲线会呈现出非常规律的正弦波形状。直接测量这个波的周期，就能得到极其精确的漩涡脱落周期 $T$。
>     
> 
> 这就像你想知道一个摆钟的周期，最直接的办法就是看着它来回摆动几次，用秒表掐一下时间。
> 
> ---
> 
> #### 途径三：“使用经验公式”——套用成熟的公式
> 
> 对于这个雷诺数区间的圆柱绕流，科学家们已经总结出了一些非常准确的、拟合实验数据的**经验公式**，可以直接计算出 $St$ 和 $Re$ 的关系。他们也可以通过这些公式直接算出 $St$ 的值。
> 
> ### 结论
> 
> 综上所述，作者**不是在猜测**这个周期，而是在实验开始前，通过查阅大量已有的、公开发表的文献，或者通过自己初步的CFD模拟，就已经**精确地知道**了在$Re=200$这个条件下，漩涡脱落的周期大约是5个无量纲时间单位。这使得他们能够在此基础上，游刃有余地设计出合理的采样方案。

### 3.2 PINN的实现

给定在横截平面上的观测数据，我们训练一个PINN模型来逼近整个时空域上的流场。本节中的PINN以 $(x,t) = (x, y, z, t)$ 为输入，并输出速度和压力 $(u, v, w, p)$。PINN中的损失函数可以定义为：
$$
L = L_{data} + L_{PDE}, \tag{4}
$$
其中
$$
\begin{align}
L_{data} = &\frac{1}{N_u}\sum_i^{N_u}||u(X_{data}^i,\ t_{data}^i)-u_{data}^i||^2 \\
        &+\frac{1}{N_v}\sum_i^{N_v}||v(X_{data}^i,\ t_{data}^i)-v_{data}^i||^2 \tag{5}\\
        &+\frac{1}{N_w}\sum_i^{N_w}||w(X_{data}^i,\ t_{data}^i)-w_{data}^i||^2, \\
\end{align}
$$
以及
$$
\begin{align}
L_{PDE} &= \frac{1}{N_f}\sum_i^{N_f}\sum_j^{4}||f_j(X_f^i,t_f^i||^2, \tag{6}\\
f_{1,2,3} &=\frac{\partial u}{\partial t}+(u \cdot \nabla)u+\nabla p-\frac{1}{Re}\nabla^2u, \\
f_4 &= \nabla \cdot u.
\end{align}
$$
数据损失 $L_{data}$ 由三个分量组成，其训练数据量（即 $N_u$, $N_v$ 和 $N_w$）取决于观测平面的数量、每个平面的数据分辨率以及快照的数量。另一方面，$L_{PDE}$ 的残差点可以被随机选取，这里我们在所研究的时空域 $\Omega_s$ 中采样了 $N_f = 3 \times 10^6$ 个点。注意，在本研究中，与经典设置不同，初始条件和边界条件是不需要的。此外，没有给定任何关于压力的信息。各项损失项的权重系数都等于1。

我们采用了一个包含8个隐藏层、每层200个神经元的全连接神经网络。每个神经元的激活函数是 $\sigma = \sin(·)$。我们应用带小批量（mini-batch）的ADAM优化器进行网络训练，其中数据点和残差点的批量大小都使用了 $N = 10^4$。网络总共训练了150个轮次（epochs），学习率在前50个轮次为 $1 \times 10^{−3}$，中间50个轮次为 $5\times 10^{−4}$，最后50个轮次为 $1\times 10^{−4}$。训练结束后，在欧拉网格上对速度和压力场进行评估，以进行比较和可视化。

**图4：PINN在不可压缩流中的案例研究：针对不同流动重构设置的速度和压力的相对L2范数误差。** 这三个案例对应于图3中所示的案例。误差是在整个研究区域上计算的。

![[week1_paperlearning6.png]]

> [!TIP]- 注释
> 
> 这一部分详细描述了这场“考试”的具体细节：**考卷是如何设计的**，**考试有哪些特殊规则**，以及**这位“考生”（PINN）的智力水平和学习计划是怎样的**。
> 
> ---
> 
> #### 1. “考卷”的设计——损失函数
> 
> 总的考试成绩
> 
> $L$ 由两门主课的成绩相加而成，并且这两门课同等重要（权重都为1）。
> 
> - 主课一：数据填空题 ($L_{data}$)
>     
>     * 这部分考查PINN的预测与我们已知的、稀疏的2D2C观测数据是否吻合。
>     
>     - 它会检查在所有我们安放了“探测器”的点上，PINN预测的速度 $u, v, w$ 与“标准答案”中的速度差了多少。这个差值的平方和，就是这门课的扣分。
>         
> - 主课二：物理综合论述题 ($L_{PDE}$)
>     
>     * 这部分考查PINN给出的完整三维流场，是否在 **所有地方** 都遵守了流体力学的基本定律——**纳维-斯托克斯方程**。
>     
>     - 作者在这里采样了**三百万个**随机点，在每一个点上都检查PINN的答案是否满足 $f=0$。$f$ 就是纳维-斯托克斯方程的“残差”，它应该等于 $0$。
>         
>     - 这个 $f$ 分为两部分：
>         
>         * **$f_{1,2,3}$ (动量守恒)**: 
>             描述了流体运动的“力平衡”。可以理解为：流体的**惯性力** = **压力** + **粘性力（摩擦力）**。
>         
>         * **$f_4$ (质量守恒)**: $\Delta \cdot u = 0$ 是不可压缩流的质量守恒方程，意思是对于任何一个微小的空间，**流进去的水必须等于流出来的水**，水不会凭空产生或消失。
>         
> 
> ---
> 
> #### 2. 特殊的“考试规则”
> 
> 这次考试有几个非常规的、出人意料的规则，旨在凸显PINN的强大能力：
> 
> * **无需初始/边界条件 (No BC/IC)**: 
>     作者明确指出，这次训练**没有**使用我们之前讨论的初始条件损失 $L_{IC}$ 和边界条件损失 $L_{BC}$。这很惊人！传统的CFD方法是绝对离不开这些条件的。这里的潜台词是：我们给的内部数据点（$L_{data}$）虽然稀疏，但其包含的信息已经足够强大，足以“锁定”正确的解，使得额外的边界条件变得不再是必需的了。
> 
> * **压力完全未知 (No pressure information)**: 
>     “考卷”上所有已知的观测数据**只涉及速度**，没有任何关于压力的提示。但最终PINN却需要同时给出准确的速度场和压力场。这意味着PINN必须利用物理定律中速度和压力之间的内在联系（$\Delta p$ 项），仅从速度信息中“悟出”整个压力场应该是怎样的。
> 
> ---
> 
> #### 3. “考生”与“学习计划”
> 
> - “考生”的配置 (网络架构)
>     
>     * 这是一个拥有 **8个隐藏层、每层200个神经元** 的深度神经网络。8 你可以把它想象成一个“大脑”有8层楼，每层楼有200个房间，是一个规模相当大的“大脑”。
>     
>     * 激活函数选择了 **$\sin(·)$**。这是一个有趣的选择，因为正弦函数是周期性的，这可能有助于网络更好地学习像“卡门涡街”这样具有周期性规律的流动。
>     
> - “考生”的学习计划 (训练过程)
>     
>     * **优化器**: 使用我们熟悉的 **ADAM** 优化器。
>     
>     * **分批学习**: 采用**小批量 (mini-batch)** 的方式，每次只从三百万个物理采样点和一堆数据点中随机抽取一小部分（$10^4$个）来计算损失和更新参数，这可以大大提高训练效率。
>     
>     * **复习轮次**: 总共学习 **150轮 (epochs)**。
>     
>     * **动态学习率**: 学习的“步长”是动态调整的。前50轮用大学习率 $10^{-3}$ 快速入门；中间50轮用中等学习率 $5\times 10^{-4}$ 稳步提升；最后50轮用小学习率 $10^{-4}$ 精雕细琢，进行微调。

> [!NOTE]- 解释纳维-斯托克斯方程的核心项 公式（6）
> 
> #### 1. 公式一：$f_{1,2,3}$ (动量守恒方程)
> 
> > $f_{1,2,3}​=\frac{\partial u}{\partial t} +(u \cdot \nabla)u+\nabla p−\frac{1}{Re}\nabla^2​u$
> 
> 这个公式本质上就是牛顿第二定律 **$F=ma$** 在流体中的“豪华升级版”。它描述了一个微小的流体团块在各种力的作用下是如何加速的。整个公式被写成了 $ma - F = 0$ 的形式，所以 $f$ 就是那个应该等于 $0$ 的“残差”。
> 
> 我们可以把它拆成四个部分来看：
> 
> - **$\frac{\partial u}{\partial t}$ (本地加速度)**: 
>     这是 $ma$ 中的一部分。想象你站在河边的一个固定点，你感觉到风速（流速）随时间越来越快。这种在**固定点**上感受到的速度随**时间**的变化，就是本地加速度。
>     
> - **$(u \cdot \nabla)u$ (迁移加速度)**: 
>     这也是 $ma$ 中的一部分，但更微妙。想象你坐在一艘皮划艇上，随波逐流。河流从宽阔平缓的区域流入狭窄湍急的峡谷。虽然你没有划桨，但因为你**被水流带到了一个速度更快的新位置**，所以你还是加速了。这种由于**位置移动**导致的速度变化，就是迁移加速度。
>     
> - **$\nabla p$ (压力梯度力)**: 
>     这是 **$F$** (力) 的一部分。$\nabla p$ 指向压力增长最快的方向。流体受到的力总是从高压区推向低压区。就像挤牙膏，你后面施加的高压把牙膏从管口这个低压区挤出去。
>     
> - **$\frac{1}{Re}\nabla^2​u$ (粘性力/摩擦力)**: 
>     这也是 **$F$** 的一部分。它代表了流体内部的“摩擦力”。$Re$是雷诺数，所以$1/Re$ 代表了粘性的大小。想象一下搅动水和搅动蜂蜜的区别，蜂蜜的粘性力远大于水，更能抵抗运动。
>    
> 
> ---
> 
> #### 2. 公式二：$f_4$ (质量守恒/连续性方程)
> 
> > $f_4 = \nabla \cdot u$
> 
> 对于像水这样的**不可压缩流体**，这个方程的完整形式是 $\nabla \cdot u = 0$。
> 
> - **$\nabla \cdot u$ (散度 - Divergence)**: “散度”这个词听起来很抽象，但它的物理意义非常直观。它衡量的是在一个无限小的点上，流体是在“汇聚”还是在“发散”。
>     
> - **“魔法水槽/龙头”的比喻**:
>     
>     - 想象在水流中有一个我们看不见的、无限小的点。
>         
>     - 如果这个点的散度 **$\nabla \cdot u \gt 0$**，就意味着流体正从这个点向四周“发散”出去，就好像这里藏着一个 **凭空产生水的“魔法龙头”**。
>         
>     - 如果  **$\nabla \cdot u \lt 0$，就意味着四周的流体正向这个点“汇聚”并消失，就好像这里藏着一个**不断吞噬水的“魔法水槽”**。
>         
>     - 对于水这种不可压缩的流体，它既不会凭空产生，也不会凭空消失。因此，在任何一个点，**流进去的量必须等于流出来的量**。所以它的散度必须**处处为0**，即  **$\nabla \cdot u = 0$。** 这就是不可压缩流体的质量守恒定律。
>         
> 
> ---
> 
> #### 3. 关键区别: $u \cdot \nabla$ (算子) vs. $\nabla \cdot u$ (标量)
> 
> 现在我们来解决你最关心的问题。这两个表达式看起来很像，但意义完全不同。
> 
> 我们可以把 $\nabla$ 想象成一个“提问器”，它专门用来问关于“空间变化”的问题。
> 
> - **$\nabla \cdot u$ (散度)**:
>     
>     - **结构**: 
>         $提问器 \cdot 向量$
>         
>     - **过程**: 
>         这是“提问器” $\nabla$ 直接向速度向量 $u$ **提问**。它问的问题是：“嘿，速度 $u$，在你所在的这个点，你整体上是倾向于 **“流入”** 还是 **“流出”**？”
>         
>     - **结果**: 
>         运算的结果是一个**标量**（一个数字），这个数字回答了这个问题（正数代表流出，负数代表流入，0代表不多不少）。
>         
> - **$(u \cdot \nabla)$ (方向导数算子)**:
>     
>     - **结构**: 
>         $(向量 \cdot 提问器)$
>         
>     - **过程**: 
>         这不是提问，而是速度向量 $u$ **正在对“提问器” $\nabla$ 进行重新编程**！它对 $\nabla$ 说：“你先别问所有方向上的变化。现在，请你变成一个**只问‘沿着我 $u$ 的方向’上的变化**的新提问器”。
>         
>     - **结果**: $u \cdot \nabla$ 运算的结果**不是一个数字**，而是一个**新的算子（一个新的提问器）**。这个新算子 $(u \cdot \nabla)$ 的功能就是：计算任何一个量沿着流体自身流动方向上的变化率。
>         
> 
> **总结一下**：
> 
> |表达式|类别|含义|中文比喻|
> |---|---|---|---|
> |**$\nabla \cdot u$**|运算 → **标量**|散度 (Divergence)|“提问器”问 $u$ 一个问题|
> |**$(u \cdot \nabla)$**|运算 → **算子**|方向导数算子 (Directional Derivative Operator)| $u$ 给“提问器”设定一个方向|
> 这绝对是核心中的核心问题！`∇` 这个符号在物理和工程中无处不在，理解它是如何“计算”的，就等于掌握了描述空间变化的语言。

> [!NOTE]- $\nabla$ (Nabla / Del 算子) 的计算方法详解
> 
> #### 1. 首先，最重要的概念：$\nabla$ 不是一个数，而是一个“指令包”
> 
> 你不能说 $\nabla=5$ 或者 $\nabla=(1,2,3)$。它本身没有一个具体的数值。
> 
> 它是一个**算子 (Operator)**，就像数学中的 $+$ 号或者 $d/dx$ 一样。它是一个“指令”，告诉你要对跟在它后面的函数或向量做什么操作。
> 
> **$\nabla$ 这个“指令包”里装了什么？**
> 
> 它里面装的是对**各个空间方向求偏导数**的指令。在一个三维直角坐标系中，它的定义是：
>$$\nabla = (\frac{\partial}{\partial x},\ \frac{\partial}{\partial y},\ \frac{\partial}{\partial z})$$
>
> - **$\frac{\partial}{\partial x}$ (对 $x$ 求偏导):** 意思是“只看在 $x$ 方向上的变化率，暂时忽略 $y$ 和 $z$ 方向的变化”。
>     
> - **“爬山”的比喻**:
>     
>     - 想象你站在一座山上，山的高度 $h$ 是关于你的位置 $(x, y)$ 的函数 $h(x, y)$。
>         
>     - $\partial h/\partial x$ 就是当你**锁死 $y$ 方向（南北方向）不动，只沿着 $x$ 方向（东西方向）走** 的时候，感受到的坡度。
>         
>     - $\partial h/\partial y$ 就是当你**锁死 $x$ 方向，只沿着 $y$ 方向走** 的时候，感受到的坡度。
>         
> 
> ---
> 
> #### 2. $\nabla$ 的三种基本“用法”（计算方式）
> 
> 这个“指令包”有三种主要的用法，对应着三种不同的物理意义和计算过程。
> 
> ##### **用法一：计算梯度 (Gradient) $\nabla f$**
> 
> - **操作对象**: 
>     一个**标量**函数 $f$ (比如温度 $T(x,y,z)$ 或压力 $p(x,y,z)$)。
>     
> - **计算过程**: 
>     把 $\nabla$ 直接“作用”在 $f$ 上，就是把它指令包里的每个指令都对 $f$ 执行一遍。
>$$\nabla f=(\frac{\partial f}{\partial x},\ \frac{\partial f}{\partial y},\ \frac{\partial f}{\partial z})$$
>
> - **结果**: 
>     一个**向量**。
>     
> - **物理意义**: 
>     这个新得到的向量被称为**梯度**。在“爬山”的比喻中，梯度向量 $\nabla h$ 就是你脚下指向 **“最陡的上坡方向”** 的那个箭头，箭头的长度就代表了坡度到底有多陡。
>     
> 
> ##### **用法二：计算散度 (Divergence) $\nabla \cdot u$**
> 
> - **操作对象**: 
>     一个**向量**函数 $u$ (比如速度场 $u(x,y,z) = (u_x, u_y, u_z)$)。
>     
> - **计算过程**: 
>     把 $\nabla$ 和向量 $u$ 进行**点乘 (dot product)**。点乘的规则是对应分量相乘后再相加。
 >   $$\begin{align}\nabla \cdot u&​=(\frac{\partial}{\partial x},\ \frac{\partial}{\partial y},\ \frac{\partial}{\partial z})\cdot (u_x​,\ u_y​,\ u_z​) \\&=(\frac{\partial u_x}{\partial x}+ \frac{\partial u_y}{\partial y}+\frac{\partial u_z}{\partial z})\end{align}​​​$$
> - **结果**: 
>     一个**标量**（一个数）。
>     
> - **物理意义**: 
>     这就是我们之前讨论过的**散度**。它衡量了在一个点上，流体是在“发散”（魔法龙头）还是在“汇聚”（魔法水槽）。
>     
> 
> ##### **用法三：计算旋度 (Curl) $\nabla \times u$**
> 
> - **操作对象**: 
>     也是一个**向量**函数 $u$。
>     
> - **计算过程**: 
>     把 $\nabla$ 和向量 $u$ 进行**叉乘 (cross product)**。
>$$\begin{align} \nabla \times u &= \left |\begin{array}{ccc}i&j&k\\\frac{\partial}{\partial x}&\frac{\partial}{\partial y}&\frac{\partial}{\partial z}\\u_x​&u_y​&u_z​\end{array}\right|\\&=(\frac{\partial u_z}{\partial y}- \frac{\partial u_y}{\partial z},\ \frac{\partial u_x}{\partial z}-\frac{\partial u_z}{\partial x},\ \frac{\partial u_y}{\partial x}-\frac{\partial u_x}{\partial y})\end{align}$$     
> - **结果**: 
>     一个**向量**。
>     
> - **物理意义:** 
>     **旋度**衡量了流体在一个点上的**旋转程度**。在“流场”的比喻中，如果你在流场中某点放一个极小的、可以自由旋转的叶轮，$\nabla \times u$ 这个向量的方向就是叶轮的旋转轴，向量的长度就代表了叶轮旋转的速度。它和我们之前提到的**涡量 (Vorticity)** 密切相关。
>     
> 
> ---
> 
> #### 总结
> 
> $\nabla$ 就像一个多功能的“数学瑞士军刀”，根据你和它组合的对象（标量或向量）以及组合方式（直接作用、点乘、叉乘），它会执行完全不同的计算，并得到具有不同物理意义的结果。
> 
> |运算|公式|作用对象|结果类型|物理意义|
> |---|---|---|---|---|
> |**梯度**| **$\nabla f$** |标量 $f$ |**向量**|指向最大增长率的方向（最陡的上坡）|
> |**散度**| **$\nabla \cdot u$** |向量 $u$|**标量**|衡量源或汇的强度（发散或汇聚）|
> |**旋度**|**$\nabla \times u$** |向量 $u$|**向量**|衡量场的旋转程度（漩涡的强度和方向）|
>
>---
>
>#### 补充：**$\nabla \times u$ (旋度) 的方向**
>**$\nabla \times u$ (旋度) 这个向量的方向，正是由“右手法则” (Right-hand Rule) 来决定的。**
> ##### 旋度与右手法则
> 
> 我们可以用“微型叶轮”的比喻来清晰地理解这一点：
> 
> 1. **想象一下**，在流体中某一点，水流正在打旋，形成了一个小漩涡（比如逆时针旋转）。
>     
> 2. 你把一个极小的、可以在水中自由旋转的叶轮放在这个点上，它会跟着水流一起**逆时针转动**。
>     
> 3. 现在，拿出你的**右手**，让你的**四指**（食指到小指）弯曲的方向与叶轮**旋转的方向保持一致**（即逆时针方向）。
>     
> 4. 此时，你**竖起的大拇指**所指向的方向，就是**旋度向量 $\nabla \times u$ 的方向**。
>     
> 
> ##### 例子：浴缸里的漩涡
> 
> - 当你拔掉浴缸的塞子，水流会形成一个漩涡。如果从上往下看，这个漩涡是**逆时针**旋转的。
>     
> - 你用右手，让四指顺着逆时针方向弯曲。
>     
> - 你的大拇指会**指向上方**，垂直于水面。
>     
> - 这就意味着，在这个漩涡的中心，旋度向量 $\nabla \times u$ 是一个指向上方的向量，它的方向就是漩涡的旋转轴。
>     
> 
> ---
> 
> ##### 与涡量 (Vorticity) 的关系
> 
> 在流体力学中，**涡量向量 $\omega$** 的严格数学定义就是速度场的**旋度**：
>$$\omega=\nabla \times u$$
> 因此，**涡量向量的方向也完全遵循右手法则**。它不仅告诉我们流体在旋转，还用一个向量精确地描述了这个旋转的“轴”在哪个方向，以及旋转的“剧烈程度”（由向量的长度表示）。
> 

> [!NOTE]- 图4的解释
> 好的，这张图是第一个案例研究的**“成绩单”**。它用数据清晰地展示了，在我们为PINN设置的三场难度不同的“考试”（Case 1, 2, 3）中，它分别取得了怎样的成绩。
> 
> 这张图是本次案例研究中最重要的定量结果。它告诉我们，PINN从那些稀疏、不完整的2D数据中反向推断出的三维流场，和那个由`Nektar`求解器算出的“标准答案”相比，到底有多大的偏差。
> 
> ---
> 
> #### 如何“阅卷”——理解坐标轴的含义
> 
> - **X轴 (t)**: 代表**时间**。图表展示了误差是如何从`t=0`到`t=10`这个时间段内动态变化的。
>     
> - **Y轴 (ε [%])**: 代表**“相对L2范数误差”**。这是一个衡量“总体误差百分比”的指标。
>     
>     - **误差**: 指的是PINN的预测值和“标准答案”之间的差距。
>         
>     - **L2范数**: 一种计算**总体误差**大小的方式。你可以把它理解成，我们计算了整个蓝色立方体内**所有点**的误差，然后用一种方式（平方、求和、再开方）把它们平均成一个单一的数值。
>         
>     - **相对**: 将这个总体误差值，除以“标准答案”本身的“大小”，再换算成**百分比**。
>         
> 
> **一句话总结Y轴**: **`ε` 的值越小，说明PINN的预测结果越接近“标准答案”，即考试成绩越好。**
> 
> ---
> 
> #### 分析“成绩单”——四张图表的结论
> 
> 这四张图分别是PINN在四个“科目”（`u`速度、`v`速度、`w`速度、`pressure`压力）上的成绩单。我们可以从中读出三个核心结论：
> 
> **1. “信息越多，成绩越好” (More Data, Better Result)**
> 
> - 这是最明显、最重要的结论。在**每一张**图里，我们都能看到：
>     
>     - 💙 **蓝色实线 (Case 1)**: 拥有最多的数据（5个平面），它的误差曲线**最低**，成绩最好。
>         
>     - ❤️ **红色虚线 (Case 2)**: 数据量居中（4个平面），它的误差曲线也处在**中间**。
>         
>     - ⚫ **灰色长虚线 (Case 3)**: 数据最少（3个平面），它的误差曲线**最高**，成绩最差。
>         
> - **这证明了**: PINN的重构精度与其获得的观测数据量**正相关**。给它的线索越多，它推断出的结果就越准确。
>     
> 
> **2. “各科成绩有所不同” (Performance Varies by Subject)**
> 
> - **u速度 (`u velocity`) 和 压力 (`pressure`)**: 这两门课PINN学得最好。即便是最难的Case 3，`u`速度的误差也基本在3%以下，表现非常出色。
>     
> - **w速度 (`w velocity`)**: 这是PINN最“偏科”的一门。它的相对误差明显高于其他科目。这通常是因为 `w` 速度本身的数值（量级）在流动中就比 `u` 和 `v` 小得多。当一个很小的真实值有了一点点绝对误差时，计算出的“相对误差”百分比就会显得特别大。
>     
> 
> **3. “开考和结尾时，表现稍差” (Worse Performance at Endpoints)**
> 
> - 仔细观察可以发现，很多曲线在 `t=0` 和 `t=10` 这两个时间点附近，误差有轻微的“抬头”趋势。
>     
> - **原因**: 这就像让你预测一个序列的下一个数字，如果你在序列的中间，你可以参考前后的数字来预测；但如果你在序列的**末尾**，你就没有“后面”的数字可以参考了。PINN在计算时间导数时也类似，在时间域的两个端点，由于缺少前方或后方的信息，其计算精度会略微下降。
>     
> 
> ---
> 
> ### 最终结论
> 
> 这张图有力地证明了，即使只提供非常有限和不完整的2D数据，PINN也能够成功地、以较高的精度重构出复杂的三维非定常流场。它清晰地展示了PINN作为一种数据融合和逆问题求解工具的强大潜力。

好的，这段话是对上一节图4中“成绩单”的官方解读和分析，并引出了下一张图（图5）作为更直观的视觉证据。

---
### 3.3 推断结果

为了进行定量评估，我们首先定义相对L2范数误差作为评估指标，其表达式为：
$$
\epsilon_V=\frac{||V_{CFD}-\hat V||^2}{||V_{CFD}||^2}\times 100\%, \tag{7}
$$
其中 $V$ 可以是 ${u, v, w, p}$ 之一；$V_{CFD}$ 和 $\hat V$ 分别代表CFD数据和PINN的输出。我们在所研究的时间域内计算误差，结果如图4所示。从图中可以看出，对于案例1和案例2，PINN都能非常精确地推断出三维流动；使用5个平面（案例1）比使用4个平面（案例2）的效果略好。当只有3个横截平面可用时（案例3），误差变得大得多。然而，案例3的结果仍然是可以接受的，因为我们能够高精度地推断出主要的流动特征（流向速度的误差大部分时间低于2%）。我们注意到，速度 $w$ 的误差比其他分量要大，因为速度 $w$ 的量级本身相对较小。此外，我们观察到在初始和最终时刻存在较大的偏差，这可以归因于在 $t \lt 0$ 和 $t \gt 10$ 时缺少用于计算导数的训练数据。这种情况在非定常案例中没有提供初始条件的情况下通常会发生。

为了将更多细节可视化，我们重点展示案例2的结果，并在图5中演示 $t=8.0$ 时刻的涡量大小等值面和压力等值面，其中图5(a)显示了参考的CFD数据，图5(b)显示了由PINN推断的结果。涡量值为 $|ω| = 1.2$，颜色代表流向速度分量。可以看出，PINN的推断结果（从少数2D2C观测中推断）与CFD模拟非常一致。此外，在图5(c)中绘制了单个点 ($x=3, y=0, z=6.4$) 的速度 $(u, v)$ 随时间变化的曲线，从中我们可以发现PINN能够非常精确地捕捉到涡流脱落的非定常性。

**图5：PINN在不可压缩流中的案例研究：案例2的PINN推断结果。** (a) $t=8.0$ 时刻来自CFD数据的涡量大小等值面（上）和压力等值面（下）。 (b) $t=8.0$ 时刻由PINN推断出的涡量大小等值面（上）和压力等值面（下）。 (c) 单点 $(x=3, y=0, z=6.4)$ 的速度 $(u, v)$ 随时间变化的测量。 在此案例中，三维流动是由PINN从四个横截平面中推断出来的。

![[week1_paperlearning7.png]]

> [!TIP]- 注释
> 
> 这段话分为两部分。第一部分是**“成绩分析报告”**，作者详细解读了图4数据背后的原因。第二部分是**“视觉成果展示”**，作者拿出了“画作”本身，让我们眼见为实。
> 
> ---
> 
> #### 1. 定量分析：深入解读“成绩单”
> 
> - **打分标准 (公式7):**
>     
>     - 作者首先明确了图4中“相对误差 $\epsilon$”的计算公式。
>         
>     - **一个比喻**: 
>         把它想象成**“画像的相似度得分”**。$V_{CFD}$ 是真人的高清照片，$\hat V$ 是PINN画出的肖像画。这个公式计算的就是 $(画像与照片的总差异大小) / (照片本身的信息量)$。得分越低，说明画得越像。
>         
> - **成绩解读:**
>     
>     - **基本结论**: 
>         作者确认了我们之前的观察：案例1和2非常准确，案例1略好于案例2。案例3误差大得多，但因为最重要的 $u$ 速度误差很小（低于2%），所以结果“仍可接受”。
>         
>     - **解释“偏科”现象 (w速度误差大)**: 
>         作者给出了官方解释：**因为速度 $w$（上下方向）的数值本身就很小**。
>         
>         - **“胡须问题”比喻**: 
>             想象一下画一幅人像。你把鼻子、眼睛、嘴巴这些主要特征（如同 $u$, $v$ 速度）都画得完美无缺。但有一根很细的胡须（如同 $w$ 速度），你画偏了半毫米。这个半毫米的绝对误差很小，但**相对于胡须本身的粗细**，可能就偏了50%，所以“相对误差”就显得特别大。
>             
>     - **解释“开考/结尾怯场”现象**: 
>         作者也确认了我们的推测：因为在 $t=0$ 和 $t=10$ 这两个时间边界上，模型**缺少之前或之后的数据**来帮助它精确计算时间导数，所以误差会略大一些。
>         
> 
> ---
> 
> #### 2. 视觉展示：眼见为实的证据
> 
> 数字是抽象的，图片更直观。作者选择信息量适中的**案例2**，向我们展示PINN的“画作”到底有多逼真。
> 
> - **整体结构对比 (图5a vs 5b)**:
>     
>     - 作者向我们展示了 $t=8.0$ 这个瞬间，PINN重构出的三维涡流结构（图5b）和“标准答案”的CFD结果（图5a）的对比。
>         
>     - **结论是：“非常一致 (very consistent)”**。这就像用人脸识别对比两张照片，系统会判定“匹配成功”。这说明PINN不仅在数值上，在三维形态上也完美地还原了流场的复杂结构。
>         
> - **动态过程对比 (图5c)**:
>     
>     - 作者在流场中“放置”了一个虚拟传感器，记录某一个固定点上的速度 $u$ 和 $v$ 是如何随时间波动的。
>         
>     - **结论是：PINN（虚线）和CFD（实线）的曲线几乎完美重合**。
>         
>     - **“心跳监控”比喻**: 这就像一个心跳监护仪，CFD的曲线是病人的真实心跳，PINN的曲线是仪器实时描绘出的心跳。两条线完美重合，说明PINN**非常精确地捕捉到了涡流脱落的“节奏”和“韵律”**，也就是流动的**非定常性**。
>         
> 
> 综上，作者从**定量**和**定性**两个方面，充分证明了PINN在这个极具挑战性的三维重构逆问题上的成功。

> [!NOTE]- 图5的解释
> 
> 如果说图4是本次考试的**数字“成绩单”**，那么图5就是**“试卷”的直观展示**，让我们能亲眼看看PINN的“画作”和“标准答案”到底有多像。作者在这里选择了数据量适中的**案例2**作为代表。
> 
> ---
> 
> #### 1. “大家来找茬”：三维结构的视觉对比 (图a vs 图b)
> 
> 这部分对比了在 `t=8.0` 这个时间瞬间，流场的**三维空间结构**。
> 
> * **图(a) 标准答案**: 
>     这是由强大的传统CFD求解器算出的**真实流场** 1。上面是涡量大小的等值面，下面是压力的等值面。你可以把它看作是**“真人的高清照片”。
> 
> * **图(b) PINN的答卷**: 
>     这是PINN仅根据4个不完整的2D平面数据，**重构出的整个三维流场**。你可以把它看作是“PINN画出的肖像”**。
> 
> **结论**: 
> 肉眼几乎看不出任何区别。无论是涡流的复杂形状、扭曲和位置，还是压力的分布形态，PINN的“画作”都与“照片”**高度一致**。这提供了一个强有力的视觉证据，证明PINN成功地还原了流场的三维空间结构。
> 
> ---
> 
> #### 2. “心跳监护仪”：动态过程的精确捕捉 (图c)
> 
> 这部分对比了在一个**固定的空间点**上，速度**随时间演变**的动态过程。
> 
> - **图(c)是什么**: 作者在流场中选择了一个固定的点 `(x=3, y=0, z=6.4)`，然后记录下这个点的`u`速度（上图）和`v`速度（下图）是如何随时间`t`从0到10进行周期性波动的。
>     
>     - 💙 **蓝色实线 (CFD data)**: 代表“标准答案”中这个点的真实速度变化，如同**“病人的真实心跳”**。
>         
>     - ❤️ **红色虚线 (PINN)**:  代表PINN预测的该点速度变化，如同 **“心跳监护仪上显示的曲线”**。
>         
> 
> **结论**: 两条曲线**几乎完美地重合**在一起 。这表明PINN不仅重构了流场的静态空间结构，还极其精确地捕捉到了涡旋脱落导致的 **周期性振荡**，也就是流动的**“节奏”和“韵律”**。
> 
> ---
> 
> ### 最终结论
> 
> 这张图从 **空间** 和 **时间** 两个维度，直观且有力地证明了PINN的强大能力。即便只给它4个不完整的2D平面数据，它也能够通过学习物理规律，近乎完美地反演出整个复杂、动态的三维流场。
> This section introduces the second case study, which moves from the relatively slow, incompressible flow of the first example to the challenging world of high-speed, compressible aerodynamics.
## 4 压缩流案例研究

PINNs也已被用于模拟高速流动[13]。 在本节中，我们考虑以下二维定常可压缩欧拉方程：
$$
\nabla \cdot f(U) = 0, \quad x \in \Omega \subset \mathbb R^2, \tag{8}
$$
其中
$$
\begin{align}
U &= [\rho,\ \rho u,\ \rho v, \ \rho E]^T, \\
f &= (G_1,\ G_2)
\end{align}
$$其中 $G_1(U)=[\rho u, \ p+\rho u^2, \ \rho uv, pu+\rho uE]$，$G_2(U)=[\rho v, \ \rho uv, \ p+\rho v^2, \  pv+\rho vE]$。
这里，$\rho$ 是密度，$p$ 是压力，$[u, v]$ 是速度分量，$E$ 是总能量。我们使用附加的状态方程来封闭上述欧拉方程，它描述了压力和能量之间的关系。例如，我们考虑多方气体的状态方程，由下式给出：
$$
p = (\gamma-1)(\rho E-\frac{1}{2}\rho||u||^2), \tag{9}
$$
其中 $\gamma$ 是绝热指数，$u = (u,v)$。

我们将使用PINN来解决可压缩欧拉方程(8)的逆问题。具体来说，我们将基于密度梯度的信息、有限的压力数据（物体表面的压力）、入口条件和全局物理约束，来使用PINN推断密度、压力和速度场。

#### 4.1 问题设置

我们考虑一个二维弓形激波问题。对于传统的CFD模拟，正确实施扮演着重要角色的边界条件是至关重要的。然而，对于高速流中的激波问题，真实实验中的边界条件通常是未知的，只能进行近似估计。在当前的工作中，我们没有使用传统CFD模拟所需的大部分边界条件，而是基于密度梯度 $\nabla \rho$ 的数据来使用PINN求解欧拉方程(8)，这样做的动机是实验中可用的纹影摄影技术；此外，我们使用了由压力传感器获得的有限的表面压力数据，以及全局约束（质量、动量和能量）。我们在这里也使用了入口条件。注意，与一维情况不同（在[13]中使用了整个计算域的密度梯度），在这里我们只使用了计算域 $\Omega$ 的一个子域 $D$ 内的密度梯度，即 $D \subset \Omega$。 通过结合数学模型和给定数据，我们得到PINN的加权损失函数如下：
$$
\begin{align}
Loss = \ &\omega_1Loss_F + \omega_2Loss_{\nabla \rho|_D}+ \omega_3Loss_{inflow}\\
&+\omega_4Loss_{p^*} +\omega_5(Loss_{Mass}+Loss_{Monemtum}+Loss_{Energy})\\ \tag{10}
&+\omega_6Loss_{n \cdot u}, 
\end{align}
$$
其中最后一项对应于物体表面的速度条件。

> [!TIP]- 注释
> 
> 这个案例研究将难度提升到了一个新水平，从水流中的漩涡转向了超音速飞行器周围的**激波**。作者设计这个问题的核心目的，是展示PINN如何解决一个**对传统CFD方法极不友好，但却非常贴近真实实验**的场景。
> 
> ---
> 
> #### 1. 新的挑战：可压缩流与激波
> 
> * **可压缩流 (Compressible Flow)**: 
>     当流体（通常是气体）的运动速度非常快（接近或超过音速）时，它的**密度**就不再是一个常数，而是会被急剧压缩。这种情况下的物理规律由**欧拉方程**（Navier-Stokes方程的无粘性简化版）描述。
> 
> * **弓形激波 (Bow Shock Wave)**: 
>     想象一颗子弹在空气中高速飞行。它会把自己前方的空气急剧压缩，形成一个像船头波纹一样的、非常薄的边界，在这个边界上，空气的压力、密度和温度会瞬间剧增。这个边界就是**激波**。
> 
> ---
> 
> #### 2. 传统方法的“痛点”与PINN的“机会”
> 
> 作者指出了一个非常实际的难题：
> 
> - 在做传统CFD模拟时，你必须精确地告诉计算机所有边界上的条件（比如激波后面的出口边界，那里的气流参数是什么）。
>     
>     * 但在真实的超音速实验中，你往往 **根本不知道** 这些边界条件是什么。你无法在超音速气流中轻易地安放传感器去测量它们。
>     
> 
> 这就造成了一个矛盾：传统方法需要一些真实实验给不了的数据。而PINN恰好可以解决这个问题，因为它可以用 **另一种真实实验能给出的数据** 来代替。
> 
> ---
> 
> #### 3. PINN的“考卷”：一套全新的、基于实验的“线索”
> 
> 在这个逆问题中，作者没有给PINN传统的边界条件，而是给了它一套非常规但更真实的“线索”：
> 
> * **密度梯度 $\nabla \rho$ (最关键的线索)**: 
>     在实验中，有一种叫做**纹影摄影 (Schlieren photography)** 的技术。它不能直接拍到激波，但可以拍到由密度急剧变化引起的光线扭曲，就像夏天滚烫的路面上方空气的扭曲一样。这张“光影照片”直接反映了**密度梯度**（密度变化最快的地方）的分布。这等于给了PINN一张标示出激波位置的“藏宝图”。
> 
> * **有限的表面压力**: 
>     在飞行器模型表面贴上几个**压力传感器**，可以得到几个离散点的精确压力值。
> 
> * **入口条件**: 
>     我们知道远前方（上游）的气流参数是怎样的。
> 
> * **全局约束**: 
>     保证整个计算区域内的**总质量、总动量、总能量是守恒的**。这就像一个全局的会计准则，确保收支平衡。
> 
> ---
> 
> #### 4. 全新的“记分卡”——损失函数
> 
> 对应于这些新“线索”，PINN的损失函数（“记分卡”）也被相应地重新设计了：
> $$\begin{align}Loss = \ &\omega_1Loss_F + \omega_2Loss_{\nabla \rho|_D}+ \omega_3Loss_{inflow}\\ &+\omega_4Loss_{p^*} +\omega_5(Loss_{Mass}+Loss_{Monemtum}+Loss_{Energy})\\&+\omega_6Loss_{n \cdot u}, \end{align}$$
> 
> - **物理定律得分 ($Loss_F$)**: 
>     检查解是否满足欧拉方程。
>     
> - **“对上藏宝图”得分 ($Loss_{\nabla \rho|_D}$)**: 
>     检查解的密度梯度是否和“纹影照片”给出的激波位置对得上。
>     
> - **入口条件得分 ($Loss_{inflow}$)**: 
>     检查解是否满足已知的入口条件。
>     
> - **“对上传感器读数”得分 ($Loss_{p^*}$)**: 
>     检查解在模型表面的压力是否和传感器读数对得上。
>     
> - “会计查账”得分 ($Loss_{Mass}$等): 
>     检查解是否满足全局的质量、动量、能量守恒。
>     
>     * **表面速度得分 ($Loss_{n \cdot u}$)**: 检查流体是否会穿透模型表面（法向速度应为0）。
>     
> 
> 通过最小化这个复杂的损失函数，PINN就能在缺少传统边界条件的情况下，仅凭这些“实验友好型”数据，反推出整个复杂的激波流场。

这是一个非常好的问题，它触及了PINN损失函数设计的核心思想——**如何将具体的物理信息精确地转化为数学形式**。

我们来把 `ω2Loss∇ρ|D` 这个表达式像化学式一样拆解开，看看每个部分的含义和作用。

> [!NOTE]- **$\omega_2Loss_{\nabla \rho|_D}$表达式详解**
> 
> $\omega_2Loss_{\nabla \rho|_D}$表达式触及了PINN损失函数设计的核心思想——**如何将具体的物理信息精确地转化为数学形式**。
> 
> #### 1. **$\omega_2$ (Omega 2) - 权重：这门“科目”有多重要？**
> 
> - **它是什么？** 
>     $\omega_2$ 是一个由研究者设定的**权重系数**（一个数字）。
>     
> - **它起什么作用？** 
>     在总损失函数 
>     $$\begin{align}Loss = \ &\omega_1Loss_F + \omega_2Loss_{\nabla \rho|_D}+ \omega_3Loss_{inflow}\\ &+\omega_4Loss_{p^*} +\omega_5(Loss_{Mass}+Loss_{Monemtum}+Loss_{Energy})\\ &+\omega_6Loss_{n \cdot u} \end{align}$$
>     中，每个 $\omega$ 都代表了对应那项损失的**重要程度**。
>     
> - **高中生比喻**: 
>     把它想象成你期末总成绩的计算方法。如果 $\omega_1$（物理定律）是50%，$\omega_2$（密度梯度）是30%，那么后者对你总成绩的影响就小于前者。$\omega_2$ 的作用就是**调整“科目”之间的权重平衡**。
>     
> 
> ---
> 
> #### 2. $Loss$ - **损失：一个“扣分项”**
> 
> - **它是什么？** 
>     $Loss$ 是一个统称，表明这一整项是一个**损失项**，或者说是一个“扣分项”。它的值越小越好。
>     
> 
> ---
> 
> #### 3. $\nabla \rho$ (Nabla rho) - **核心内容：匹配“激波的影子”**
> 
> - **它是什么？** 
>     $\nabla \rho$ 的数学含义是 **“密度梯度”**。我们之前讨论过，梯度 $\nabla$ 衡量的是一个量的变化有多快、方向是怎样。所以 $\nabla \rho$ 就描述了 **密度在空间中变化最剧烈的地方和方向**。
>     
> - **它在这里起什么作用？**
>     
>     - 在超音速激波问题中，流场大部分区域的密度是缓慢变化的，只有在**激波**那条非常狭窄的线上，密度会发生 **“跳崖式”的剧变**。
>         
>     - 因此，**$\nabla \rho$ 的值在激波处会变得极大，而在其他地方则接近于0**。
>         
>     - 这就意味着 $\nabla \rho$ 就像是**激波投下的“影子”**，它能精确地勾勒出激波的轮廓和位置。
>         
> - **高中生比喻**: 
>     实验中的“纹影照片”就像一张拍摄了“激波影子”的底片。$Loss_{\nabla \rho}$ 这项损失，就是让PINN“画”出来的激波影子，和这张真实的“底片”进行比对，**看两个影子的形状和位置是否匹配**。如果不匹配，就要扣分。
>     
> 
> ---
> 
> #### 4. $|_D$ (下标 D) - **计算范围：“在指定的区域内”**
> 
> - **它是什么？** 
>     这个下标 $|_D$ 是一个限制条件，它告诉我们，上面那个“匹配影子”的操作，**不是在整个计算区域 $\Omega$ 中都进行，而仅仅在一个我们特别指定的子区域 $D$ 内进行**。
>     
> - **论文中的依据**: 
>     作者在原文中明确提到了这一点：“...在这里我们只使用了计算域 $\Omega$ 的一个子域 $D$ 内的密度梯度，即 $D \subset \Omega$”。
>     
> - **为什么要这样做？**
>     
>     1. **提高效率**: 
>         激波只存在于流场的一部分区域。我们没必要在那些已知没有激波的地方（比如远离飞行器的上游）浪费计算资源去匹配一个几乎为0的密度梯度。
>         
>     2. **集中火力**: 
>         研究者可以根据经验，把子区域 $D$ 划在他们预估激波会出现的核心区域。这就像告诉PINN：“别的先不管，你必须集中所有精力，先把这个核心区域内的激波给我搞对了！”
>         
> 
> ---
> 
> ### 总结
> 
> 把所有部分组合起来，$\omega_2Loss_{\nabla \rho |_D}$ 这项损失的完整含义就是：
> 
> **“在预先设定的关键子区域 $D$ 内，计算PINN预测的密度梯度 $\nabla \rho$ 与真实实验数据（纹影照片）的差异，并将这个差异（扣分）乘以其重要性权重 $\omega_2$，作为总成绩的一部分。”**
> 
> 它体现了PINN框架的灵活性——我们可以根据实际问题和可用数据的特点，设计出非常有针对性的“考题”来训练网络。

### 4.2 推断结果

为了展示PINN在可压缩流中的有效性，我们考虑一个弓形激波问题，其入口流动条件如下：
$$
\begin{cases}
\ M_{\infty}= 4, \\
\ p_{\infty} = 101253.6\ Pa, \\
\ \rho_{\infty} = 1.225 \ kg/m^3, \\ \tag{11}
\ u_{\infty} = 1360.6963 \ m/s, \\
\ v_{\infty} = 0, 
T_{\infty} = 288 \ K.
\end{cases}
$$
压力的数据点位于物体表面。通过使用上述入口条件和CFD代码，我们可以获得定常状态的流场。我们在图6的左图中展示了由CFD计算出的密度。我们采用了一个 $6 \times 60$（6个隐藏层）的神经网络，并使用层级自适应tanh激活函数[35]和Adam优化器进行训练，学习率为 $6 \times 10^{-4}$，训练了 $3 \times 10^{5}$ 个轮次（epochs）。在这里，我们也使用了动态权重的技术[36, 11]。训练损失的历史记录显示在图6的右图中。PINN对压力和速度($u$)的求解结果显示在图7中。可以观察到，PINN的解与CFD数据高度吻合。这表明，我们可以使用一些不同于传统CFD模拟所需的边界条件的、其他可用的知识，来重构高速流的流场。

**图6：PINN在可压缩流中的案例研究。** 左图：使用入口流动条件(11)通过CFD模拟获得的密度。 右图：训练损失与训练轮次（epochs）的关系。
![[week1_paperlearning8.png]]

**图7：PINN在可压缩流中的案例研究。** PINN解与CFD解的比较。 上方：压力 $p$，下方：速度分量 $u$。

![[week1_paperlearning9.png]]

> [!TIP]- 注释
> 
> 这段话是在告诉我们这次“超音速考试”的具体参数，以及PINN这位“考生”是如何配置的，并最终公布了“考试成绩”。
> 
> ---
> 
> #### 1. “考题”的背景设定
> 
> - **$M_{\infty}= 4$**: 这是最重要的参数——**马赫数 (Mach Number)** 为4。马赫数是物体速度与音速的比值，$M=4$ 意味着来流的速度是音速的**4倍**，这是典型的高超音速流动，物理现象会非常剧烈。
>     
> - **其他参数**: 诸如压力 $p_{\infty}$、密度 $\rho_{\infty}$、温度 $T_{\infty}$ 等，基本就是标准的海平面大气条件。
>     
> - **“标准答案”的制作**: 作者首先会用这些给定的入口条件，运行一个传统的CFD程序，计算出这个问题的精确解（如图6左图所示），作为后续PINN结果对比的“标准答案”。
>     
> 
> ---
> 
> #### 2. “考生”的配置与“学习计划”
> 
> 为了应对这个高难度问题，PINN这位“考生”也进行了特殊配置和刻苦学习：
> 
> - **大脑结构**: 使用了一个 $6 \times 60$ 的网络（6个隐藏层，每层60个神经元）。
>     
> - 特殊技能:
>     
>     * **自适应激活函数**: 使用了一种更高级的“神经元”，它可以自己调整激活的灵敏度，让学习更高效 。
>     
>     * **动态权重**: 使用了我们之前提过的“动态复习”策略，在训练中智能地调整对不同“科目”（损失项）的关注度。
>     
> - **学习计划**:
>     
>     - **训练强度**: 训练了高达 **30万轮 (epochs)**！这相当于把“教科书”反复精读了30万遍，说明这个问题的求解确实需要大量的计算和优化。
>         
>     - **学习率**: 设定了一个 $6 \times 10^{-4}$ 的学习步长。
>         
> 
> ---
> 
> #### 3. “成绩公布”——结果与意义
> 
> > **“可以观察到，PINN的解与CFD数据高度吻合 (good agreement)。”**
> 
> 这是本节的核心结论。图7将会向我们展示，PINN预测的压力场和速度场，与CFD算出的“标准答案”在视觉上几乎一模一样。
> 
> - **模拟画像师的比喻**:
>     
>     - **标准答案 (CFD)**: 案件现场的一张高清监控照片。
>         
>     - **PINN的线索**: 只有一张模糊的“影子”照片（纹影/密度梯度），和几个目击者（表面压力传感器）的零星证词。
>         
>     - **PINN的“画作”**: PINN这位模拟画像师，仅凭这些不完整的线索，就画出了与高清照片几乎完全一致的嫌疑人画像（压力场和速度场）。
>         
> 
> **最重要的意义**:
> 
> > **“这表明，我们可以使用一些不同于传统CFD模拟所需的边界条件的、其他可用的知识，来重构高速流的流场。”**
> 
> 这句话是本案例研究的点睛之笔。它证明了PINN的价值：在很多真实、困难的工程问题中，我们往往无法获得教科书般完美的边界条件。但PINN提供了一条全新的路径，它可以使用那些在**真实实验中更容易获得**的、不同类型的数据（比如光学测量、表面传感器），来同样精确地解决这些复杂问题。

> [!NOTE]- 图6的解释
> 
> 这张图包含两部分信息：左边是我们要解决问题的“标准答案”，右边是PINN这位“学生”在学习过程中的“学习日记”。
> 
> ---
> 
> #### 左图：“标准答案”——激波的真实样貌
> 
> - **它是什么？** 这是一张**密度 `ρ` 的分布图**。它展示了当4倍音速的气流（从右向左）撞上一个钝头物体（左下角的缺口）时，物体周围空气密度的真实情况。
>     
> - **它是怎么来的？** 这张图是作者用一个**传统的、可靠的CFD软件**，根据4.1节中给出的精确物理条件，花费大量计算资源得到的**精确解**。它在本次研究中扮演着**“标准答案”**或**“高清参考照片”**的角色。
>     
> - **我们能看到什么？** 最引人注目的，就是那条从蓝色突然变成黄红色的、清晰的弧形分界线。这就是我们之前讨论的**弓形激波 (Bow Shock Wave)**。
>     
>     - **蓝色区域**: 激波前方的区域，空气还没被压缩，密度很低。
>         
>     - **红/黄色区域**: 气体穿过激波后，被急剧压缩，密度瞬间升高了好几倍。
>         
> 
> ---
> 
> #### 右图：“学习日记”——损失函数下降曲线
> 
> 这张图记录了PINN在长达30万轮次（epochs）训练中的“学习进步”过程。
> 
> - **Y轴 (Train loss)**: 代表“总扣分”或“犯错程度”。**这个轴是对数坐标**，意味着每下一条横线，数值就缩小10倍。所以，曲线的下降幅度远比看起来的要大。
>     
> - **X轴 (Epoch)**: 代表“学习的轮次”。一个Epoch代表PINN把所有训练数据从头到尾学习了一遍。
>     
> 
> **我们可以从这条曲线读出PINN的学习过程**:
> 
> 1. **初期 (0 - 50,000轮)**: 曲线**急剧下降**。这说明PINN这位“学生”刚开始学习，掌握基础知识非常快，水平飞速提升，“犯错程度”从超过10分迅速降低到1分以下。
>     
> 2. **中期 (50,000 - 300,000轮)**: 曲线**缓慢下降并趋于平稳**。这说明基础知识已经掌握，PINN正在对各种难题和细节进行“精雕细琢”，进步速度变慢，但仍在持续优化。
>     
> 3. **曲线上的“毛刺”**: 这些突然向上的尖峰是完全正常的。因为训练是分批次（mini-batch）进行的，偶尔PINN会遇到一批特别“难”的数据点，导致“本次小测验”的“扣分”暂时升高，但它很快就能调整过来，继续保持整体下降的趋势。
>     
> 
> **最终结论**: 这张图最重要的信息是**曲线的整体下降趋势**。它从一个很高的值，最终稳定在了一个非常低的值上（远低于0.1）。这清晰地表明**训练是成功的**，PINN这位“学生”已经充分学习了我们给它的“实验线索”和“物理定律”，达到了“学霸”的水平。

> [!NOTE]- 图7的解释
> 
> 这张图就是超音速案例研究的最终“成绩单”。它将PINN这位“模拟画像师”最终画出的“作品”，与CFD模拟出的“高清参考照片”并排放在一起，让我们进行最直观的比较。
> 
> ---
> 
> #### 1. 上方两图：压力场 `p` 的对比
> 
> - **左图 (Data for p)**: 这是由传统CFD软件精确计算出的、流场中**压力**的真实分布情况（“高清照片”）。你可以看到，在激波处，压力从蓝色（低压）瞬间跳变到了青色/黄色（高压）。
>     
> - **右图 (PINN solution: p)**: 这是PINN在只利用“纹影照片”（密度梯度）和几个表面压力点的情况下，**独立推断**出的整个压力场分布（“模拟画像”）。
>     
> 
> **对比结论**: 两张图几乎一模一样。无论是激波的位置、形状、弯曲弧度，还是物体表面和激波后方区域的压力分布，PINN的重构结果都与“标准答案”高度吻合。
> 
> ---
> 
> #### 2. 下方两图：速度场 `u` 的对比
> 
> - **左图 (Data for u)**: 这是CFD计算出的、流场中**x方向速度分量 `u`** 的真实分布情况。你可以看到，气体穿过激波后，速度急剧下降（从红色变为黄色/青色）。
>     
> - **右图 (PINN solution: u)**: 这是PINN推断出的速度场 `u` 的分布。
>     
> 
> **对比结论**: 同样，两张图在视觉上几乎无法区分。PINN非常准确地捕捉到了流速在穿过激波时的剧烈变化以及在整个流场中的平滑过渡。
> 
> ---
> 
> ### 最终结论
> 
> 这张图提供了决定性的视觉证据：
> 
> 即使在没有传统边界条件，仅依靠**非常规的、贴近真实实验的稀疏数据**的情况下，PINN也能够凭借其对物理定律（欧拉方程）的“理解”，**成功且精确地重构出复杂的高超音速流场**。
> 
> 这充分证明了PINN作为一种新型的、数据与物理相融合的科学计算工具，在解决传统方法难以处理的实际工程和实验问题上，具有巨大的潜力和优势。

> [!NOTE]- 弓形激波问题的初始场景说明
> 我们考虑一个弓形激波问题，其入口流动条件如下：
> 
> $$\begin{cases}\ M_{\infty}= 4, \\ \ p_{\infty} = 101253.6\ Pa, \\ \ \rho_{\infty} = 1.225 \ kg/m^3, \\ \tag{11} \ u_{\infty} = 1360.6963 \ m/s, \\ \ v_{\infty} = 0, T_{\infty} = 288 \ K. \end{cases}$$
> 
> #### 1. 什么是“入口流动条件 (Inlet Flow Conditions)”？
> 
> 想象一下你在做一个**风洞实验**。
> 
> - **风洞 (Wind Tunnel)**: 是一个能产生稳定、均匀气流的巨大管道，用来测试飞机、火箭模型。
>     
> - **入口 (Inlet)**: 就是风洞产生气流的那一端。
>     
> - **入口流动条件**: 就是你设定风洞产生的这股气流的**“出厂参数”**。在你把模型放进风洞之前，这股气流在不受任何干扰时，它本身的速度、压力、密度、温度是多少。
>     
> 
> 在计算机模拟中，这个概念是完全一样的。我们设定的计算区域（一个虚拟的“风洞”）的**入口边界**处的流动参数，就是“入口流动条件”。
> 
> ---
> 
> #### 2. 下标 `∞` (无穷远) 代表什么？
> 
> 这是一个在空气动力学中非常标准的符号。下标 `∞` 并不真的指无限远的地方，而是指**“远前方”**或者**“自由来流 (Freestream)”**的条件。
> 
> - **含义**: 指的是气流在**完全不受**你要研究的那个物体（比如钝头体）**任何干扰时**的状态。这个状态就等同于我们设定的“入口流动条件”。
>     
> - **为什么用`∞`？**: 因为从物体的角度看，这股气流来自非常非常远（无穷远）的地方，在抵达物体之前，它就是这个状态。
>     
> 
> ---
> 
> #### 3. 各个参数的详细解释
> 
> 这段话里的每一个参数，都在描述这股来自“无穷远”的、不受干扰的气流的“性格”：
> 
> - **`M∞ = 4` (马赫数)**
>     
>     - **含义**: 这是最重要的参数，代表来流的速度是当地音速的**4倍**。这是一个**高超音速 (Hypersonic)** 的流动（通常马赫数大于5被定义为高超音速，4是一个典型的超音速/高超音速边界案例）。
>         
> - **`p∞ = 101253.6 Pa` (压力)**
>     
>     - **含义**: `Pa` (帕斯卡) 是压力的单位。这个数值约等于**一个标准大气压**。
>         
> - **`ρ∞ = 1.225 kg/m³` (密度)**
>     
>     - **含义**: 这是空气在标准海平面高度的**平均密度**。
>         
> - **`T∞ = 288 K` (温度)**
>     
>     - **含义**: `K` (开尔文) 是热力学温度单位。288K 大约等于 **15°C**，这也是一个标准的**海平面大气温度**。
>         
> - **`u∞ = 1360.6963 m/s` (x方向速度)**
>     
>     - **含义**: 这是气流沿x轴方向的速度，大约是**每秒1.36公里**。这个速度非常快，作为对比，普通民航客机的飞行速度大约是250 m/s。
>         
> - **`v∞ = 0` (y方向速度)**
>     
>     - **含义**: 气流在y方向上没有速度分量。这意味着这股气流是**沿着x轴笔直地吹过来**的，没有侧风。
>         
> 
> ### 总结
> 
> 综合来看，这段话用一组精确的物理数据，定义了一个非常具体的场景：**“在标准的海平面大气条件下，一股4倍音速的、均匀的、笔直的气流，将要撞上我们放置在计算区域中的钝头体。”**
> 
> 这组数据不仅为传统的CFD模拟提供了生成“标准答案”所需的精确输入，也为PINN的损失函数（`Loss_inflow`这一项）提供了必须满足的“已知线索”。

> [!NOTE]- 层级自适应tanh激活函数、Adam优化器、动态权重技术—“考生”使用的高级“学习技巧”详解
> 
> 
> #### 1. 层级自适应tanh激活函数 (layer-wise adaptive tanh activate function)
> 
> 这个技巧是为了让神经网络的“神经元”变得更聪明、更灵活。
> 
> - **激活函数 (Activation Function)**: 它是每个神经元内部的一个“开关”，用来决定这个神经元在接收到信号后，应该被“激活”到什么程度，以及如何将信号传递给下一层。
>     
> - **tanh**: 一种常见的激活函数，它的形状像一个平滑的'S'曲线。
>     
> - **自适应 (Adaptive)**: 关键就在这里。“自适应”意味着这个'S'形开关的**形状不是固定的**，而是可以在训练过程中**自我调整**。就像一个可以自动调节灵敏度的音量旋钮，神经元可以自己学习什么样的“S”形曲线最适合处理当前的数据。
>     
> - **层级 (Layer-wise)**: 更进一步，网络中**每一层**的神经元都可以拥有自己独立的、自适应的激活函数形状。
>     
> 
> **一句话比喻**: 普通神经网络里的所有神经元都使用同一种“固定型号的开关”；而这个技术，则是给**每一层**的神经元都换上了可以**“自我学习、自我调节”的智能开关**，大大提升了网络的学习能力和效率。
> 
> ---
> 
> #### 2. Adam优化器 (Adam Optimizer)
> 
> 我们之前已经详细讨论过这个概念，这里可以快速回顾一下。
> 
> - **它是什么？** Adam是一种高效的、自适应的优化算法，用来指导神经网络如何更新自己的参数（权重和偏置）以降低损失函数。
>     
> - **“智能登山者”比喻**: 它就像一位经验丰富的登山者，在寻找“损失函数”这个复杂地形的最低谷时，同时具备两项法宝：
>     
>     1. **动量 (Momentum)**: 像一个滚下山的雪球，有惯性，可以帮助它更快地冲向谷底，并越过一些小障碍（局部最优解）。
>         
>     2. **自适应学习率**: 能够根据地形的陡峭程度，智能地调整自己每一步的“步长”。在平缓的地方大步走，在险峻的地方小步挪。
>         
> 
> Adam是目前深度学习领域最常用和最可靠的优化器之一。
> 
> ---
> 
> #### 3. 动态权重的技术 (technique of dynamic weights)
> 
> 这个技巧是为了让神经网络在学习过程中，能智能地分配自己的“注意力”。
> 
> - **它是什么？** 在计算总损失 `Loss = ω₁Loss₁ + ω₂Loss₂ + ...` 时，这个技术可以让权重系数 `ω` **不再是固定不变的**，而是可以在训练过程中**动态地自我调整**。
>     
> - **“动态复习计划”比喻**:
>     
>     - 想象一下，PINN的总成绩由好几门“科目”（比如物理定律`Loss_PDE`、数据匹配`Loss_Data`等）决定。
>         
>     - 在复习初期，PINN可能发现自己的“物理定律”这门课学得特别差，分数最低。
>         
>     - 启动了“动态权重”技术后，PINN会自动说：“看来我的物理是弱项，我得把更多的复习时间（即更高的权重`ω`）分配给它！”
>         
>     - 于是，在接下来的训练中，它会更“在意”物理定律的损失，并优先去优化它。当这门课的成绩跟上来之后，它又会动态地调整权重，去关注其他落后的科目。
>         
> 
> 这个技术可以帮助PINN更好地平衡各项复杂的学习任务，防止在训练中出现“偏科”现象，从而更快地达到一个整体最优的解。

> [!NOTE]- 训练中“调整”发生的真实频率
> **上面说的这些调整，都是在完成一个epoch之后调整吗？**
> 问得非常好！这是一个关于“何时”进行调整的关键问题，能帮助你更深入地理解神经网络的训练过程。
>
>答案是：**不是的，恰恰相反，大部分调整的频率比一个epoch快得多！** 它们发生在更小的时间尺度上。
>
>---
>
> 为了理解这一点，我们首先要明确两个关键的时间单位：**Epoch** 和 **Step/Batch**。
> 
> #### 关键的时间单位：Epoch vs. Step/Batch
> 
> 让我们用一个**“复习备考”**的比喻：
> 
> - **数据集 (Dataset)**: 你需要复习的、厚厚的一整本**教科书**。
>     
> - **Epoch (轮次)**: 把这本教科书**从头到尾完整地复习一遍**。你训练了 `3 × 10⁵` 个epochs，就等于你把整本书复习了30万遍。
>     
> - **Batch (批次)**: 因为教科书太厚了，你不可能一口气读完。所以你把它分成一页一页或者一章一章地来读。这每一小部分，就是一个**批次 (Batch)**。
>     
> - **Step/Iteration (步/迭代)**: 每当你读完一页（一个批次）的内容，你就停下来思考一下，更新一下你的知识体系和笔记。这个**“学习-更新”的动作**，就是一步 **(Step)**。
>     
> 
> 所以，**一个Epoch包含了很多个Step**。例如，如果你的“教科书”有1000页，你每次读10页（Batch Size = 10），那么你读完整本书（1个Epoch）就需要 `1000 / 10 = 100` 次学习更新（100个Step）。
> 
> ---
> 
> #### 三种“调整”分别发生在何时？
> 
> 现在我们来看你提到的三种调整：
> 
> **1. Adam优化器 (参数更新)**
> 
> - **调整频率**: **每一步 (At every single Step)！**
>     
> - **解释**: 这是最频繁的调整。神经网络**每“读”完一小批数据 (a mini-batch)，就会立刻计算一次损失，然后Adam优化器马上根据这个损失来更新一次网络的所有参数（权重和偏置）**。它不会等到整个Epoch（整本书）读完才去更新。
>     
> - **比喻**: 这就像一个高效的学习者，他**每看完一页书，就会立刻在旁边做笔记、划重点**，而不是等全书看完才凭记忆去总结。
>     
> 
> **2. 动态权重 (Dynamic Weights)**
> 
> - **调整频率**: **在Epoch之内，但不一定是每一步。**
>     
> - **解释**: 动态权重调整的频率取决于具体的算法。但它的核心思想是**在训练过程中**根据各项损失的表现来调整。它会持续监控“各科小测验”（每个batch的损失）的成绩。
>     
> - **比喻**: 你的“智能导师”（动态权重算法）**不会在你每做完一道题后就调整你的复习计划，但他肯定也不会等到你期末考试考完才调整**。他可能会观察你一个上午的“小测验”成绩，然后发现你的“物理”特别差，于是在下午的复习中，就动态地把你的重心（权重）调整到物理上来。这个调整发生在**一个Epoch（一整天复习）之内**。
>     
> 
> **3. 自适应激活函数 (Adaptive Activation Functions)**
> 
> - **调整频率**: **每一步 (At every single Step)！**
>     
> - **解释**: “自适应”激活函数中那些可以调整的参数（比如控制'S'形曲线有多陡峭的参数），通常被看作是神经网络自身参数的一部分，和权重`W`、偏置`b`的地位是一样的。
>     
> - **比喻**: 当Adam优化器在每一步更新所有“旋钮”时，它会**一视同仁地**把这些“智能开关”上的“调节旋钮”也一起更新了。它们是同一个“学习-更新”循环的一部分。
>     
> 
> ---
> 
> ### 总结表格
> 
> |调整类型|调整频率|中文比喻|
> |---|---|---|
> |**Adam优化器 (参数更新)**|**每一步 (After each Batch)**|看完**一页书**，立刻更新笔记。|
> |**动态权重**|在Epoch之内 (e.g., 每N步)|观察**几场小测验**后，调整复习计划。|
> |**自适应激活函数**|**每一步 (After each Batch)**|看完**一页书**，笔记和开关的设置一起更新。|
> 
> 所以，你的问题非常关键，它揭示了深度学习训练的**高频迭代**本质，这也是它为什么能够学习如此复杂模式的原因。

## 5 生物医学流案例研究

除了前面提到的流动例子，PINNs也已被用于生物医学流。在本节中，我们考虑推断动脉流中一个血栓的材料属性，该问题由纳维-斯托克斯和卡恩-希利尔德方程描述。此类方程可用于描述血栓和血流之间的力学相互作用，作为一个流固耦合（FSI）问题。该偏微分方程（PDE）系统可写为：
$$
\begin{align}
\rho(\frac{\partial u}{\partial t}+u \cdot \nabla u)+\nabla p &= \nabla \cdot(\sigma_{vis}+\sigma_{coh})-\mu\frac{(1-\phi)u}{2\kappa(\phi)}, \tag{12}\\

\nabla \cdot u &= 0, \tag{13}\\

\frac{\partial \psi}{\partial t}+u \cdot \nabla \psi &= 0, \tag{14}\\

\frac{\partial \phi}{\partial t}+u \cdot \nabla \phi &= \tau \Delta \omega, \tag{15}\\

\omega &= \Delta\phi+\gamma g(\phi), \tag{16}
\end{align}
$$
$g(\phi)$ 是双势阱势能 $(\phi^2-1)^2/4h^2$ 的导数。$u(x,t)$, $p(x,t)$, $\sigma(x,t)$ 和 $\phi(x,t)$ 分别代表速度、压力、应力和相场。$h$ 是界面长度；$\psi$ 表示一个辅助向量，其梯度是形变梯度张量 $F$ 的分量，如下所示：
$$
\mathbf{F} :=
\begin{bmatrix}
-\dfrac{\partial \psi_1}{\partial y} & -\dfrac{\partial \psi_2}{\partial y} \\
\dfrac{\partial \psi_1}{\partial x} & \dfrac{\partial \psi_2}{\partial x}
\end{bmatrix}
$$
方程12是包含了粘性应力和内聚应力的纳维-斯托克斯方程，它们可分别写为：
$$
\begin{align}
\sigma_{vis} &= \mu\nabla u, \tag{17}\\

\sigma_{coh} &= \lambda \nabla \cdot(\nabla \phi\otimes \phi). \tag{18}
\end{align}
$$
此外，$\gamma$, $\tau$ 和 $\lambda$ 分别是界面迁移率、弛豫参数和混合能量密度。我们遵循[55]中的归一化方法，记 $\kappa=\frac{k_f}{a^2_f}$，$k_f$ 是真实的渗透率，$a_f$ 是纤维蛋白半径。我们设定密度 $\rho=1$，粘度 $\mu=0.1$，$\lambda=4.2428\times10^{-5}$，$\tau=10^{-6}$，以及界面长度 $h= 0.05$。这些在PINN中的参数都是无量纲化的值，以便与CFD求解器保持一致。

入口 $\Gamma_i$ 的速度被设为狄利克雷边界条件 $u = g,\ (x, t)\in\Gamma_i\times(0,T)$ 。我们在壁面 $\Gamma_w$ 上施加无滑移边界，并为 $\phi$ 和 $\omega$ 在所有边界上施加诺伊曼边界条件，即 $\dfrac{\partial \phi}{\partial n}=\dfrac{\partial \omega}{\partial n}= 0, \ x\in\Gamma_w\cup\Gamma_i\cup\Gamma_o$ 。

---

> [!TIP]- 注释
> 
> 别被这一大堆复杂的方程吓到。这一节的核心思想非常明确，就是用PINN来解决一个极具挑战性但也非常有价值的医学问题。我们可以把它分成三个部分来理解：**问题是什么**，**“游戏规则”有多复杂**，以及**PINN的任务是什么**。
> 
> ---
> 
> #### 1. “大背景”——问题是什么？
> 
> - **研究对象**: 动脉血管中的一个**血栓 (Thrombus)**，也就是血凝块。
>     
> - **核心物理现象**: **流固耦合 (Fluid-Structure Interaction, FSI)**。
>     
>     - **“果冻大坝”的比喻**: 
>         想象一条河里有一个用果冻做成的大坝。河水（**流体**）流过时会冲击果冻坝（**结构**），导致果冻坝变形、晃动。而果冻坝形状的改变，又会反过来影响河水的流动路径和速度。这种**流体和结构之间持续的、双向的相互作用**，就是“流固耦合”。血液（流体）和血栓（可变形的固体结构）之间的相互作用正是如此。
>         
> 
> ---
> 
> #### 2. “游戏规则”——极其复杂的方程组
> 
> 为了描述上面那个“果冻大坝”的复杂行为，科学家们需要用一套耦合在一起的偏微分方程（PDEs）作为“游戏规则”。我们不需要深入每个公式的细节，只需要知道它们各自的**分工**：
> 
> - **方程(12)和(13) - 纳维-斯托克斯方程**:
>     
>     - **职责**: 
>         掌管**“流体”**（血液）的行为。这是我们熟悉的动量守恒和质量守恒定律，但它被修改了，额外增加了一些描述血栓如何与血液相互作用的力。
>         
> - **方程(15)和(16) - 卡恩-希利尔德方程**:
>     
>     - **职责**: 
>         掌管**“结构”**（血栓）的形态。这里的关键是**相场 $\phi$**。
>         
>     - **“成分地图”比喻**: 
>         $\phi$ 就像一张“成分地图”。比如，$\phi=1$ 的地方是纯粹的血液，$\phi=-1$ 的地方是血栓最密的核心，而介于 $-1$ 和 $1$ 之间的值则代表血栓边缘多孔的、像海绵一样的“半流体半固体”区域。这组方程描述了这张“成分地图”是如何随着血流的冲击而移动和演变的。
>         
> - **方程(14) - 形变追踪器**:
>     
>     - **职责**: 
>         追踪血栓这个**“固体”**部分是如何被拉伸、挤压和扭曲的。
>         
>     - **“动作捕捉”比喻**: 
>         想象在血栓上贴满了微小的动作捕捉点。$\psi$ 这个辅助向量就负责追踪这些点的位置，从而计算出整个血栓的**形变**情况。
>         
> 
> ---
> 
> #### 3. PINN的终极任务：执行“无创诊断”
> 
> 在定义了这么多复杂的物理规则和参数后，作者终于亮出了这次案例研究的真正目标——一个非常困难的**逆问题**。
> 
> - **已知**: 
>     我们可以通过医学影像（比如超声）观测到血栓的**形状和运动**（即 $\phi$ 和 $u$ 的部分信息）。我们也知道血液的一些基本属性（比如粘度 $\mu$）。
>     
> - **未知**: 
>     我们不知道这个血栓内部的**材料属性**。具体来说，我们不知道它的**渗透性 $\kappa$ (Permeability)** 是多少。
>     
> - **渗透性 $\kappa$**: 这个参数衡量了血液渗透进血栓这个“多孔海绵”的难易程度。它是诊断血栓类型和危险性的一个关键指标。
>    
> 
> **PINN的任务**:
> 
> **能否只根据外部观测到的血流和血栓形态，就反向推断出这个隐藏在内部的、无法直接测量的关键材料参数 $\kappa$？**
> 
> 这就像医生不需要做手术切开血栓，就能通过外部成像和血流分析，准确诊断出这个血栓的内部结构有多“疏松”或多“紧密”。这是一个极具临床价值的“无创诊断”问题。

> [!NOTE]- 方程组详解：描述“血流中的海绵”
> 好的，这是一组相当复杂的耦合偏微分方程。要完全理解其数学推导需要很深的专业知识，但我们可以从**物理层面**，用比喻的方式，去理解**每一条方程和每一个参数分别在扮演什么角色，它们合在一起要讲述一个怎样的故事**。
>
>这个故事的主角就是：**动脉血管中，血流(流体)与血栓(结构)的相互作用**。
>
> ---
> 
> #### 方程 (13): $\nabla \cdot u = 0$ — **规则一：血液的“体积”是守恒的**
> 
> - **角色**: 
>     **质量守恒方程 (Mass Conservation)**
>     
> - **解释**: 
>     这是最简单的一条。它规定了血液是**不可压缩**的。在我们熟悉的“魔法水槽/龙头”比喻中，这意味着在任何一个微小的空间点，流进去的血液必须等于流出来的血液，血液不会凭空产生或消失。
>     
> 
> ---
> 
> #### 方程 (12): $\rho(\frac{\partial u}{\partial t}+u \cdot \nabla u)+\nabla p = \nabla \cdot(\sigma_{vis}+\sigma_{coh})-\mu\frac{(1-\phi)u}{2\kappa(\phi)}$ — **规则二：血液的“运动”遵循牛顿定律 (F=ma)**
> 
> - **角色**: 
>     **动量守恒方程 (Momentum Conservation / Navier-Stokes)**
>     
> - **解释**: 
>     这是描述血液运动的核心方程，但它是一个“加强版”的纳维-斯托克斯方程，包含了血栓带来的额外作用力。
>     
> - **左边**: 
>     $\rho(\frac{\partial u}{\partial t}+u \cdot \nabla u)$ 是 **$ma$** 项，代表血液的**惯性**。
>     
> - **右边**: 
>     是血液受到的各种**力 $F$** 的总和：
>     
>     - $\nabla p$：**压力**，像水压一样，从高压区推向低压区。
>         
>     - $\nabla \cdot \sigma_{vis}$：**粘性力**，血液内部的摩擦力，让血液感觉“粘稠”。
>         
>     - $\nabla \cdot \sigma_{coh}$：**内聚力**，这个力与相场 $\phi$有关。你可以把它想象成血液和血栓交界处的**“表面张力”**，它是一种维持界面形态的力。
>         
>     - $\mu\frac{(1-\phi)u}{2\kappa(\phi)}$：**渗透阻力**。这是最关键的一项，代表血液流**“穿过”**血栓这个多孔结构时受到的**阻力**。
>         
>         - **“海绵”比喻**: 血栓就像一块海绵。这个力就是水流穿过海绵时感受到的阻力。**$\kappa$(kappa)** 就是**渗透率**，代表海绵孔隙的大小。**$\kappa$ 越大，海绵越疏松，阻力越小。**这个 $\kappa$ 就是我们要反推的未知量**。
>             
> 
> ---
> 
> #### 方程 (15) & (16): $\frac{\partial \phi}{\partial t}+u \cdot \nabla \phi = \tau \Delta \omega$ 和 $\omega = \Delta\phi+\gamma g(\phi)$— **规则三：血栓的“形态”如何变化**
> 
> - **角色**:：
>     **卡恩-希利尔德方程 (Cahn-Hilliard Equations)**，描述相场演化。
>     
> - **解释**：
>     这组方程专门负责描述血栓本身的**形状、边界和运动**。
>     
> - **关键角色 $\phi$（相场）**：
>     $\phi$ 是一张 **“成分地图”**。 比如 $\phi=1$ 是纯血液，$\phi=-1$ 是血栓核心，中间值是两者的混合过渡区。
>     
> - **方程(15)左边**：
>     $\frac{\partial \phi}{\partial t}+u \cdot \nabla \phi$ 描述了这张“成分地图”被血流 $u$ **“裹挟”着移动**的情况。
>     
> - **方程(15)右边与方程(16)**: 
>     描述了除了被动移动外，血栓作为一种“物质”本身想要维持自己形态的趋势。
>     
>     - **“油和水”的比喻**: 就像油滴在水中会尽量保持球形一样，这个力（由 $\omega$ 和 $g(\phi)$描述）会阻止血栓无限地溶解、弥散到血液中，从而维持一个相对清晰的边界。
>         
> 
> ---
> 
> #### 方程 (14): $\frac{\partial \psi}{\partial t}+u \cdot \nabla \psi = 0$ — **规则四：血栓的“变形”如何追踪**
> 
> - **角色**：
>     **形变追踪方程 (Deformation Tracker)**
>     
> - **解释**：
>     这条方程是为了精确追踪血栓这个“固体”部分是如何被血流挤压、拉伸的。
>     
> - **关键角色 $\psi$（Psi）：**
>     $\psi$ 是一个数学上的“辅助工具”，没有直接的物理意义。
>     
> - **“在果冻里画格子”的比喻**：
>     想象血栓是一个透明的果冻。我们在初始状态，用一支无形的笔在果冻内部画上了一个完美的坐标网格。当血流 $u$ 冲刷果冻，使其变形时，内部的网格也会跟着一起扭曲。$\psi$ 的作用就是**追踪这些网格线的运动**。通过计算网格的扭曲程度（即 $\psi$ 的梯度，也就是形变梯度张量 $F$），我们就能精确地知道血栓每一点的形变情况。
>     
> 
> ---
> 
> #### 参数“词典”
> 
> - **主要变量 (要求解的量)**
>     
>     - $u$: 血液的**速度**
>         
>     - $p$: **压力**
>         
>     - $\phi$: **相场** (判断是血液还是血栓的“成分地图”)
>         
>     - $\psi$: **形变追踪**的辅助向量
>         
>     - $\omega$: 相场演化的辅助变量
>         
>     - $\sigma$: **应力** (物体内部的力)
>         
> - **已知或待求的物理参数**
>     
>     - $\rho$: 流体**密度**
>         
>     - $\mu$: 流体**粘度** (粘稠程度)
>         
>     - $\kappa(\phi)$: **渗透率** (血液穿透血栓的难易程度，**这是本次案例的核心未知量**)
>         
>     - $\lambda$: 混合能量密度 (与界面力有关)
>         
>     - $\tau$: 弛豫参数 (相场演化的快慢)
>         
>     - $\gamma$: 界面迁移率 (相场演化的快慢)
>         
>     - $h$: 界面长度 (血栓边界的“模糊”程度)
>         
> 
> **总结**: 这是一个高度耦合的复杂系统。PINN的目标就是，在只知道部分 $u$ 和 $\phi$ 信息的情况下，利用这个庞大的“规则书”（方程组），反向推断出隐藏在规则背后的关键未知参数——渗透率$\kappa$。

> [!NOTE]- $\mathbf{F} :=\begin{bmatrix}-\dfrac{\partial \psi_1}{\partial y} & -\dfrac{\partial \psi_2}{\partial y}  \\ \dfrac{\partial \psi_1}{\partial x} & \dfrac{\partial \psi_2}{\partial x}\end{bmatrix}$ (形变梯度张量) 的详细解释
>
> ### 1. `:=` (冒号等于) 的含义
>
> 这个符号 `:=` 在数学和计算机科学中是一个**定义符号**。
>
> * **它的意思是**: **“左边的东西，从现在开始，被定义为右边的东西”**。
>
> ##### 它和 `=` (等于) 有什么区别？
>
> * **`=` (等于)**: 通常表示一种**陈述或断言**。比如 `x = 5` 是在陈述x的值是5，`a + b = b + a` 是在陈述一个已知的真理。它是一个双向的关系。
> * **`:=` (定义为)**: 表示一种**赋值或命名**。它是一个单向的操作。在使用 `:=` 之前，左边的符号可能还没有任何意义。正是这个定义动作，赋予了它意义。
>
> **“起外号”的比喻**:
> * 你的朋友叫“克里斯托弗·诺兰”。你觉得太长了，于是你说：“我们以后叫他‘诺导’吧！”
> * 这个行为，用数学语言来写就是：**诺导 `:=` 克里斯托弗·诺兰**
> * 你不是在“断言”诺导等于克里斯托弗·诺兰，而是在**“定义”**“诺导”这个新名字作为那个人的简称。
>
> 所以，在公式中，**`F := [...]`** 的意思是：“我们现在创造一个叫做 `F` 的新东西，它的具体内容被定义为后面那个矩阵。”
>
> ---
>
> ### 2. `F` (形变梯度张量) 的详细解释
>
> `F` 的全称是**形变梯度张量 (Deformation Gradient Tensor)**。虽然名字听起来很吓人，但它的工作职责很明确。
>
> * **核心职责**: **精确地、定量地描述一个物体（比如血栓）的每一小块是如何被拉伸、挤压和剪切的。**
>
> ##### 回忆“在果冻里画格子”的比喻
>
> 我们之前说过，`ψ = (ψ₁, ψ₂)` 是一个**形变追踪器**。想象血栓是一块果冻，`ψ` 就代表了果冻内部我们画好的网格。当果冻变形时，`ψ` 就记录了网格是如何扭曲的。
>
> **`F` 则是在 `ψ` 的基础上，进行“总结分析”，给出一份“形变分析报告”。**
>
> ##### `F` 矩阵里每一项的含义
>
> 这个 `F` 矩阵是一个2x2的表格，里面有4个数字，每一项都是一个**偏导数**。我们来解读一下这些偏导数告诉了我们什么信息：
>
> > $F := \begin{bmatrix}
> > -\frac{\partial \psi_1}{\partial y} & -\frac{\partial \psi_2}{\partial y} \\
> > \frac{\partial \psi_1}{\partial x} & \frac{\partial \psi_2}{\partial x}
> > \end{bmatrix}$
>
> * `ψ₁` 是**水平方向**的位移，`ψ₂` 是**竖直方向**的位移。
>
> * **`∂ψ₁/∂x`**: 代表“当你在**水平方向(x)**上移动一小步时，**水平位移(ψ₁)**变化了多少”。这主要衡量了物体在**x方向上的拉伸或压缩**。
>
> * **`∂ψ₂/∂y`**: 代表“当你在**竖直方向(y)**上移动一小步时，**竖直位移(ψ₂)**变化了多少”。这主要衡量了物体在**y方向上的拉伸或压缩**。
>
> * **`∂ψ₁/∂y`**: 代表“当你在**竖直方向(y)**上移动一小步时，**水平位移(ψ₁)**变化了多少”。这衡量了**竖直线条的倾斜程度**，是一种**剪切变形**。
>
> * **`∂ψ₂/∂x`**: 代表“当你在**水平方向(x)**上移动一小步时，**竖直位移(ψ₂)**变化> 
> 
>> [!QUESTION]- F矩阵中的负号代表什么？
>> 这是一个观察力极其敏锐的问题！你注意到了这个矩阵形式中最不寻常、也最不符合“标准”教科书定义的一点。
>>
>> ---
>>
>> ### `F` 矩阵中负号的解释
>>
>> 最直接、最诚实的回答是：**这篇论文本身没有解释为什么这里有负号。**
>>
>> 作者在这里只是**直接给出**了这个`F`矩阵的**定义**，并将其作为他们所使用的这个特定的生物医学流动模型的一部分。这种复杂的数学模型的完整推导，通常会发表在更专门的、更基础的理论论文中（比如本文引用的参考文献[14]），而这篇综述性的文章里往往会省略这些细节。
>>
>> 不过，作为物理学家，我可以根据在连续介质力学中的经验，为你提供几种最可能的**专业推测**：
>>
>> #### 1. 最可能的原因：坐标系的特殊定义与旋转
>>
>> 在描述物体变形时，我们通常会用到两种坐标系：
>> * **空间坐标系 (Spatial Coordinates)**: 我们作为外部观察者的、固定的坐标系（比如屏幕上的x, y轴）。
>> * **物质坐标系 (Material Coordinates)**: 想象一个“纹在”果冻内部的、会随着果冻一起移动、拉伸和**旋转**的坐标系。
>>
>> `F`矩阵正是描述了这两种坐标系之间的关系。这个矩阵中出现负号，并且`y`的导数出现在第一行、`x`的导数出现在第二行，这种“错位”和“变号”的形式，在数学上**非常符合一个旋转了90度的坐标变换**。
>>
>> * **地图的比喻**:
>>     * 想象你的世界地图是“上北下南”的（空间坐标系）。
>>     * 现在你在果冻上“纹”了一个本地地图，但这个本地地图是**旋转了90度**的，它的“北方”指向世界地图的“东方”。
>>     * 当你试图用世界地图的语言（对x, y求导）来描述这个旋转了的本地地图（`ψ`的变化）时，为了正确表达，你的变换矩阵里自然就会出现负号和位置交换。
>>
>> 这种为了数学处理上的便利而选择一个“旋转”的或者“镜像”的内部坐标系的做法，在复杂的力学模型中是存在的。
>>
>> #### 2. 为了数学推导的便利性
>>
>> 有时候，一个看似奇怪的定义，可能是为了让后续更复杂的方程（比如计算弹性能量或应力的方程）变得异常简洁和优美。科学家们可能会选择一个“丑陋”的中间步骤（定义一个奇怪的`F`），来换取一个“漂亮”的最终结果。
>>
>> #### 核心功能并未改变
>>
>> **最重要的一点是**：尽管这个`F`矩阵的具体形式看起来有些奇怪，但它所包含的四个偏导数项（`∂ψ₁/∂x`, `∂ψ₁/∂y`, `∂ψ₂/∂x`, `∂ψ₂/∂y`）依然是描述变形的核心要素。
>>
>> **它的核心功能没有变**：它仍然是一个打包了物体在x方向拉伸、y方向拉伸、以及两种剪切变形信息的“形变体检报告”。这个负号和位置的变换，只是改变了我们“解读”这份报告的方式，而报告本身的内容（变形信息）依然完整地包含在里面。
>>
>> **总结**: 这个负号的根源，几乎可以肯定隐藏在构建这个特定相场模型（FSI）的深层数学推导中，很可能与坐标系的选取有关。对于理解这篇PINN论文而言，我们只需要记住`F`在这里的核心作用——**定量描述形变**——就可以了。

> [!NOTE]- “双势阱势能”的详细解释
> 
> ---
> 
> #### 1. 核心思想：势能 (Potential Energy) 和 “稳定状态”
> 
> 首先，我们来回顾一下高中物理中的一个基本概念：**势能**。
> 
> 宇宙中的所有物体都有一个最基本的“懒惰”倾向：**它们总是试图待在势能最低的位置**。
> 
> * **比喻**: 想象一个小球在一个山谷里。无论你把它放在山坡的哪个位置，它最终都会滚到**谷底**并停下来。因为谷底是这个系统里**重力势能最低**的地方。这个谷底，就是一个**“稳定状态”**。
> 
> 
> 
> ---
> 
> #### 2. “能量地形图”：双势阱势能 `(φ² − 1)²/4h²`
> 
> 这句话里的数学公式 `(φ² − 1)²/4h²`，就是在描述一个特殊的**“势能地形图”**。这个地形图的特殊之处在于，它不是一个山谷，而是**两个山谷 (Double-Well)**。
> 
> 让我们来分析一下这个“地形”的样貌：
> 
> * **横坐标**: 是我们的**相场变量 `φ`**。它代表了物质的状态。
> * **纵坐标**: 是这个状态对应的**势能**。
> 
> 我们来看看几个关键位置的海拔：
> 
> * **当 `φ = 1` (比如纯血液) 或 `φ = -1` (比如纯血栓) 时**:
>     * `φ² = 1`，所以 `(φ² - 1)² = (1 - 1)² = 0`。
>     * 此时，势能为**0**，是整个地形的**最低点**。这两个点就是我们那两个**山谷的谷底**，也就是两个最**“稳定”**的状态。
> 
> * **当 `φ = 0` (血液和血栓的“不稳定混合态”) 时**:
>     * `(φ² - 1)² = (0 - 1)² = 1`。
>     * 此时，势能为 `1/4h²`，是这两个山谷之间的**最高点**。这个点就是一个**山峰的峰顶**。
> 
> **这个“地形图”的样子**: 它就像一个字母**“W”**。有两个谷底（在`φ=-1`和`φ=1`），中间有一个山峰。
> 
> 
> 
> 这个“W”形的地形图完美地描述了我们想要的物理情景：系统**极其不情愿**待在`φ=0`这个混合状态（山顶），而是会想尽一切办法**待在**`φ=1`（纯血液）或`φ=-1`（纯血栓）这两个稳定的状态（谷底）。
> 
> ---
> 
> #### 3. “推动力”：势能的导数 `g(φ)`
> 
> 在物理学中，**力** 和 **势能** 之间有一个最基本的关系：**力是势能的（负）导数** (`F = -dU/dx`)。导数代表了“地形”的**坡度**。
> 
> * **`g(φ)` 是什么？** `g(φ)` 就是这个“W”形地形图的**坡度函数**。
> 
> * **它起什么作用？** `g(φ)` 提供了将物质状态推向稳定点的“驱动力”。
>     * 想象一个小球（代表当前的物质状态）被放在了`φ=0.5`的**半山腰**上。
>     * 在这个位置，**坡度 `g(φ)` 显然不为0**。这个非零的坡度就会产生一个“力”，推动小球**滚下山**，最终掉进`φ=1`的那个谷底。
>     * 只有当小球处在谷底 (`φ=1`或`φ=-1`) 或山顶 (`φ=0`) 时，坡度才为0，它受到的这个“力”才为0，从而能够（暂时）停下来。
> 
> 所以，在卡恩-希利尔德方程中引入 `g(φ)` 这一项，其物理作用就是**驱动“相分离” (Phase Separation)**。它就是那个让油和水自动分开、让血栓和血液之间维持清晰界面的“幕后推手”。
> 
> ### 总结
> 
> **“g(φ)是双势阱势能的导数”** 这句话的完整物理图像是：
> 
> 1.  我们构建了一个叫做**“双势阱势能”**的“W”形能量地形图，它规定了“纯血液”(`φ=1`)和“纯血栓”(`φ=-1`)是两个能量最低的稳定状态。
> 2.  我们利用这个地形图的**坡度（导数g(φ)）**，来产生一个“物理力”。
> 3.  这个“力”会驱动任何处于不稳定混合态的物质，自动地向那两个纯粹的、稳定的状态分离，从而在模拟中形成和维持血液与血栓之间的清晰界面。

> [!NOTE]- $\sigma_{vis}$ 和 $\sigma_{coh}$：力和参数的详细定义
> 
> 
> ### 1. 详细解释两种“应力 (Stress)”
> 
> “应力”是物理学中描述物体内部相互作用力的一个概念。在这里，它指的就是作用在流体团上的力。方程(12)提到了两种应力：
> 
> #### **方程 (17): $\sigma_{vis}$ (粘性应力 - Viscous Stress)**
> 
> * **它是什么？** $\sigma_{vis}$ 代表了流体内部的 **“摩擦力”**。
> * **公式解读**: $\sigma_{vis}=\mu\nabla u$
>     * `μ` (mu) 是**粘度 (viscosity)**，代表了流体有多“粘稠”。蜂蜜的`μ`就比水大得多。
>     * `∇u` (速度梯度) 衡量了流速在空间中的变化有多快。
> * **高中生比喻**: 想象一条河，河中心的流速最快，靠近河岸的流速慢。因为流速不同，中间的快水会“拖着”旁边的慢水前进，而岸边的慢水会“拽着”中间的快水减速。这种由于速度差异而产生的**内摩擦力**，就是粘性应力。`∇u` 越大（速度差异越剧烈），这个摩擦力就越大。
> 
> #### **方程 (18): $\sigma_{coh}$ (内聚应力 - Cohesive Stress)**
> 
> * **它是什么？** $\sigma_{vis}$ 代表了维持**血液和血栓之间界面**的 **“表面张力”** 或 **“内聚力”**。
> * **公式解读**: $\sigma_{coh}=\lambda\nabla\cdot(\nabla\phi\otimes\phi)$
>     * 这个公式在数学上很复杂，但它的核心与**相场 `φ` 的梯度 `∇φ`** 有关。
>     * `∇φ` 在哪里最大？恰好是在血液和血栓的**交界处**（即“成分地图”`φ`变化最快的地方）。在纯血液或纯血栓内部，`∇φ` 几乎为0。
> * **高中生比喻**: 想象一颗水珠落在荷叶上，水珠会因为**表面张力**而聚集成一个球形，而不是完全散开。这个表面张力就是一种“内聚力”，它作用在水和空气的**界面**上，试图让这个界面的面积变得最小。$\sigma_{vis}$ 在这里扮演着类似的角色，它是一个只在血液-血栓界面上才显著存在的力，作用是维持血栓的整体形态，抵抗血流的冲击。
> 
> ---
> 
> ### 2. 参数“词典” (续)
> 
> 这段话还定义了一系列具体的物理参数，大部分都被作者设为了固定的无量纲数值：
> 
> * `γ` (gamma): **界面迁移率**，决定了血栓-血液边界移动和变形的快慢。
> * `τ` (tau): **弛豫参数**，也和相场演化的时间快慢有关。
> * `λ` (lambda): **混合能量密度**，决定了上面提到的“内聚力”有多强。
> * `κ` (kappa): **渗透率**，衡量血液穿透血栓多孔结构的难易程度。它由真实的渗透率`kf`和纤维蛋白半径`af`计算得出。**这是本案例中PINN需要推断的核心未知量**。
> * `ρ`, `μ`, `h`: 分别是设为常数的密度、粘度和界面长度（决定了血栓边界的“模糊”程度）。
> 
> **一句话总结**: 这段话为这个极其复杂的生物流动问题**设定了具体的参数和力的计算公式**。它明确了血液遵守标准的“内摩擦”规则（粘性应力），并引入了一个由相场 `φ` 决定的、类似“表面张力”的特殊规则（内聚应力）来描述血栓的独特性质。

> [!NOTE]- 内聚应力 σ_coh 的原理详解
> 
> ---
> 
> #### 1. 核心思想：界面的“表面张力”
> 
> **`σ_coh` (内聚应力) 的物理原理，本质上就是一种“表面张力”**。
> 
> 想象一颗荷叶上的水珠。是什么力量让水珠保持一个圆滚滚的形状，而不是完全摊开？就是表面张力。这个力只作用在**水和空气的界面**上，并且总是试图让这个界面的面积收缩到最小。
> 
> `σ_coh` 在我们的模型中扮演着完全相同的角色：它是一个只在**血液和血栓的界面**上才显著存在的力，它的作用就是**维持血栓的形状，抵抗被血流冲散的趋势**。
> 
> ---
> 
> #### 2. “拉伸的橡胶薄膜”的比喻
> 
> 为了理解这个复杂的公式，我们可以把**血液和血栓的界面**想象成一张嵌入在流体中的、**被拉伸的、极薄的橡胶薄膜**。
> 
> 
> 
> 这张薄膜就是 `σ_coh` 这个力的来源。现在我们来看看公式的每个部分是如何描述这张薄膜的。
> 
> #### 3. 用“橡胶薄膜”比喻来拆解公式
> 
> > `σ_coh = λ ∇· (∇φ ⊗ ∇φ)`
> 
> * **第一步: `∇φ` — 找到“橡胶薄膜”的位置**
>     * 我们知道 `φ` 是“成分地图”，`∇φ` (相场梯度) 则**只在**地图上成分变化最快的地方（也就是血液和血栓的边界）才有显著的值。
>     * 所以，`∇φ` 的作用就是**定位**，它像一个探测器，准确地找到了这张“橡胶薄膜”在空间中的位置。
> 
> * **第二步: `∇φ ⊗ ∇φ` — 描述薄膜的“绷紧状态”**
>     * `⊗` 符号代表“张量积”，这是一个比较抽象的数学运算。
>     * 在这里，`∇φ ⊗ ∇φ` 这个整体，描述了“橡胶薄膜”在每一点的**局部朝向和绷紧状态**。它就像一个微小的坐标系，贴在薄膜表面，告诉我们薄膜在这里是沿着哪个方向被拉伸的，以及拉伸的强度有多大。
> 
> * **第三步: `∇·(...)` — 计算由“弯曲”产生的“合力”**
>     * 这是最关键的一步。`∇·` (散度) 在这里的作用是计算**力的不平衡**。
>     * 想象一下：
>         * 如果一小块橡胶薄膜是**完全平坦**的，那么它表面上各个方向的张力是互相抵消的，它对周围的流体（血液）产生的**合力为零**。
>         * 但如果这块薄膜是**弯曲的**，就像一个圆弧，那么朝向圆弧内侧的张力就会比外侧的更“集中”，导致张力无法完全抵消，从而产生一个指向**圆弧内侧的净合力**！
>     * **散度 `∇·` 这个数学工具，恰好就是用来计算由这种“弯曲”所导致的“张力不平衡”的。** 它计算出的这个净合力，就是内聚力 `σ_coh` 的最终来源。这个力总是试图将弯曲的薄膜“拉平”，从而减小总的表面积。
> 
> * **第四步: `λ` (lambda) — 决定薄膜的“韧性”**
>     * `λ` (混合能量密度) 是一个常数，它决定了这张“橡胶薄膜”的**韧性有多强**。`λ` 越大，薄膜绷得越紧，由弯曲产生的内聚力也就越大。
> 
> ---
> 
> ### 总结
> 
> 所以，`σ_coh = λ∇·(∇φ ⊗ ∇φ)` 这个公式背后的物理原理可以这样一步步理解：
> 
> 1.  首先，通过相场梯度 `∇φ` **找到**血栓和血液的界面。
> 2.  然后，通过张量积 `∇φ ⊗ ∇φ` **描述**这个界面上每一点的“表面张力”的大小和方向。
> 3.  最后，通过计算这个张量场的散度 `∇·(...)`，**找出**由于界面**弯曲**而导致的**不平衡的净合力**。
> 4.  这个净合力就是**内聚应力**，它像表面张力一样，作用在流体上，试图让血栓界面收缩，维持其团块状。

> [!NOTE]- 张量积 (Tensor Product) 的详细解释
> 
> ---
> 
> ### 1. 故事的起点：我们已知的“乘法”
> 
> 在物理学中，我们经常需要把不同的量“乘”在一起。你已经熟悉了两种“乘法”：
> 
> * **标量 × 向量**: 比如 `5 * u`。`u`是一个带方向的速度向量。这个乘法的结果是一个新的向量，方向和`u`一样，但长度是原来的5倍。这很简单，就是**缩放**。
> 
> * **向量 · 向量 (点乘 - Dot Product)**: 比如 `F · d` 计算功。你用一个力`F`（向量）作用在一个位移`d`（向量）上。点乘的结果是一个**标量**（一个数字），它告诉你这个力在位移方向上“贡献”了多少。它只捕捉了两个向量**在同一个方向上相互作用**的程度。
> 
> **问题来了**: 如果我想描述两个向量之间**所有方向上、全方位**的相互作用，点乘给出的一个数字显然信息量不够。这时，我们就需要一种更强大的“乘法”。
> 
> ---
> 
> ### 2. 新的“乘法”：张量积 (Tensor Product, `⊗`)
> 
> **张量积**，也常被称为**外积 (Outer Product)**，是另一种“向量乘以向量”的方法。
> 
> > **核心思想**: 张量积的目标，是捕捉第一个向量的**每一个分量**与第二个向量的**每一个分量**之间**所有可能**的相互作用，并把这些信息完整地、有组织地打包起来。
> 
> 它不像点乘那样把所有信息“压缩”成一个数字，而是把信息“展开”成一个更高级的数学对象——**张量 (Tensor)**（在这里，你可以暂时把它看作一个**矩阵**）。
> 
> ---
> 
> ### 3. 如何计算？——“全面撒网”式的乘法
> 
> 计算过程非常直观。假设我们有两个二维向量：
> $$
> \mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}
> $$
> 
> 它们的张量积 `a ⊗ b` 的计算方法是，将 `a` 的每一个分量，分别乘以 `b` 的每一个分量，然后把结果排列在一个矩阵里：
> 
> $$
> \mathbf{a} \otimes \mathbf{b} = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix} \begin{pmatrix} b_1 & b_2 \end{pmatrix} = \begin{pmatrix} a_1 \times b_1 & a_1 \times b_2 \\ a_2 \times b_1 & a_2 \times b_2 \end{pmatrix}
> $$
> 
> 你看，结果是一个2x2的矩阵，它包含了`a`和`b`分量之间全部4种可能的乘法组合。
> 
> ---
> 
> ### 4. 为什么要用它？——张量积的物理意义
> 
> 那么，这个包含了4个数字的矩阵，到底有什么物理意义呢？
> 
> **“风与船帆”的比喻**:
> * 想象向量 `u` 代表**风**，它有x方向的风速`u_x`和y方向的风速`u_y`。
> * 向量 `v` 代表**船帆的朝向和大小**，它也有x方向的分量`v_x`和y方向的分量`v_y`。
> 
> 我们想描述风对船帆的**总作用力**。这个力是非常复杂的：
> * x方向的风，会推着船帆的x方向分量（**正推力**），也会推着船帆的y方向分量（**侧向力**）。
> * y方向的风，同样也会对船帆的x和y分量产生作用。
> 
> **点乘 `u · v`** 只能给你一个总的“推进效率”的数字，很多细节都丢失了。
> 
> **张量积 `u ⊗ v`** 则给出了**完整的“作用力分析报告”**：
> $$
> \mathbf{u} \otimes \mathbf{v} = \begin{pmatrix} u_x v_x & u_x v_y \\ u_y v_x & u_y v_y \end{pmatrix}
> $$
> * **左上角 `u_x v_x`**: x方向的风作用在x方向船帆上的效果。
> * **右上角 `u_x v_y`**: x方向的风作用在y方向船帆上的效果。
> * **左下角 `u_y v_x`**: y方向的风作用在x方向船帆上的效果。
> * **右下角 `u_y v_y`**: y方向的风作用在y方向船帆上的效果。
> 
> 这个矩阵（一个二阶张量）**完整地描述了**风这个向量和船帆这个向量之间全方位的相互作用。
> 
> **回到我们的论文**: `∇φ ⊗ ∇φ` 也是同理。`∇φ`这个向量描述了界面的朝向和“陡峭”程度。`∇φ ⊗ ∇φ`这个张量，就**完整地描述了**这个界面上由于“陡峭”而产生的、**所有方向上**的“张力”状态。
> 
> ---
> 
> ### 5. 那么，张量到底是什么？
> 
> 现在你可以更好地理解张量的概念了。它是一个用来描述物理量的数学工具，是标量和向量的推广。
> 
> * **0阶张量**: **标量** (1个数字，比如温度)。
> * **1阶张量**: **向量** (一列数字，比如速度 `(vx, vy, vz)`)。
> * **2阶张量**: **矩阵** (一个二维数表，比如我们上面得到的 `u ⊗ v`，或者应力张量)。
> 
> **张量积，就是一种将低阶张量（比如两个1阶的向量）结合起来，创造出更高阶张量（一个2阶的矩阵）的数学运算。**

> [!NOTE]- 生物医学流中的边界条件详解
> 
> ---
> 
> ### 翻译
> 
> 入口`Γi`的速度被设为狄利克雷边界条件`u = g`, `(x,t) ∈ Γi × (0, T )`。我们在壁面`Γw`上施加无滑移边界，并为`φ`和`ω`在所有边界（`Γw ∪ Γi ∪ Γo`）上施加诺伊曼边界条件，即`∂φ/∂n = ∂ω/∂n = 0`。
> 
> ---
> 
> ### 详细解释
> 
> 我们可以把计算区域想象成一段动脉血管。这段血管有**入口 (inlet, `Γi`)**、**出口 (outlet, `Γo`)** 和**血管壁 (wall, `Γw`)**。这段话就在规定这三处地方的物理规则。
> 
> #### 1. 入口规则: “设定水龙头的水流”
> 
> > **“入口`Γi`的速度被设为狄利克雷边界条件 u = g...”**
> 
> * **`Γi` (Gamma i)**: 符号`Γ`(Gamma)通常用来表示边界，下标`i`代表`inlet`（入口）。所以`Γi`就是**入口边界**。
> * **狄利克雷边界条件 (Dirichlet boundary conditions)**: 我们之前讨论过，这种条件是**直接指定一个确切的数值**。
> * **`u = g`**: `u`代表血液的**速度**，`g`是一个给定的函数。
> * **含义**: 这句话的意思是，在血管的入口处，我们**强制规定**了血液流入的速度是多少。就像我们设定一个水龙头的开关开到多大，流出来的水速是固定的、已知的。
> 
> #### 2. 壁面规则 (之一): “墙壁是粘稠的”
> 
> > **“我们在壁面`Γw`上施加无滑移边界...”**
> 
> * **`Γw`**: `w`代表`wall`，所以`Γw`就是**血管壁**这个边界。
> * **无滑移边界 (no-slip boundary)**: 这是流体力学中一个非常基本和重要的边界条件。
> * **含义**: 它规定了紧贴着固体壁面的那一层流体，其**速度必须和壁面的速度完全一样**。因为这里的血管壁是静止不动的，所以紧挨着它的那一层血液，速度必须为**零**。
> * **比喻**: 想象灰尘落在快速旋转的电风扇叶片上。你会发现紧贴叶片的那一层灰尘也跟着高速旋转，不会“滑走”。这就是“无滑移”的体现。在这里，血液会“粘”在血管壁上。
> 
> #### 3. 所有边界的规则 (之二): “边界是防水的”
> 
> > **“...并为`φ`和`ω`在所有边界(`Γw ∪ Γi ∪ Γo`)上施加诺伊曼边界条件，即`∂φ/∂n = ∂ω/∂n = 0`...”**
> 
> * **`Γw ∪ Γi ∪ Γo`**: 这个符号`∪`代表“并集”，意思是**入口、壁面、出口这三个边界加在一起**，也就是**所有边界**。
> * **`φ` 和 `ω`**: 这两个是与**相场**（描述血栓形态）有关的变量。
> * **诺伊曼边界条件 `∂/∂n = 0`**: 我们之前也讨论过，这种条件规定了某个量在**垂直于边界方向上的变化率为零**，这通常意味着**“没有通量”或“没有东西流过边界”**。
> * **含义**: 这句话的意思是，在计算区域的所有边界上（入口、出口、血管壁），相场变量`φ`和`ω`都**不能“流出”或“流入”**。
> * **“防水”比喻**: `φ`是我们描述血栓和血液这张“成分地图”的变量。这个条件就好像我们给整个计算区域的边界都涂上了一层“防水涂层”。我们只关心区域内部的血栓和血液是如何相互作用和演化的，不希望有任何“相场”物质从边界处“泄漏”出去或“渗透”进来。
> 
> ### 总结
> 
> 这段话用精确的数学语言，为这个复杂的生物医学问题设定了清晰的边界规则：
> 1.  **入口 (`Γi`)**: 血液以一个**已知的速度**流入。
> 2.  **壁面 (`Γw`)**: 血液会**粘在**静止的血管壁上（速度为0）。
> 3.  **所有边界 (`Γi, Γo, Γw`)**: 边界是**封闭的**，不允许相场变量（血栓/血液的“成分”）流进流出。

> [!NOTE]- 辅助变量 ω (Omega) 的解释
> 
> ---
> 
> #### 1. `ω` 的数学定义
> 
> 根据论文中的**方程(16)**，`ω` 是由以下公式计算得出的：
> 
> $$
> \omega = \Delta\phi+\gamma g(\phi)
> $$
> 
>
> 
> 你看，`ω` 本身不是一个独立的物理量，它的值完全由我们已经讨论过的**相场 `φ`** 及其相关参数所决定。
> 
> ---
> 
> #### 2. `ω` 的物理角色：“化学势”
> 
> 虽然 `ω` 是一个数学上的中间变量，但在物理学（特别是热力学和材料科学）中，它扮演着一个非常重要的角色，被称为**“化学势 (Chemical Potential)”**。
> 
> **“厨师的备菜碗”比喻**:
> * 想象一下，**相场 `φ`** 的演化（即血栓形态的变化）是一道非常复杂的菜肴。
> * 方程(15) `∂φ/∂t + ... = τ∆ω` 是最终的**“烹饪步骤”**。
> * 你会发现，最终步骤 `τ∆ω` 使用的不是最原始的食材 `φ`，而是用到了 `ω`。
> 
> `ω` 就像一个**“厨师的备菜碗”**。在正式烹饪前，厨师会预先把几样东西在这个碗里混合好：
> 1.  **`∆φ`**: 这个量与相场 `φ` 的**“弯曲程度”**有关。它代表了由于**界面张力**（我们之前用“橡胶薄膜”比喻）而产生的能量。系统总是倾向于让界面更平滑来降低这部分能量。
> 2.  **`γg(φ)`**: 这个量与**“双势阱势能”**有关。它代表了**“相分离”**的驱动力，也就是让物质保持在“纯血液”或“纯血栓”这两个稳定状态的能量倾向。
> 
> **`ω` (化学势)** 就是把**“界面张力”**和**“相分离”**这两种驱动力混合在一起，形成的一个总的“势能”。
> 
> ---
> 
> #### 3. `ω` 在方程中的作用
> 
> 这个混合好的“化学势” `ω`，它的**梯度**（或者说空间变化 `∆ω`）会产生一个“力”，这个“力”正是驱动血栓-血液界面移动、变形和演化的最终动力。
> 
> 这就是方程(15)所描述的：相场`φ`的变化（左边），是由`ω`的空间变化（右边`∆ω`）所驱动的。
> 
> ---
> 
> #### 4. 边界条件 `∂ω/∂n = 0` 的含义
> 
> > **“...∂ω/∂n = 0 ... at all boundaries.”**
> 
> 这个边界条件的意思是，在计算区域的所有边界上（入口、出口、血管壁），**化学势`ω`的通量为零**。
> 
> 换句话说，我们不允许这个驱动相场演化的“势能”从边界处“泄漏”出去或“渗透”进来。这是一个封闭体系的假设，确保了我们研究的血栓和血液的总量在区域内是守恒的。
> 
> **总结**: `ω` 是一个被称为**化学势**的辅助变量，它巧妙地将界面张力（与`∆φ`有关）和相分离力（与`g(φ)`有关）这两个驱动界面演化的核心物理因素打包在了一起，从而简化了最终的演化方程。

> [!NOTE]- 卡恩-希利尔德方程组 (15) & (16) 详解
> 
> ---
> 
> 这个“食谱”的目标是：计算出“成分地图”`φ`在下一瞬间会变成什么样子。
> 
> #### 第一步 (方程16): “备菜”——计算驱动力 `ω`
> 
> > `ω = Δφ + γg(φ)` (16)
> 
> 在真正开始“烹饪”（演化`φ`）之前，我们需要先准备好“调味料”，也就是驱动界面变化的“力”。这个“力”的势能，就是我们之前讨论过的**化学势 `ω`**。
> 
> 方程(16)就是在“配制”这个`ω`。它混合了两种核心的物理驱动力：
> 
> 1.  **`Δφ` (界面张力效应)**:
>     * `Δφ` (拉普拉斯算子) 与相场 `φ` 的**弯曲程度**有关。
>     * 它的物理作用是**“表面张力”**。就像水珠总是想变成球形一样，这个力会试图让血栓和血液的界面变得更**平滑**，来减小总的界面能量。
> 
> 2.  **`γg(φ)` (相分离效应)**:
>     * `g(φ)` 来自我们讨论过的**“W”形双势阱势能**。
>     * 它的物理作用是**驱动“相分离”**。就像油和水会自动分开一样，这个力会惩罚那些“不三不四”的混合状态，把它们推向“纯血液”(`φ=1`)或“纯血栓”(`φ=-1`)这两个稳定的状态。
> 
> **总结第一步**: 方程(16)像一个**“备菜碗”**，它把“让界面变平滑的力”和“让成分变纯粹的力”这两种物理倾向混合在一起，得到了一个总的驱动力势能——化学势`ω`。
> 
> ---
> 
> #### 第二步 (方程15): “烹饪”——演化“成分地图”`φ`
> 
> > `∂φ/∂t + u · ∇φ = τΔω` (15)
> 
> 现在，“调味料”`ω`已经配好，我们可以开始真正的“烹饪”了。这条方程描述了“成分地图”`φ`随时间的变化，这个变化由两部分组成：
> 
> * **方程左边: `∂φ/∂t + u · ∇φ` (被动漂流)**
>     * `∂φ/∂t` 是`φ`在固定点的变化。
>     * `u · ∇φ` 是**平流项 (Advection Term)**。
>     * **物理意义**: 这部分描述了血栓-血液界面，作为一个整体，被血流 `u` **被动地“裹挟”着向前移动**。
>     * **比喻**: 想象河面上的一片油渍（代表血栓界面）。河水（`u`）流淌，会带着这片油渍一起**向下游漂去**。方程的左边描述的就是这个“随波逐流”的过程。
> 
> * **方程右边: `τΔω` (主动变形)**
>     * **物理意义**: 这部分描述了界面**自身的主动演化**，它试图让系统的总能量（由`ω`代表）变得更低。
>     * `Δω` 描述了化学势`ω`在空间上的不均匀性，这种不均匀性会驱动`φ`发生“扩散”，从高化学势区域流向低化学势区域。
>     * **比喻**: 继续看河面上的油渍。除了整体向下游漂移，油渍自身也在变化：许多小的油滴可能会**合并**成一个大的油滴，并且这个大油滴会尽量变成**圆形**。为什么？因为这样可以使总的油-水界面面积最小，从而让表面张力能最低。方程的右边描述的就是这个**为了降低自身能量而发生的“合并、变形、变圆”**的过程。
>     * `τ` (tau) 是一个时间参数，它控制了这个“主动变形”过程发生的**快慢**。
> 
> ---
> 
> ### 整体故事线
> 
> 这两个方程讲述了一个连续的故事：
> 
> 在任何一个时刻，系统首先根据当前的血栓形态`φ`，通过**方程(16)**计算出各处的“能量压力”——化学势`ω`。然后，系统再根据**方程(15)**，在血流`u`的带动下，同时朝着能让`ω`更均匀（即总能量更低）的方向，去移动和改变`φ`的形态，从而得到下一时刻的血栓样貌。这个过程周而复始，就构成了血栓在动脉流中复杂的动态演化过程。

> [!NOTE]- 拉普拉斯算子 (Δ 或 ∇²) 详解
> 
> ---
> 
> #### 1. 拉普拉斯算子的数学定义
> 
> 拉普拉斯算子有两种等价的数学定义：
> 
> 1.  **梯度的散度**: `Δf = ∇ · (∇f)`
>     * 这个定义非常优雅。我们先用梯度算子 `∇f` 计算出一个指向“最陡上坡方向”的向量场，然后再用散度算子 `∇·` 计算这个梯度场的“发散”程度。
> 
> 2.  **二阶偏导数的和**: 在三维直角坐标系中，它的计算形式是：
>     $$
>     \Delta f = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} + \frac{\partial^2 f}{\partial z^2}
>     $$
>     它把一个函数在所有空间方向上的**二阶导数**（变化率的变化率，可以理解为“凹凸性”或“曲率”）加了起来。
> 
> ---
> 
> #### 2. 拉普拉斯算子的物理意义：“我与邻居的差异”
> 
> 它的数学形式看起来很抽象，但物理意义却非常直观。
> 
> > **拉普拉斯算子的值，衡量了一个点的函数值 `f`，与它周围邻居的平均值之间的差异。**
> 
> **“爱管闲事的邻居”比喻**:
> * 想象在一个由无数个小方格组成的社区里，每个方格都住着一个人，每个人都有一个财富值`f`。
> * 你是住在社区中心的一个居民，你的财富是 `f_center`。
> * 你的东、西、南、北四个邻居的平均财富是 `f_average`。
> 
> 拉普拉斯算子 `Δf` 就相当于在问：“**你的财富，和你的邻居们的平均财富，差了多少？**”
> 
> 具体来说：
> 
> * **`Δf = 0`**: 你的财富**恰好等于**你邻居的平均财富。你和你的邻里“和谐相处”，没有财富的“凸起”或“凹陷”。这是一条**直线或者平坦的平面**。
> 
> * **`Δf > 0`**: 你的财富**低于**你邻居的平均财富。你是一个“洼地”。就像一个**山谷的谷底**，你的值比周围低。
> 
> * **`Δf < 0`**: 你的财富**高于**你邻居的平均财富。你是一个“富豪”，是一个“凸起”。就像一个**山峰的峰顶**，你的值比周围高。
> 
> 
> 
> ---
> 
> #### 3. 拉普拉斯算子在物理中的应用
> 
> 理解了“我与邻居的差异”这个核心思想后，我们就能明白为什么它在物理中如此重要。
> 
> * **热传导 (Heat Conduction)**
>     * 在热传导方程 `∂T/∂t = αΔT` 中，`T`是温度。
>     * `ΔT` 衡量了某一点的温度 `T` 与其周围邻居平均温度的差异。
>     * 如果 `ΔT < 0`，说明这个点比周围**热**（一个“热点”）。根据物理规律，热量会从热的地方流向冷的地方，所以这个点的温度会**下降** (`∂T/∂t < 0`)。
>     * 如果 `ΔT > 0`，说明这个点比周围**冷**（一个“冷点”）。热量会从周围流向它，所以这个点的温度会**上升** (`∂T/∂t > 0`)。
>     * 拉普拉斯算子完美地描述了**扩散 (Diffusion)** 现象的本质：**物质（或能量）总是从“凸起”的地方（高浓度/高温）流向“洼地”（低浓度/低温），试图将一切抹平。**
> 
> * **流体力学 (Fluid Mechanics)**
>     * 在纳维-斯托克斯方程的粘性项 `μΔu` 中，`Δu` 衡量了一个流体微团的速度 `u` 与其周围流体微团平均速度的差异。
>     * 如果 `Δu` 不为零，说明速度不均匀，存在速度差异，这就产生了**内摩擦（粘性力）**，试图让速度变得均匀。
> 
> * **电磁学 (Electromagnetism)**
>     * 在没有电荷的地方，电势 `V` 满足拉普拉斯方程 `ΔV = 0`。这意味着电势场是“平滑”的，任何一点的电势值都等于它周围点的平均值。
> 
> ### 总结
> 
> 拉普拉斯算子 `Δ` 是一个强大的数学工具，它通过计算一个点与其邻域平均值的差异，深刻地描述了自然界中无处不在的**扩散和平衡**现象。当一个物理量 `f` 满足 `Δf = 0` 时，我们说这个场是**“和谐的”**；当 `Δf ≠ 0` 时，`Δf` 的大小和符号就成为了驱动这个场回归“和谐”状态的**“源动力”**。

### 5.1 PINN的实现
我们构建了两个全连接神经网络，$Net\ U$ 和 $Net \ W$。其中 $Net\ U$ 的输出代表了PDE解 $u, v, p, \phi$ 的一个代理模型，而 $Net W$ 的输出是PDE的解 $\omega, \ \phi_1, \ \phi_2$。每个网络都有9个隐藏层，每层20个神经元。总损失 $L$ 是不同损失的组合，形式如下：
$$
L = \omega_1L_{PDE} + \omega_2L_{IC} + \omega_3L_{BC} + \omega_4L_{data} \tag{19}
$$
其中 $L_{PDE}$ 是PDEs的残差损失，$L_{IC}$ 是初始条件损失，$L_{BC}$ 是边界条件损失，$L_{data}$ 是数据损失。具体来说：
$$
\begin{align}
L_{PDE}(\theta,\ \lambda;\ X_{PDE})&=\frac{1}{|X_{PDE}|}\sum_{x\in X_{PDE}}||f(x,\ \partial_t \hat {\mathbf{u}},\ \partial_x \hat {\mathbf{u}}, \ \dots; \ \lambda)||_2^2\ , \tag{20}\\
L_{BC}(\theta,\ \lambda;\ X_{BC})&=\frac{1}{|X_{BC}|}\sum_{x\in X_{BC}}||\mathfrak{B}(\hat {\mathbf{u}}, \ x)||_2^2\ , \tag{21}\\ 
L_{IC}(\theta,\ \lambda;\ X_{IC})&=\frac{1}{|X_{IC}|}\sum_{x\in X_{IC}}||\hat {\mathbf{u}} -{\mathbf{u}}_{t_0}||_2^2\ , \tag{22}\\ 
L_{data}(\theta,\ \lambda;\ X_{data})&=\frac{1}{|X_{data}|}\sum_{x\in X_{data}}||\hat {\mathbf{u}} -{\mathbf{u}}_{data}||_2^2\ , \tag{23}\\
\end{align}
$$
其中 $\omega_1, \ \omega_2, \ \omega_3, \ \omega_4$ 是每一项的权重。训练集 $X_{PDE}$, $X_{BC}$ 和 $X_{IC}$ 分别从内部时空域、边界和初始快照中采样。$X_{data}$ 是包含传感器坐标和点测量值的数据集；$|\cdot|$ 表示训练集中的训练数据数量。特别地，$\mathfrak{B}$ 代表了边界上狄利克雷和诺伊曼残差的组合。

最后，我们通过迭代地最小化总损失 $L(\theta,\ \lambda)$ 来优化模型参数 $\theta$ 和PDE参数 $\lambda=[\kappa]$，直到损失满足停止标准。最小化总损失是一个针对 $\lambda$ 的优化过程，以使得PINN的输出能够满足PDE系统、初始/边界条件和点测量值。

> [!TIP]- 生物医学流PINN实现详解
> 这段话详细描述了如何设置PINN来解决这个极其复杂的生物医学问题。
> 
> #### 1. “超级英雄二人组”——网络架构
> 
> 面对这个复杂的流固耦合问题，作者没有使用单个神经网络，而是派出了一个**“超级英雄二人组”**：
> 
> * **`Net U` (超人)**: 负责处理主要的、大家熟悉的物理量，包括速度`u,v`，压力`p`，以及描述血栓形态的相场`φ`。
> * **`Net W` (闪电侠)**: 负责处理那些更抽象、更专业的辅助变量，包括化学势`ω`和形变追踪器`ψ₁, ψ₂`。
> 
> 将任务拆分给两个专门的网络，有助于提高训练的稳定性和效率。这两个网络都拥有 **9个隐藏层，每层20个神经元**，这是一个“瘦高个”型的深度网络结构。
> 
> #### 2. “四合一”的期末考卷——损失函数
> 
> 总损失 `L` 依然是我们熟悉的“四门主课”的加权总和，但这里作者给出了每一门课详细的“评分标准”（公式20-23）：
> 
> * **`L_PDE` (物理综合题)**: 检查PDE方程组（我们之前讨论过的所有12-16号方程）的残差 `f` 是否接近于0。它在计算区域内部随机采样 `X_PDE` 个点来进行检查。
> * **`L_BC` (地理边界题)**: 检查在区域的边界上，解 `û` 是否满足我们设定的边界条件（`B`代表了所有边界条件的总和）。
> * **`L_IC` (历史开端题)**: 检查在 `t=0` 的初始时刻，解 `û` 是否和已知的初始状态 `u_t0` 一致。
> * **`L_data` (实验数据题)**: 检查在传感器的位置上，解 `û` 是否和我们测量到的真实数据 `u_data` 一致。
> 
> 这四个公式的本质都是**均方误差 (Mean Squared Error)**：即计算 **(PINN的答案 - 标准答案)²** 的平均值。
> 
> #### 3. “一石二鸟”的优化目标
> 
> > **“...我们通过...最小化总损失 `L(θ, λ)` 来优化模型参数 `θ` 和PDE参数 `λ = [κ]`...”**
> 
> 这是整个案例研究的**核心目标**。最小化总损失`L`的过程，就像一个侦探在解一个谜题，他需要同时找到两样东西：
> 
> 1.  **找到最佳的模型参数 `θ`**: 这代表了神经网络内部所有的“旋钮”设置。这相当于侦探根据所有线索，在脑中对案情进行**最合理的重构**。
> 2.  **找到最佳的PDE参数 `λ = [κ]`**: 在这个案例中，这个未知的物理参数就是血栓的**渗透率 `κ`**。这相当于侦探在重构案情时，发现现场有一条关键线索缺失了（比如凶器的型号），他需要通过推理，**反推出这条缺失的线索到底是什么**。
> 
> PINN的优化过程，就是不断调整 `θ` (案情重构) 和 `κ` (缺失的线索)，直到找到一组完美的 `θ` 和 `κ`，使得最终的“故事”（PINN的解）能够完美地同时满足物理定律、边界/初始条件和所有已知的测量数据。

> [!NOTE]- 四大损失函数的物理意义与数学表达式详解
> 
> ---
> 
> ### 1. `L_PDE` (物理定律损失) — “物理理论综合题”
> 
> > $L_{PDE}(\theta,\ \lambda;\ X_{PDE})=\frac{1}{|X_{PDE}|}\sum_{x\in X_{PDE}}||f(x,\ \partial_t \hat {\mathbf{u}},\ \partial_x \hat {\mathbf{u}}, \ \dots; \ \lambda)||_2^2$
> 
> * **物理意义**:
>     * 这部分的“考题”是**开卷**的，考卷内容就是物理定律本身（比如纳维-斯托克斯方程、卡恩-希利尔德方程等）。
>     * 它检查的是PINN给出的**整个解 `û`** 是否**在计算区域内的每一个角落**都遵守了这些物理定律。
>     * 这就像老师批改你的物理大题，他会检查你的每一个推导步骤是否都符合物理公式，确保你不是在胡编乱造。
> 
> * **表达式解释**:
>     * `f(...)`: `f`就是**物理方程的残差**。理想情况下，一个完美的解代入物理方程后，`f`应该等于0。`f`不等于0，就说明这个解违反了物理定律，`f`的数值就是“违反的程度”。
>     * `‖f(...)‖²`: 我们把这个“违反程度”`f`**平方**一下。这有两个作用：确保扣分是正数，并且严重违反物理定律的地方会被“加倍扣分”。
>     * `∑ ...`: 把在所有随机采样点 `X_PDE` 上的“扣分”都**加起来**。
>     * `1/|X_PDE|`: 除以采样点的总数，求一个**平均分**。
>     * **一句话总结公式**: **在计算区域内部随机抽取大量点，计算解在这些点上违反物理定律的程度的“平均平方值”**。
> 
> ---
> 
> ### 2. `L_IC` (初始条件损失) — “历史知识开卷题”
> 
> > `L_IC(...) = ...`
> 
> * **物理意义**:
>     * 这部分考查PINN是否**“尊重历史”**。它检查的是在**起始时刻 `t=0`**，PINN给出的解`û`是否和我们已知的、真实的初始状态`u_t0`相符。
>     * 这就像老师问你：“在故事开始的时候，主人公的状态是什么？”你必须回答正确。
> 
> * **表达式解释**:
>     * `û - u_t0`: 计算PINN的答案`û`和标准答案（已知的初始状态`u_t0`）之间的**差距**。
>     * `‖...‖²`: **平方**这个差距，同样是为了加重惩罚。
>     * `∑ ... / |X_IC|`: 在所有初始时刻的点`X_IC`上，计算这个**“差距平方”的平均值**。
> 
> ---
> 
> ### 3. `L_BC` (边界条件损失) — “地理知识开卷题”
> 
> > `L_BC(...) = ...`
> 
> * **物理意义**:
>     * 这部分考查PINN是否**“遵守规则”**。它检查的是在计算区域的**所有边界**上，PINN给出的解`û`是否满足我们设定的边界条件（比如入口速度、壁面无滑移等）。
>     * 这就像老师问你：“在这个游戏棋盘的边界上，有哪些特殊的规则？”
> 
> * **表达式解释**:
>     * `B(û, x)`: `B`代表了**边界条件的残差**。如果PINN的解`û`在边界点`x`上完美满足边界条件，`B`就等于0。否则`B`就是一个误差值。
>     * `‖...‖²`: **平方**这个边界误差。
>     * `∑ ... / |X_BC|`: 在所有边界点`X_BC`上，计算这个**“边界误差平方”的平均值**。
> 
> ---
> 
> ### 4. `L_data` (数据损失) — “实验数据填空题”
> 
> > `L_data(...) = ...`
> 
> * **物理意义**:
>     * 这部分考查PINN是否**“尊重事实”**。它检查的是在那些我们安放了**真实传感器**的位置上，PINN的预测`û`是否和传感器测量到的数据`u_data`相符。
>     * 这就像考卷里有几道送分题，已经把标准答案`u_data`写在旁边了，看你有没有抄对。
> 
> * **表达式解释**:
>     * `û - u_data`: 计算PINN的答案`û`和标准答案（已知的传感器数据`u_data`）之间的**差距**。
>     * `‖...‖²`: **平方**这个差距。
>     * `∑ ... / |X_data|`: 在所有传感器所在的位置`X_data`上，计算这个**“差距平方”的平均值**。
> 
> **总结**: 这四项损失函数虽然形式略有不同，但都遵循着**“均方误差 (Mean Squared Error)”**这一核心思想。它们从物理定律、初始状态、边界规则和真实数据这四个不同的维度，共同构成了一个全面而严谨的“评分体系”，指导着PINN朝着正确、真实且符合物理规律的解不断学习和进化。

> [!NOTE]- 范数符号 $||...||_2$ 的解释
> 
> ---
> 
> **一句话回答**:
> 
> 下标 **`2`** 代表 **L2范数 (L2-norm)**。L2范数就是我们最熟悉的**“欧几里得范数”**，也就是初中数学里学的**勾股定理**和**直线距离公式**在更高维度空间的推广。
> 
> ---
> 
> ### 详细解释
> 
> 我们来一步步拆解 `||f(...) ||₂²` 这个表达式的含义。
> 
> #### 1. `f(...)` 是一个向量 (Vector)
> 
> 首先要明确，在损失函数的这个公式里，`f` 不是一个单独的数字（标量），而是一个包含了多个分量的**向量**。
> 
> 在我们之前的讨论中，`f` 代表了纳维-斯托克斯方程的残差，它至少包含4个分量：
> $$
> \mathbf{f} = (f_1, f_2, f_3, f_4)
> $$
> * `f₁`: x方向动量方程的误差
> * `f₂`: y方向动量方程的误差
> * `f₃`: z方向动量方程的误差
> * `f₄`: 质量守恒方程的误差
> 
> 这个向量 `f` 整体描述了PINN的解在某一个点上对物理定律的**总违反情况**。
> 
> #### 2. `||f||₂` (L2范数) — 计算向量的“长度”
> 
> **L2范数**的作用，就是计算一个向量的**“长度”或“大小” (Magnitude)**。它的计算方法就是我们非常熟悉的**距离公式**：**将向量的每个分量平方，然后加在一起，最后开根号**。
> 
> 对于我们的`f`向量，它的L2范数就是：
> $$
> ||\mathbf{f}||_2 = \sqrt{f_1^2 + f_2^2 + f_3^2 + f_4^2}
> $$
> 
> * **几何意义**: `||f||₂` 的值，代表了在4维空间中，从原点 `(0,0,0,0)`（代表“零误差”）指向我们计算出的误差点 `(f₁, f₂, f₃, f₄)` 的那根箭头的**几何长度**。
> * **物理意义**: 它把所有物理定律的违反度（4个分量）**打包成一个单一的、总的误差大小**。这个数字越大，说明在这一点上对物理定律的总体违反越严重。
> 
> #### 3. `||f||₂²` (平方L2范数) — “惩罚”误差
> 
> 现在我们把上标的平方 `²` 和下标的 `₂` 结合起来看。
> 
> **`||f||₂²`** 就代表 **L2范数的平方**。
> $$
> ||\mathbf{f}||_2^2 = \left(\sqrt{f_1^2 + f_2^2 + f_3^2 + f_4^2}\right)^2 = f_1^2 + f_2^2 + f_3^2 + f_4^2
> $$
> **为什么要用平方？**
> * **计算方便**: 它消掉了开根号的运算，让求导等后续计算变得更简单。
> * **放大惩罚**: 和我们之前讨论均方误差时一样，平方项会不成比例地放大那些较大的误差，迫使优化算法优先去修正那些错得最离谱的地方。
> 
> ### 总结
> 
> 在损失函数的这个公式中：
> 1.  我们首先计算出一个包含多个分量的**误差向量 `f`**。
> 2.  然后，我们用**L2范数 `||f||₂`** 来衡量这个多维误差向量的总大小（它的几何长度）。
> 3.  最后，我们取这个长度的**平方 `||f||₂²`** 作为该点的最终“扣分”，以便于计算和优化。
> 
> 所以，下标`2`是这个“长度”计算方法（勾股定理）的正式数学名称，它告诉我们要用每个分量的平方和再开根号的方式来计算向量的大小。

> [!NOTE]- 范数符号 ||...|| 的解释
> 
> ---
> 
> ### 1. 核心含义：“长度”的推广
> 
> > **“范数” `||...||` 的核心含义，就是用来衡量一个向量的“长度”或“大小” (Magnitude)。**
> 
> 它是我们熟悉的“长度”这个概念在更广义、更抽象的数学空间中的推广。
> 
> ---
> 
> ### 2. 它和我们刚讨论的 `||...||₂` (L2范数) 是什么关系？
> 
> 关系非常密切！**L2范数是范数中最常见、最著名的一种。**
> 
> * **`||v||₂` (L2范数)**: 这是有明确下标“2”的范数，计算方法是“各分量平方和再开根号”（即勾股定理）。它代表了向量在空间中的**“欧几里得距离”或“直线长度”**。
> 
> * **`||v||` (不带下标的范数)**: 当你在绝大多数物理和工程文献中看到不带下标的范数符号 `||...||` 时，它**通常默认指的就是L2范数 `||...||₂`**。
> 
> **所以，在刚才的损失函数公式中，`||û - u_data||` 和 `||û - u_data||₂` 的意思是完全一样的。** 作者只是省略了下标`2`，因为它是一种通用的写法。
> 
> ---
> 
> ### 3. 范数是一个“家族”
> 
> 实际上，“范数”是一个大家族，L2范数只是其中最出名的一个成员。为了让你理解得更全面，我再介绍一个它的兄弟：
> 
> * **L1范数 `||v||₁` (“曼哈顿距离”)**:
>     * **计算方法**: **将所有分量的绝对值相加**。
>     * **比喻**: 想象你在一个像曼哈顿那样的、由横平竖直的街道组成的城市里。你想从A点走到B点，你不能斜着穿过大楼，只能沿着街道走。你走过的总路程（向东走的距离 + 向北走的距离），就是A到B的“曼哈顿距离”或“L1范数”。
> 
> **例子**: 向量 `v = (3, 4)`
> * **L2范数 (直线距离)**: `||v||₂ = √(3² + 4²) = √25 = 5`
> * **L1范数 (曼哈顿距离)**: `||v||₁ = |3| + |4| = 7`
> 
> ### 总结
> 
> * **`||...||`** 是一个通用的**范数符号**，代表向量的“长度”。
> * 当它不带下标时，通常**默认指的就是 `||...||₂` (L2范数)**，也就是我们熟悉的直线距离。
> * 在这篇论文的所有损失函数公式中，它衡量了两个向量（比如“预测值”和“真实值”）之间的**误差大小**。

> [!NOTE]- 损失函数计算的详细步骤
> 
> ---
> 
> ### 1. “备菜”：如何决定和获取用于计算的坐标点 `(x, t)`？
> 
> 在开始计算之前，我们需要准备好用来“考问”神经网络的坐标点。不同类型的损失，其坐标点的来源是完全不同的。
> 
> * #### `L_data` (实验数据损失)
>     * **来源**: 这些点的坐标 `(x, t)` 是**预先设定好的、固定的**。它们就是你**物理实验中传感器的精确位置**，或者是你在“标准答案”（CFD模拟结果）中**选定的那几个观测平面上的网格点**（如图3所示）。
>     * **比喻**: 这些点就像是**“标准答案”**或**“小抄”**。它们的位置和对应的正确答案 (`u_data`) 都是已知的。
> 
> * #### `L_PDE` (物理定律损失)
>     * **来源**: 这些点的坐标 `(x, t)` 是在整个计算区域（时空域`Ωs`）内**随机生成的**。在第一个案例中，作者就随机撒了三百万个这样的点。
>     * **比喻**: 这就像老师为了检查你是否真的理解了物理定律，**从你的整篇论文中随机抽取大量句子**来进行检查。他要在各个角落都确认你的逻辑是自洽的。
> 
> * #### `L_IC` 和 `L_BC` (初始和边界损失)
>     * **来源**: `L_IC`的点**只在初始时刻 `t=0` 的空间区域中**采样。`L_BC`的点则**只在计算区域的边界上**采样。
>     * **比喻**: 这就像老师专门检查你论文的**“引言”部分（初始条件）**和**“封面格式”（边界条件）**是否符合规范。
> 
> ---
> 
> ### 2. “预测”：`û` 具体代表什么？
> 
> > **`û` (u-hat) 代表的就是神经网络的直接输出，也就是它对真实物理量的“预测值”或“近似值”。**
> 
> * **它是一个函数**: 你可以把训练好的神经网络看作一个函数 `NN(x, t)`。
> * **输入**: 你给它一个具体的时空坐标 `(x, t)`。
> * **输出**: 它会返回一个向量，这个向量就是`û`。`û` 包含了该点上所有物理量的预测值。
>     * 例如，`û = (u_pred, v_pred, w_pred, p_pred)`，其中 `u_pred` 是对真实速度`u`的预测，`p_pred`是对真实压力`p`的预测。
> 
> ---
> 
> ### 3. “阅卷”：`û` 如何被用于计算 `L_data` (Step-by-Step)
> 
> 现在我们来看在一个训练步 (step) 中，`L_data` 是如何被具体计算出来的。
> 
> #### **第 1 步: 抽取一批“考题”**
> 
> 从我们庞大的、预先设定好的`X_data`数据集中（比如包含了几万个点的坐标和对应的真实速度值），**随机抽取一小批 (mini-batch)** 数据。假设批量大小是10000。
> 
> 那么现在我们手上有10000组数据，每一组都形如：
> `(坐标x, 坐标t, 真实速度u_data)`
> 
> #### **第 2 步: “考生”入场答题**
> 
> 我们把这10000组数据中的**坐标 `(x, t)` 部分**，作为输入，**喂给神经网络**。
> 
> `Input = (x_1, t_1), (x_2, t_2), ..., (x_10000, t_10000)`
> 
> #### **第 3 步: 得到“考生”的答案 `û`**
> 
> 神经网络对这10000个坐标点进行计算，并输出它自己的**预测值 `û`**。
> 
> `Output = û_1, û_2, ..., û_10000`
> 
> #### **第 4 步: 逐题批改，计算“误差”**
> 
> 现在我们同时有了“标准答案” `u_data` 和“考生的答案” `û`。我们可以逐个对比，计算每一道题的误差。
> 
> `error_1 = û_1 - u_data_1`
> `error_2 = û_2 - u_data_2`
> ...
> `error_10000 = û_10000 - u_data_10000`
> 
> #### **第 5 步: 计算“最终得分”（均方误差）**
> 
> 我们把上面所有的误差**平方**，然后**加起来**，最后再**求一个平均值**。这个最终的数字，就是本次小批量计算得到的 `L_data` 的值。
> $$
> L_{data} = \frac{1}{10000} \sum_{i=1}^{10000} ||\text{error}_i||^2
> $$
> 
> ---
> 
> **总结**:
> 
> 1.  从不同来源（预设、随机、边界）**获取坐标点**。
> 2.  将坐标点喂给神经网络，得到**预测值 `û`**。
> 3.  将预测值 `û` 与对应的**“标准答案”**（真实数据、物理定律残差=0、初始/边界条件）进行比较，计算出**误差**。
> 4.  通过**均方误差**公式，将所有点的误差汇总成一个**最终的损失值**。
> 
> 这个损失值随后会被Adam优化器用来更新神经网络的参数，然后开始下一批数据的计算，如此循环往复。

> [!NOTE]- 边界条件残差 $\mathfrak{B}$ 的详解
> 
> ---
> 
> ### 1. $\mathfrak{B}$ 的物理意义
> 
> > **$\mathfrak{B}(\hat{\mathbf{u}}, \ x)$ 代表了PINN的解 $\hat {\mathbf{u}}$ 在边界点 $x$ 上，对我们设定的边界条件的“违反程度”，我们称之为“边界条件残差 (Boundary Condition Residual)”**。
> 
> * **理想情况**: 如果PINN的解 $\hat {\mathbf{u}}$ 在边界上**完美地**满足了所有规则，那么 $\mathfrak{B}$ 的值就应该**等于0**。
> * **现实情况**: 在训练初期，$\hat {\mathbf{u}}$ 几乎肯定不满足边界条件，$\mathfrak{B}$ 就会是一个非零的数值（或向量）。
> 
> `L_BC` 这个损失项的作用，就是通过惩罚这个非零的 $\mathfrak{B}$，迫使神经网络不断调整，直到 $\mathfrak{B}$ 变得无限接近于0为止。
> 
> ---
> 
> ### 2. $\mathfrak{B}$ 的具体计算方法 (Step-by-Step)
> 
> $\mathfrak{B}$ 的具体计算方法取决于我们在该边界上设定的是哪种类型的边界条件（狄利克雷、诺伊曼等）。让我们分情况来看：
> 
> #### 情况一：狄利克雷条件 (Dirichlet Condition)
> 
> **规则**: 直接规定边界上某个物理量 $\hat {\mathbf{u}}$ 的值必须等于一个已知值 `g`。
> **例子**: 在血管入口处，我们规定流入速度必须等于 `g`。
> 
> * **Step 1: 获取PINN的预测值**
>     * 我们取边界上的一个点 `x_bc`，把它输入到PINN中，得到PINN在该点的预测速度 `û(x_bc)`。
> 
> * **Step 2: 获取“标准答案”**
>     * 在这个边界上，我们已知的“标准答案”就是规定的速度值 `g`。
> 
> * **Step 3: 计算残差 $\mathfrak{B}$**
>     * **$\mathfrak{B}$ 就是“预测值”与“标准答案”之间的差距**。
>     $$
>     \mathfrak{B} = \hat{\mathbf{u}}(x_{bc}) - \mathbf{g}
>     $$
> 
> **比喻**: 就像检查一个房间的恒温器。规定温度 `g` 必须是25°C（标准答案），而PINN预测的当前墙壁温度 $\hat {\mathbb{u}}$ 是22°C（预测值）。那么边界残差 $\mathfrak{B}$ 就是 `22 - 25 = -3`。
> 
> #### 情况二：诺伊曼条件 (Neumann Condition)
> 
> **规则**: 规定边界上某个物理量 $\hat {\mathbf{u}}$ 的**法向导数**必须等于一个已知值 `g_n`（在我们的生物医学案例中，这个值是0）。
> **例子**: 在所有边界上，我们规定相场 `φ` 的法向导数必须为0，即 `∂φ/∂n = 0`。
> 
> * **Step 1: 获取PINN的预测值及其导数**
>     * 我们取边界上的一个点 `x_bc`，把它输入到PINN中，得到预测的相场值 `φ̂(x_bc)`。
>     * 利用我们之前讨论过的**自动微分 (AD)**，立刻计算出 `φ̂` 在 `x_bc` 点的**梯度 `∇φ̂`**。
>     * 再计算出这个梯度在**边界法线方向 `n`** 上的投影，这就得到了**法向导数的预测值 `∂φ̂/∂n`**。
> 
> * **Step 2: 获取“标准答案”**
>     * 在这个边界上，我们规定的“标准答案”是 `g_n = 0`。
> 
> * **Step 3: 计算残差 $\mathfrak{B}$**
>     * **$\mathfrak{B}$ 同样是“预测值”与“标准答案”之间的差距**。
>     $$
>     \mathfrak{B} = \frac{\partial \hat{\phi}}{\partial n} - g_n = \frac{\partial \hat{\phi}}{\partial n} - 0
>     $$
> 
> **比喻**: 就像检查一个房间的“保温性能”。规定墙壁必须是完美绝热的，即热量通量 `g_n` 为0（标准答案）。PINN的解经过自动微分计算后，得出的热量通量预测值 `∂T̂/∂n` 是0.5（预测值）。那么边界残差 $\mathfrak{B}$ 就是 `0.5 - 0 = 0.5`。
> 
> ---
> 
> ### 总结
> 
> **$\mathfrak{B}$ 是一个通用的符号，它代表了在边界上各种“规则”的误差总和**。如果一个边界上同时有多种规则，$\mathfrak{B}$ 就会把这些规则的残差组合起来。
> 
> 整个 `L_BC` 的计算流程就是：
> 1.  在边界上选取一批点 `X_BC`。
> 2.  在每一个点上，根据该点的边界规则（是狄利克雷还是诺伊曼），用上述方法计算出**边界残差 $\mathfrak{B}$**。
> 3.  将所有点上的 $\mathfrak{B}$ 值平方，再求平均值，就得到了最终的 `L_BC`。

> [!NOTE]- “一石二鸟”的优化过程详解
> ### 1. 名词概念解释
> 
> * **`θ` (theta) - 模型参数 (Model Parameters)**
>     * **含义**: `θ` 是一个集合，它代表了两个神经网络 (`Net U` 和 `Net W`) 内部**所有可以调节的参数**的总和，也就是我们常说的权重(weights)和偏置(biases)。
>     * **比喻**: `θ` 就是我们那两个“超级英雄”（`Net U`和`Net W`）大脑里所有的“神经元连接强度”和“激活阈值”。**优化`θ`，就等于在训练这两个“超级英雄”，提升他们解决问题的能力**。
> 
> * **`λ = [κ]` (lambda & kappa) - PDE参数 (PDE Parameters)**
>     * **含义**: `λ` (lambda) 代表了**物理方程本身包含的、未知的物理常数**。
>     * `λ = [κ]` 这个写法的意思是：“在这个具体的生物医学案例中，我们定义的未知物理参数集合`λ`里面，**只包含一个成员，那就是渗透率`κ`(kappa)**”。
>     * **比喻**: `λ` 是“考题”本身的一个**“未知条件”**。优化`λ`，就等于在**反向推断这个“未知条件”到底是多少**。
> 
> ---
> 
> ### 2. 优化过程 (The Process)
> 
> > **“...通过迭代地最小化总损失 `L(θ, λ)`...直到损失满足停止标准。”**
> 
> 这句话描述了“学习”是如何发生的。
> 
> * **迭代地 (iteratively)**: 意思是这个过程不是一步完成的，而是像我们之前讨论的那样，一步一步、周而复始地进行的。
> * **最小化总损失 `L(θ, λ)`**: 这是**唯一的目标**。我们的“智能登山者”（Adam优化器）在每一步，都会想办法调整 `θ` 和 `λ`，来让总的“扣分”`L`变得更小一点。
> * **停止标准 (stopping criteria)**: 这是训练结束的条件。通常可以是：
>     * 总损失`L`已经小到了一个我们满意的程度。
>     * 或者，我们预设的训练轮次（epochs）已经全部完成。
> 
> ---
> 
> ### 3. 核心思想：“一石二鸟”的逆问题求解
> 
> > **“最小化总损失是一个针对 `λ` 的优化过程，以使得PINN的输出能够满足...”**
> 
> 这是对这个过程最精辟的总结，它揭示了PINN求解逆问题的“魔法”所在。
> 
> **“侦探与缺失的关键线索”比喻**:
> * **PINN的输出 `û`**: 侦探（神经网络 `θ`）对整个案情（流场）的详细重构。
> * **损失函数 `L`**: 衡量这个“案情重构”有多么完美的“评分系统”。一个完美的重构，必须同时满足所有条件：物理定律（没有逻辑漏洞）、边界/初始条件（符合案发背景）、以及测量数据（和所有已知证据吻合）。
> * **缺失的线索 `λ = [κ]`**: 案发现场有一个关键的未知物理参数，比如，地上的脚印很奇怪，我们不知道罪犯穿的是什么**型号的鞋子 (`κ`)**。
> 
> **优化过程就是侦探的推理过程**:
> 1.  侦探先**假设**一个鞋子型号（给`κ`赋一个初始值）。
> 2.  基于这个假设，他尽力去重构一个最完美的案情（优化`θ`，让`L`尽可能小）。但他发现，无论怎么重构，这个案情总有一些地方和已知证据对不上（总损失`L`比较大）。
> 3.  于是，侦探**换一个鞋子型号**的假设（优化器调整`κ`的值）。
> 4.  他再次基于新的假设去重构案情（再次优化`θ`）。他发现这次的重构，能更好地吻合所有证据和逻辑（总损失`L`变得更小了）。
> 5.  ...
> 
> **周而复始，Adam优化器同时在调整`θ`和`κ`**。最终，它会找到那个**独一无二的、正确的鞋子型号`κ`**，只有在这个型号下，侦探`θ`才能构建出一个**完美无瑕、能解释所有线索和逻辑**的案情重构，使得**总损失`L`达到最小值**。
> 
> **总结**: 这个过程通过一个统一的“最小化损失”目标，巧妙地将**“学习如何求解问题”（优化`θ`）**和**“推断问题中的未知参数”（优化`λ`）**这两个任务**同步完成**了，展现了其在解决逆问题上的巨大威力。

### 5.2 问题设置
我们考虑一个计算设置：在一个通道中，有一个半圆形的、可渗透的血栓，稳态的抛物线形水流从左边流入（图8）。这个设置是有意义的，因为它呈现了一个理想化的血栓，该血栓有一个由纤维蛋白凝块组成的不可渗透的核心，以及一个由松散堆积、部分激活的血小板组成的可渗透的外壳。这个模型在以前的工作中[56, 57, 58]已被用作理想化的血栓。其目标是基于可测量的相场数据，来推断未知的渗透率（对于外壳和核心）和速度场。
 
 图8(b)展示了所有类型的训练数据，即初始快照 $t_0(\textcolor{cyan}\star)$、从 $t_1$ 到 $t_n$ 的内部时空域 ($\textcolor{orange}\star$)、以及在边界上的 ($\textcolor{violet}\star$)。此外，点测量值 ($\textcolor{brown}\star$) 包括它们的坐标和相场值，在时空域中被采样，用于计算总损失中的数据损失项。我们从一个初始快照中抽取1000个点，抽取10000个内部点来计算PDE残差，以及1000个边界点来计算边界上的残差。注意，点测量值和内部点都是从内部时空域中抽取的；前者包含PDE的解，而后者不包含。

如图8(a)所示，血栓存在于通道中间，其核心 $(\phi = -1)$ 的渗透率 $\kappa = 0.001$，而在外壳层 $(\phi = 0)$ 的渗透率 $\kappa = 1$。为了明确地表达这种空间变化，我们在本案例中考虑 $\phi$ 和 $\kappa$之间的一种关系：
$$
\kappa(\phi)=e^{a\phi}+b, \tag{24}
$$
其中 $a$ 和 $b$ 是PINN模型中需要优化的模型参数，而 $a$ 和 $b$ 的真实值是 $6.90$ 和 $0.0$。值得注意的是，这种关系的形式不是唯一的，只要渗透率的值在 $\phi=0$ 和 $\phi=-1$ 时与真实值匹配即可。（*注：原文此处`φ=1`应为`φ=-1`*）

**图9：具有相位相关渗透率的二维血栓绕流推断结果。** (a) 网络损失（`Loss PDE`, `Loss IC`, `Loss BC` 和 `Loss Data`）的历史记录。(b) 推断出的渗透率 $\kappa$ 作为 $\phi$ 的函数。(c)$t=0.78$ 时，$\kappa(\phi)$ 的相场和速度场的比较以及它们的绝对误差。核心渗透率的实际值为 $0.001$，外壳渗透率的实际值为 $1$。从30个快照 $(t \in [0.03, 0.93])$ 的时空域中散布了 $10,000$ 个数据点作为训练数据来推断渗透率。推断出的 $a$ 和 $b$ 分别为 $7.10$ 和 $0.0003$，而真实值为 $6.9$ 和 $0.0$ 。（图改编自[14]）
![[week1_paperlearning11.png]]

> [!NOTE]- 生物医学案例的问题设置详解
> 
> ---
> 
> ### 翻译
> 
> #
> 
> ---
> 
> ### 解释
> 
> #### 1. “实验模型”——一个简化的“夹心蛋糕”式血栓
> 
> 作者首先构建了一个理想化但物理意义明确的血栓模型，我们可以把它想象成一个**“夹心蛋糕”**：
> 
> * **不可渗透的核心 (Impermeable core)**: `φ = -1`。
>     * 蛋糕的**“饼干夹心”**。它非常致密，由纤维蛋白构成，血液几乎无法流过。因此它的**渗透率`κ`极低**，设为`0.001`。
> * **可渗透的外壳 (Permeable shell)**: `φ = 0`。
>     * 蛋糕的**“海绵蛋糕外层”**。它由松散的血小板构成，结构疏松多孔，血液可以相对容易地从中流过。因此它的**渗透率`κ`较高**，设为`1`。
> 
> 这个模型虽然是简化的，但很好地抓住了真实血栓“内密外疏”的关键特征。
> 
> #### 2. “任务目标”——只“看”不动手，猜出渗透性
> 
> 本次案例的逆问题任务非常明确和巧妙：
> 
> > **目标：仅根据可测量的“成分地图”（相场数据`φ`），来反向推断出整个血栓未知的“渗透性地图”(`κ`)以及完整的速度场**。
> 
> * **“看菜谱猜菜”的比喻**: 想象一下，我只给你看一张这个“夹心蛋糕”的**设计图**（`φ`数据，告诉你哪里是饼干，哪里是海绵蛋糕），同时让你观察水流是如何冲刷这个蛋糕并使其变形的。你的任务是，不准亲自尝或者捏，只通过观察，就要准确地分析出这个“海绵蛋糕”到底有多**松软**（即渗透率`κ`是多少）。
> 
> #### 3. “解题技巧”——聪明的数学假设
> 
> “渗透率`κ`”不是一个固定的数字，而是一个随着`φ`变化的函数`κ(φ)`。直接让神经网络去学习一个未知的函数是非常困难的（有无穷的可能性）。
> 
> 于是作者用了一个非常聪明的技巧：**为一个未知的函数，假设一个已知的形式**。
> 
> > `κ(φ) = e^(aφ) + b` (24)
> 
> * **“聪明的菜谱”比喻**: 与其让你凭空创造一个菜谱，我不如给你一个**菜谱模板**：“这道菜的味道`κ`，是由A香料（`a`）和B香料（`b`）按 `e^(aφ) + b` 的公式调配出来的。”
> 
> 这样一来，PINN的任务就从“学习一个无限复杂的未知函数`κ(φ)`”，**大大简化**成了“学习**两个未知的数字`a`和`b`**”。
> 
> 作者已经提前设定好了这个问题的“标准答案”：真实的`a`应该是6.90，`b`应该是0.0。最终，PINN“猜”出的`a`和`b`越接近这两个值，就说明它完成逆问题的能力越强。

> [!NOTE]- 生物医学案例的推断结果详解
> ### 图(a): “学习日记”——损失函数历史记录
> 
> * **它是什么？** 这张图展示了在30万轮的训练过程中，四个损失项（“四门主课”）的“扣分”是如何变化的。
> * **我们能看到什么？**
>     * **蓝色线 (`Loss PDE`)**: 物理定律损失是“扣分”的大头，一直保持在较高水平。这很正常，因为要让解在成千上万个随机点上都完美满足复杂的物理方程是最难的任务。
>     * **其他三条线 (IC, BC, Data)**: 初始、边界和数据损失都被**成功地压到了非常非常低**的水平（低于`10⁻³`）。这说明PINN这位“学生”已经完美地掌握了所有已知的“开卷信息”（初始、边界和测量数据）。
>     * **尖峰**: 图中周期性的尖峰可能是由于训练中学习率的调整或其他动态权重策略导致的，但总体趋势是稳定收敛的。
> * **结论**: 训练过程是**成功的**。模型在努力满足最难的物理定律的同时，也精确地拟合了所有已知的数据点。
> 
> ---
> 
> ### 图(b): “破解配方”——渗透率的推断结果
> 
> 这是本次逆问题任务的**核心答案**。我们要求PINN去猜测渗透率函数`κ(φ) = e^(aφ) + b`中的未知参数`a`和`b`。
> 
> * **它是什么？** 这张图画出了PINN最终“猜”出的`κ(φ)`函数曲线（蓝色曲线）。
> * **我们能看到什么？**
>     * 图中标注了PINN的推断结果和“标准答案”的对比：
>         * **在外壳处 (`φ=0`)**: 真实`κ`应为`1.0`。PINN推断出 `κ(0) = 1.0003`，误差极小。
>         * **在核心处 (`φ=-1`)**: 真实`κ`应为`0.0010`。PINN推断出 `κ(-1) = 0.0011`，误差同样非常小。
>     * 最终推断出的参数`a=7.10`, `b=0.0003`，也与真实值`a=6.9`, `b=0.0`非常接近。
> * **结论**: **PINN成功地破解了“配方”**！它精确地反推出了隐藏在物理现象背后的、未知的材料属性参数。
> 
> ---
> 
> ### 图(c): “画作对比”——流场的重构结果
> 
> 在成功推断出渗透率`κ`后，PINN也同时给出了完整的流场解。这张图将PINN的“画作”与“标准答案”进行了对比。
> 
> * **它是什么？**
>     * **左列 (`φ_pred`, `V_pred`)**: PINN在`t=0.78`时刻画出的“成分地图”（相场`φ`）和“速度地图”（速度大小`V`）。
>     * **右列 (`|φ_ref - φ_pred|`, `|V_ref - V_pred|`)**: **“误差地图”**。它显示了PINN的画作与“标准答案”在每一个像素点上的差异大小（绝对误差）。
> * **我们能看到什么？**
>     * 从左列的图像看，PINN重构出的血栓形态和速度分布在宏观上是**非常准确**的。
>     * 从右列的误差图看，绝大部分区域都是深蓝色（代表误差接近0）。误差主要集中在**血栓的边界层和尾流区域**，这是物理变化最剧烈、最难精确模拟的地方，但即便是这些地方，最大的误差值也很小（`0.10`左右）。
> * **结论**: PINN不仅成功猜出了“配方”，还用这个配方**高质量地还原了整道“菜肴”**（流场）。特别是，它在**只给了`φ`数据**的情况下，还能准确地反推出**隐藏的速度场`V`**，这非常了不起。
> 
> ### 最终总结
> 
> 这张图全面展示了PINN在解决这个复杂的生物医学逆问题上的巨大成功。它证明了PINN能够：
> 1.  **精确地反演未知的物理参数**。
> 2.  **在只有部分物理场信息（只有`φ`）的情况下，准确地重构出其他隐藏的物理场（速度`V`）**。

### 5.3 推断结果
我们在图9中展示了推断结果。图9(a)显示了不同损失的历史记录，即PDE损失、边界条件损失(`Loss BC`)、初始条件损失(`Loss IC`)和数据损失(`Loss Data`)。我们用300,000个轮次训练了模型，其中PDE损失是最大的组成部分。其他误差都低于$O(10^{-3})$。图(b)展示了渗透率 $\kappa$ 作为 $\phi$ 函数的推断结果；推断出的 $a$ 和 $b$ 分别为 $7.1$ 和 $0.0003$，这表明在核心区域 $(\kappa(\phi = -1) = 0.0011)$ 和外壳区域 $(\kappa(\phi = 0) = 1.0003)$ 的渗透率与参考值匹配得很好。
作为一个定性比较，我们在图9(c)中展示了`t=0.78`时刻预测的相场 `φ_pred` 和速度场 `v_pred`，以及它们与参考数据的差异。相场的预测与真实数据相比，展现出了良好的一致性。更重要的是，隐藏的速度场也能够仅基于相场数据被精确地推断出来。注意到相场的误差主要分布在血栓的出口层及其周围，而速度场的误差主要局限在外壳层内。

> [!NOTE]- 生物医学案例推断结果的官方解读
> 
> 这段话就是对图9三部分内容的详细“阅卷报告”。
> 
> #### 1. 关于训练过程 (对应图9a)
> 
> * 作者确认，模型成功地训练了30万轮。
> * 在整个训练中，**PDE损失**是最大、最难降低的一项，是“扣分”的主要来源。
> * 而其他三项损失（初始、边界、数据损失）都被压得非常低（低于`10⁻³`，即0.001）。
> * **结论**: 这说明训练是成功的。PINN在努力攻克最难的“物理定律”这门主课的同时，完美地满足了所有已知的“开卷信息”。
> 
> #### 2. 关于逆问题求解 (对应图9b)
> 
> 这是本次任务的核心——反推未知的渗透率参数`a`和`b`。
> 
> * 作者报告了最终的推断结果：`a = 7.1`，`b = 0.0003`。
> * 将这两个值代入公式`κ(φ) = e^(aφ) + b`后，计算出的核心渗透率(`κ(φ=-1)`)和外壳渗透率(`κ(φ=0)`)与我们已知的真实值**“匹配得很好 (match the reference values well)”**。
> * **结论**: PINN**成功地完成了逆问题的求解**，精确地找出了隐藏在物理现象背后的材料参数。
> 
> #### 3. 关于流场重构 (对应图9c)
> 
> 在“破解”了渗透率之后，PINN也同时给出了完整的流场“画作”。
> 
> * **相场 `φ` 的重构**: PINN预测的血栓形态`φ_pred`与真实的形态**“展现出了良好的一致性 (exhibits a good agreement)”**。
> * **最重要的发现**:
>     > **“更重要的是，隐藏的速度场也能够仅基于相场数据被精确地推断出来。”**
> * 这句话是本案例的点睛之笔。我们给PINN的“已知线索”`L_data`中**只包含了**关于血栓形态`φ`的信息，完全没有速度信息。但PINN却能利用物理定律，从`φ`的变化中，无中生有地、且精确地反推出完整的速度场。
> 
> * **误差分析**: 作者最后还指出了微小误差的分布位置：
>     * 相场的误差主要在**血栓的下游边缘**。这是因为流体从血栓剥离的区域，物理上最复杂，最难模拟。
>     * 速度的误差主要在**血栓多孔的外壳层**。这也是合理的，因为这是流体与多孔介质相互作用最剧烈的区域。

## 6 总结
PINN为模拟真实的流体流动提供了一种新方法，在这些场景中，我们能从多模态测量中获得一些数据，但边界条件或初始条件却可能是未知的。尽管这在实践中或许是普遍存在的情况，但现有的CFD求解器无法处理这类**病态问题 (ill-posed problems)**，因此，我们可以将用于流体问题的PINN看作是现有大量用于理想化问题的CFD数值方法的一种**补充方法**。

未来有几个值得进一步研究的机会，例如，使用PINN进行**主动流动控制**，以替代昂贵的实验和耗时的大规模模拟（如[59]）；或者，假设我们有较低雷诺数下的可用解，使用**迁移学习**技术来快速预测新的高雷诺数下的流动。此外，一个新的探索领域可能是，使用PINN提供的自动数据同化方法，为极高雷诺数下未解析的流动动力学开发**封闭模型**。

大规模计算流动问题需要在数据并行[28]或数据并行与模型并行混合[27]的框架下实现高效的**多GPU**实现。迄今为止在流动模拟中获得的并行加速效果非常好，这表明PINN在不久的将来可以用于解决CFD方法无法应对的、**工业复杂度**的大规模问题。

> [!TIP]- 总结 (Summary) 与展望
> 
> 这段总结可以分为三部分：**PINN的定位**、**未来的应用方向**和**走向大规模应用的途径**。
> 
> #### 1. PINN的定位：不是“替代者”，而是“最佳拍档”
> 
> 作者首先明确了PINN在科学计算领域的生态位：
> 
> * **它解决了什么问题？** 解决了现实世界中那种“脏活累活”：我们有一些实验测量数据，但对问题的边界/初始条件却不完全清楚。这种信息不完整的“病态问题”，是传统CFD的“天敌”，但却是PINN的“主场”。
> * **它的角色是什么？** PINN不是要“干掉”传统CFD，而是作为一种强大的**“补充方法 (complementary approach)”**。
>     * **CFD**: 像一个在无菌实验室里做精密实验的科学家，适合处理条件完美的**“理想化问题”**。
>     * **PINN**: 像一个在野外勘探的工程师，擅长利用手头有限的、不完美的工具和线索，解决**“现实世界问题”**。
> 
> #### 2. 未来的“新战场”：三个充满想象力的研究方向
> 
> 作者为PINN指明了几个激动人心的未来发展方向：
> 
> * **主动流动控制 (Active Flow Control)**
>     * **是什么**: 比如在飞机机翼上安装微小的传感器和执行器，实时感知气流的变化，并立刻通过吹气或吸气来调整气流，以达到减阻或增升的目的。
>     * **PINN的作用**: PINN可以作为一个快速、智能的“大脑”，连接传感器和执行器，取代昂贵且危险的真实飞行实验，在数字世界里高效地找出最佳的控制策略。
> 
> * **迁移学习预测高雷诺数流动 (Transfer Learning for High Reynolds Number)**
>     * **是什么**: 模拟极高雷诺数（完全湍流）的流动非常耗费计算资源。但我们有很多低雷诺数下的精确模拟结果。
>     * **PINN的作用**: 利用**“迁移学习”**，PINN可以先在大量“简单模式”（低雷诺数）的数据上进行学习，掌握流动的基本规律，然后再把这些学到的知识“迁移”到“困难模式”（高雷诺数）上，从而**大大加快**对高雷诺数流动的预测速度。
> 
> * **为湍流开发封闭模型 (Development of Closure Models)**
>     * **是什么**: 湍流的完全模拟（DNS）成本太高，工程上常用一些简化模型（如RANS/LES）来近似。但这些模型的“封闭模型”（用来近似未知的小尺度湍流效应的经验公式）通常不够准确。
>     * **PINN的作用**: PINN强大的数据同化能力，可以完美地融合稀疏的实验数据和不完美的简化模型，从而**自动地“学习”出一个更精确、更符合物理现实的湍流封闭模型**。这有望解决困扰流体力学界几十年的湍流建模难题。
> 
> #### 3. 走向现实：大规模并行计算
> 
> * **挑战**: 要想让PINN解决真实的、工业级别的复杂问题（比如模拟一整架飞机或一辆汽车），计算量依然是巨大的。
> * **解决方案**: 必须使用**多GPU并行计算**。就像用一个GPU集群（团队）来代替单个GPU（个人）进行工作。
> * **前景**: 作者最后指出，目前PINN在并行计算上已经展现出了**非常好的加速效果**，这预示着PINN离解决那些传统CFD算不动的大规模工业难题，已经为时不远了。

> [!NOTE]- 湍流“封闭模型”详解
> 好的，这是一个非常深刻的问题，它触及了**湍流（Turbulence）**模拟这个流体力学“圣杯”级难题的核心。
>
>“封闭模型 (Closure Model)” 的概念有点抽象，但我们可以用一个天气预报的比喻来把它彻底讲清楚。
>
> ---
> 
> #### 1. 问题的根源：湍流的“尺度”太多了
> 
> **什么是湍流？** 湍流就是你看到的一条混乱的、翻滚的河流，或者烟囱里冒出的滚滚浓烟。它充满了大大小小、杂乱无章的漩涡（涡旋）。
> 
> **核心挑战**: 湍流的“尺度”范围极其巨大。
> * **天气预报的比喻**: 想象一下，你要做一个覆盖整个亚洲的、为期一周的天气预报。
>     * **大尺度**: 你需要模拟巨大的**台风**和**西伯利亚冷空气**的移动。这些涡旋直径可达上千公里。
>     * **小尺度**: 同时，在你的城市里，高楼之间会刮起**阵风**，甚至你家阳台上一盆花后面都会形成**微小的气流漩涡**。这些涡旋可能只有几厘米大小。
> 
> **问题**: 要想用计算机精确模拟出从台风到阳台小漩涡的**所有细节**，所需要的计算资源是目前全世界最强大的超级计算机也无法承受的。这被称为**直接数值模拟 (Direct Numerical Simulation, DNS)**，成本极高。
> 
> ---
> 
> #### 2. “偷懒”的智慧：如何建立简化模型？
> 
> 既然我们算不起所有细节，工程师们就想了一个聪明的“偷懒”办法：
> 
> > **我们只精确计算那些“大尺度”的、重要的涡旋（比如台风），而那些“小尺度”的、我们算不起的微小漩涡，我们就不直接计算了，而是用一个简化的数学模型去“猜测”它们对大尺度涡旋的总体影响。**
> 
> **这个用来“猜测”小尺度涡旋总体效应的简化数学模型，就叫做“封闭模型 (Closure Model)”**。
> 
> * **天气预报的比喻**:
>     * 你放弃了计算每一栋高楼之间的阵风。
>     * 取而代之，你制定了一条**经验法则（封闭模型）**：“在一个城市区域，由于大量高楼的存在，气流的整体能量会平均衰减10%，并且会给大的气团带来一个额外的、平均大小的‘拖拽力’。”
>     * 你把这条简单的“经验法则”加入到你的台风模拟中，来近似地模拟整个城市对台风的影响。
> 
> ---
> 
> #### 3. 传统封闭模型的问题
> 
> 传统的封闭模型（比如 `k-ε` 模型、`k-ω` 模型）都是由科学家们基于物理直觉和简化假设推导出来的“经验法则”。
> 
> **缺点**:
> * **不够普适**: 这条“城市拖拽力”的法则，可能在北京很好用，但在山城重庆就完全不准了。传统封闭模型在一种流动中表现良好，换一种流动可能就错得离谱。
> * **依赖“调参数”**: 模型里有很多需要手动调节的经验常数，像老式收音机一样，需要不断“微调”才能对准一个台。
> 
> ---
> 
> #### 4. PINN如何“革命”封闭模型？
> 
> > **“...使用PINN提供的自动数据同化方法，为极高雷诺数下未解析的流动动力学开发封闭模型。”**
> 
> 这句话描绘了一个激动人心的未来。PINN可以成为一个**“智能的封闭模型学习机”**。
> 
> * **“数据同化 (Data Assimilation)”的比喻**:
>     * 你的天气预报模型（包含一个旧的封闭模型）正在运行。
>     * 这时，你收到了来自亚洲各地几百个气象站的**稀疏、真实的测量数据**（比如风速、气压）。
>     * **数据同化**，就是把这些真实的、零散的数据，“喂”给你的模型，实时地去**修正**模型的预测结果。
> 
> **PINN的作用**:
> 
> PINN的损失函数可以完美地融合这两者：
> * `L_PDE`: 包含那个**不完美的、需要改进的**传统封闭模型。
> * `L_data`: 包含那些**稀疏但绝对真实**的实验测量数据。
> 
> PINN在最小化总损失`L`的过程中，为了让模型的预测结果与真实数据吻合，它会自动地、智能地去**“修正”或“学习”**那个封闭模型中的未知项或不准确的部分。
> 
> **最终**: PINN不再是使用一个“一成不变”的经验公式，而是**直接从真实数据中，“学习”出了一个更精确、更符合当前物理问题的、定制化的“封闭模型”**。这有望从根本上提升我们对湍流等复杂多尺度问题的预测能力。

> [!NOTE]- 迁移学习 (Transfer Learning) 实现路径详解
>**迁移学习 (Transfer Learning)** 是机器学习领域一个非常强大且高效的策略。在PINN的应用中，它就像是让我们的“AI学生”学会**“举一反三”**，而不是每次考试都从零开始学习。
>
>论文中提到它的应用场景是：“...假设我们有较低雷诺数下的可用解，使用迁移学习技术来快速预测新的高雷诺数下的流动”。
>
>下面，我们就以这个场景为例，一步步详细说明它的实现路径。
>
> ---
> 
> ### 核心思想：不要浪费已经学到的知识
> 
> 传统的训练方式是，每次遇到一个新问题（比如一个新的雷诺数），我们都初始化一个“无知”的、参数随机的神经网络，然后从头开始训练。这非常耗时。
> 
> **迁移学习**的核心思想是：很多问题之间是有共性的。一个在“简单问题”上训练好的模型，已经学到了很多关于这个领域的“基础知识”。我们可以把这些“基础知识”**迁移**到一个新的、更难的模型中，作为它学习的**起点**，从而大大加快学习速度。
> 
> ### “高中生升入大学”的比喻
> 
> * **简单问题 (低雷诺数流动)**: **高中物理**。知识体系相对简单、规律清晰。
> * **困难问题 (高雷诺数流动)**: **大学的《高等流体力学》**。知识更复杂、更困难。
> 
> 一个学生（PINN模型）在进入大学前，肯定不需要把高中知识忘掉，而是应该带着这些知识去学习大学课程。
> 
> ---
> 
> ### Step-by-Step 实现路径
> 
> #### 第1步：“打基础”——训练一个“高中物理学霸”模型 (Pre-training)
> 
> 1.  **选择源任务 (Source Task)**: 我们选择一个**低雷诺数**的流动问题，比如我们第一个案例中的 `Re=200` 的圆柱绕流。这是一个我们已经有精确解（CFD“标准答案”）的、相对“简单”的问题。
> 2.  **构建基础模型**: 我们构建一个PINN模型（我们称之为`Model_LowRe`），并使用`Re=200`的数据和物理方程对它进行**充分训练**，直到它的损失函数收敛，能够准确地预测`Re=200`的流场。
> 3.  **保存知识**: 训练完成后，`Model_LowRe`的权重`θ`中就包含了关于流体运动（比如涡旋、边界层等）的**基础物理知识**。我们将这个训练好的模型（特别是它的权重`θ`）保存下来。
> 
> > **至此，我们有了一个“精通高中物理”的学霸。**
> 
> #### 第2步：“知识迁移”——让“学霸”去读大学 (Transferring Weights)
> 
> 4.  **确定目标任务 (Target Task)**: 我们现在要解决一个全新的、非常困难的**高雷诺数**问题，比如 `Re=5000`的流动。
> 5.  **构建新模型**: 我们构建一个新的PINN模型（`Model_HighRe`），它的网络结构通常和`Model_LowRe`非常相似，甚至完全一样。
> 6.  **关键操作 - 迁移**: 我们**不再**随机初始化`Model_HighRe`的权重，而是**直接加载**我们在第1步中保存下来的`Model_LowRe`的权重`θ`，作为它的**初始权重**。
> 
> > **至此，我们让这位“高中学霸”直接坐在了大学教室里，他已经具备了所有的前置知识，而不是一个什么都不懂的新生。**
> 
> #### 第3步：“深造”——针对性地学习大学知识 (Fine-tuning)
> 
> 7.  **准备新教材**: 我们把`Re=5000`这个新问题的物理方程和相关数据（如果有一些的话）准备好，作为`Model_HighRe`的新“教科书”。
> 8.  **微调训练**: 我们在新的“教科书”上继续训练`Model_HighRe`。因为它的起点非常高（而不是随机的），所以这个训练过程会：
>     * **收敛得非常快**: 它不需要从头学习“什么是流体”，只需要学习“高雷诺数下的流体有什么新特性”。
>     * **需要的数据更少**: 它的“物理直觉”已经很好了，可能只需要少量新的数据就能举一反三。
>     * **更容易找到好的解**: 它已经处在一个很好的“山谷”附近，更容易找到那个最低点。
> 9.  **可选技巧 - “冻结”部分知识**: 在微调时，我们甚至可以“冻结”前面几层的网络权重，让它们保持在“高中物理”的水平上（因为这些是基础，不会变），只训练后面几层，让它们专门学习更高级的“大学知识”。
> 
> > **至此，这位“学霸”通过短暂的“深造”，迅速成为了“大学流体力学”的专家。**
> 
> ### 总结
> 
> 迁移学习的路径，本质上就是一个**“预训练 + 微调” (Pre-training + Fine-tuning)** 的过程。它通过在一个数据充足、相对简单的任务上预先学习通用知识，然后将这些知识作为“热启动”的起点，去快速适应和解决一个更困难、数据更稀疏的新任务。
> 
> 对于昂贵的流体模拟来说，这意味着我们可以把大量计算资源投入到几个经典的低雷诺数问题上，训练出强大的“基础模型”，然后用很少的计算资源，就能快速地将这个模型的能力“迁移”到各种各样的高雷诺数新问题上，极大地节省了时间和成本。

